const t=JSON.parse('{"key":"v-1dd111c8","path":"/train/DDP/DDP.html","title":"Pytorch分布式训练","lang":"zh-CN","frontmatter":{"date":"2023-07-19T00:00:00.000Z","category":["炼丹"],"tag":["DDP","分布式"],"description":"Pytorch分布式训练 本文主要介绍DistributedDataParallel以及DataParallel并行的区别和DDP的原理、方法。 单机多卡并行 常用切分方案有：数据并行、模型并行、通道并行(数据+模型并行) 数据并行：将batch分成n块，每个GPU拿到完整参数计算一块数据的梯度，该方法通常性能更好 模型并行：将模型分成n块，每个GPU拿到一块模型计算其前向和方向结果，该方法通常适用于单GPU放不下大模型的情况，这就会导致整个计算过程是串行的，加大了性能优化难度 DDP 与 DP 的区别","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/train/DDP/DDP.html"}],["meta",{"property":"og:site_name","content":" "}],["meta",{"property":"og:title","content":"Pytorch分布式训练"}],["meta",{"property":"og:description","content":"Pytorch分布式训练 本文主要介绍DistributedDataParallel以及DataParallel并行的区别和DDP的原理、方法。 单机多卡并行 常用切分方案有：数据并行、模型并行、通道并行(数据+模型并行) 数据并行：将batch分成n块，每个GPU拿到完整参数计算一块数据的梯度，该方法通常性能更好 模型并行：将模型分成n块，每个GPU拿到一块模型计算其前向和方向结果，该方法通常适用于单GPU放不下大模型的情况，这就会导致整个计算过程是串行的，加大了性能优化难度 DDP 与 DP 的区别"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-10-18T15:20:18.000Z"}],["meta",{"property":"article:author","content":"Mr.R"}],["meta",{"property":"article:tag","content":"DDP"}],["meta",{"property":"article:tag","content":"分布式"}],["meta",{"property":"article:published_time","content":"2023-07-19T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-10-18T15:20:18.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Pytorch分布式训练\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-19T00:00:00.000Z\\",\\"dateModified\\":\\"2023-10-18T15:20:18.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Mr.R\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"单机多卡并行","slug":"单机多卡并行","link":"#单机多卡并行","children":[{"level":3,"title":"DDP 与 DP 的区别","slug":"ddp-与-dp-的区别","link":"#ddp-与-dp-的区别","children":[]},{"level":3,"title":"DDP与DP的区别","slug":"ddp与dp的区别","link":"#ddp与dp的区别","children":[]},{"level":3,"title":"DDP 多卡训练的原理","slug":"ddp-多卡训练的原理","link":"#ddp-多卡训练的原理","children":[]}]}],"git":{"createdTime":1697642418000,"updatedTime":1697642418000,"contributors":[{"name":"WillebrordSnell","email":"799976781@qq.com","commits":1}]},"readingTime":{"minutes":3.41,"words":1022},"filePathRelative":"train/DDP/_DDP.md","localizedDate":"2023年7月19日","excerpt":"<h1> Pytorch分布式训练</h1>\\n<p>本文主要介绍DistributedDataParallel以及DataParallel并行的区别和DDP的原理、方法。</p>\\n<h2> 单机多卡并行</h2>\\n<p>常用切分方案有：数据并行、模型并行、通道并行(数据+模型并行)</p>\\n<p>数据并行：将batch分成n块，每个GPU拿到完整参数计算一块数据的梯度，该方法通常性能更好</p>\\n<p>模型并行：将模型分成n块，每个GPU拿到一块模型计算其前向和方向结果，该方法通常适用于单GPU放不下大模型的情况，这就会导致整个计算过程是串行的，加大了性能优化难度</p>\\n<h3> DDP 与 DP 的区别</h3>","autoDesc":true}');export{t as data};
