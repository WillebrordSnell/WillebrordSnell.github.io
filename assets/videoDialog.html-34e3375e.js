const e=JSON.parse('{"key":"v-34f29d1e","path":"/keyan/videoDialog/videoDialog.html","title":"视频对话方向大论文性质的记录","lang":"zh-CN","frontmatter":{"date":"2023-10-26T00:00:00.000Z","category":["码头"],"tag":["视频对话"],"description":"视频对话方向大论文性质的记录 视频对话领域在近几年顶刊上的paper寥寥无几并且在大模型的冲击下许多工作本就是换皮，本无打算特地做综述性质的报告，但由于开题和毕业论文需要，故留记录 Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue 论文地址： 本文提出AVSD中存在：不理解问题的情况下不加区分地抄袭输入文本，其原因是由于数据集中的答案句子通常包含输入文本中的单词，因此VGD(Video-grounded Dialogue)系统过度依赖于复制输入文本中的单词，希望这些单词与地面实况文本重叠，从而学习到虚假的相关性。","head":[["meta",{"property":"og:url","content":"https://github.com/WillebrordSnell/keyan/videoDialog/videoDialog.html"}],["meta",{"property":"og:site_name","content":" "}],["meta",{"property":"og:title","content":"视频对话方向大论文性质的记录"}],["meta",{"property":"og:description","content":"视频对话方向大论文性质的记录 视频对话领域在近几年顶刊上的paper寥寥无几并且在大模型的冲击下许多工作本就是换皮，本无打算特地做综述性质的报告，但由于开题和毕业论文需要，故留记录 Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue 论文地址： 本文提出AVSD中存在：不理解问题的情况下不加区分地抄袭输入文本，其原因是由于数据集中的答案句子通常包含输入文本中的单词，因此VGD(Video-grounded Dialogue)系统过度依赖于复制输入文本中的单词，希望这些单词与地面实况文本重叠，从而学习到虚假的相关性。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-10-26T13:39:41.000Z"}],["meta",{"property":"article:author","content":"Mr.R"}],["meta",{"property":"article:tag","content":"视频对话"}],["meta",{"property":"article:published_time","content":"2023-10-26T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-10-26T13:39:41.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"视频对话方向大论文性质的记录\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-10-26T00:00:00.000Z\\",\\"dateModified\\":\\"2023-10-26T13:39:41.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Mr.R\\",\\"url\\":\\"https://github.com/WillebrordSnell\\"}]}"]]},"headers":[{"level":2,"title":"Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue","slug":"information-theoretic-text-hallucination-reduction-for-video-grounded-dialogue","link":"#information-theoretic-text-hallucination-reduction-for-video-grounded-dialogue","children":[]},{"level":2,"title":"Video Dialog as Conversation about Objects Living in Space-Time","slug":"video-dialog-as-conversation-about-objects-living-in-space-time","link":"#video-dialog-as-conversation-about-objects-living-in-space-time","children":[]},{"level":2,"title":"Structured Co-reference Graph Attention for Video-grounded Dialogue","slug":"structured-co-reference-graph-attention-for-video-grounded-dialogue","link":"#structured-co-reference-graph-attention-for-video-grounded-dialogue","children":[]},{"level":2,"title":"BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues","slug":"bist-bi-directional-spatio-temporal-reasoning-for-video-grounded-dialogues","link":"#bist-bi-directional-spatio-temporal-reasoning-for-video-grounded-dialogues","children":[]},{"level":2,"title":"Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents","slug":"describing-unseen-videos-via-multi-modal-cooperative-dialog-agents","link":"#describing-unseen-videos-via-multi-modal-cooperative-dialog-agents","children":[]},{"level":2,"title":"Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers","slug":"dynamic-graph-representation-learning-for-video-dialog-via-multi-modal-shuffled-transformers","link":"#dynamic-graph-representation-learning-for-video-dialog-via-multi-modal-shuffled-transformers","children":[]},{"level":2,"title":"Video-Grounded Dialogues with Pretrained Generation Language Models","slug":"video-grounded-dialogues-with-pretrained-generation-language-models","link":"#video-grounded-dialogues-with-pretrained-generation-language-models","children":[]},{"level":2,"title":"Multi-Speaker Video Dialog with Frame-Level Temporal Localization","slug":"multi-speaker-video-dialog-with-frame-level-temporal-localization","link":"#multi-speaker-video-dialog-with-frame-level-temporal-localization","children":[]},{"level":2,"title":"Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems","slug":"multimodal-transformer-networks-for-end-to-end-video-grounded-dialogue-systems","link":"#multimodal-transformer-networks-for-end-to-end-video-grounded-dialogue-systems","children":[]}],"git":{"createdTime":1698325248000,"updatedTime":1698327581000,"contributors":[{"name":"WillebrordSnell","email":"799976781@qq.com","commits":2}]},"readingTime":{"minutes":3.09,"words":928},"filePathRelative":"keyan/videoDialog/_videoDialog.md","localizedDate":"2023年10月26日","excerpt":"<h1> 视频对话方向大论文性质的记录</h1>\\n<p>视频对话领域在近几年顶刊上的paper寥寥无几并且在大模型的冲击下许多工作本就是换皮，本无打算特地做综述性质的报告，但由于开题和毕业论文需要，故留记录</p>\\n<h2> Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue</h2>\\n<blockquote>\\n<p>论文地址：</p>\\n</blockquote>\\n<p>本文提出AVSD中存在：不理解问题的情况下不加区分地抄袭输入文本，其原因是由于数据集中的答案句子通常包含输入文本中的单词，因此VGD(Video-grounded Dialogue)系统过度依赖于复制输入文本中的单词，希望这些单词与地面实况文本重叠，从而学习到虚假的相关性。</p>","autoDesc":true}');export{e as data};
