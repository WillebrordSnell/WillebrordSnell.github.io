"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1730],{6093:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,r]of t)a[e]=r;return a}},9125:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>u,data:()=>f});var r=a(362);const n=a.p+"assets/img/image.6b09a44a.png",i=a.p+"assets/img/image-2.11d815eb.png",o=a.p+"assets/img/image-1.987c40f5.png",s=a.p+"assets/img/image-3.3088f0fc.png",p=a.p+"assets/img/image-4.9eee59f2.png",g=a.p+"assets/img/image-5.27b48ba0.png",c=a.p+"assets/img/image-6.7a36532a.png",m=a.p+"assets/img/image-7.164bf2ab.png",h=a.p+"assets/img/image-8.0d1294fb.png",l=a.p+"assets/img/image-9.7a54f1ce.png",d=a.p+"assets/img/image-10.cc8372de.png",b={},u=(0,a(6093).A)(b,[["render",function(e,t){return(0,r.uX)(),(0,r.CE)("div",null,[...t[0]||(t[0]=[(0,r.Fv)('<h1 id="对比学习综述性质的记录" tabindex="-1"><a class="header-anchor" href="#对比学习综述性质的记录"><span>对比学习综述性质的记录</span></a></h1><h2 id="bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning" tabindex="-1"><a class="header-anchor" href="#bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning"><span>Bootstrap your own latent: A new approach to self-supervised Learning</span></a></h2><h4 id="论文地址-https-arxiv-org-abs-2006-07733" tabindex="-1"><a class="header-anchor" href="#论文地址-https-arxiv-org-abs-2006-07733"><span>论文地址:<a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2006.07733</a></span></a></h4><h4 id="项目代码-https-github-com-deepmind-deepmind-research-tree-master-byol" tabindex="-1"><a class="header-anchor" href="#项目代码-https-github-com-deepmind-deepmind-research-tree-master-byol"><span>项目代码:<a href="https://github.com/deepmind/deepmind-research/tree/master/byol" target="_blank" rel="noopener noreferrer">https://github.com/deepmind/deepmind-research/tree/master/byol</a></span></a></h4><p><img src="'+n+'" alt="框架" loading="lazy"><br> 训练的时候需要使online network不断逼近target network，其中t表示图像增强策略，f表示encoder，g表示projection</p><p><img src="'+i+'" alt="一个粒子" loading="lazy"><br><strong>为什么BYOL没有使用负样本和contrastive loss但是没有造成模型坍塌并且依然work呢？作者提到同一张图像在经过数据增强两次之后从encoder得到representation其实是关联性不大的，需要再经过一层MLP这种projection投影到更高维的latent space里面，在这个更高维的空间里面再经过contrastive learning使得两个vector有一定的空间关系，例如 尽可能靠近。</strong></p><p>显而易见的是，在online network里面经过projection层之后还有一层多的MLP层进行投影，这是因为毕竟这一张图片是经过两个不同的增强方式和网络(仅参数)，所以也不可能做到在projection空间中是完全相同的位置，因此需要再通过几层MLP把online network里面的vetor投影到target network。</p><figure><img src="'+o+'" alt="伪代码" tabindex="0" loading="lazy"><figcaption>伪代码</figcaption></figure><p><img src="'+s+'" alt="消融实验" loading="lazy"><br> 由于本文是没有通过负样本进行学习的，所以像SimCLR那种需要负样本的对比学习会对batch size更加敏感。但是<strong>对比学习通常是会对batch size和optimizer比较敏感的</strong>。</p><h2 id="exploring-simple-siamese-representation-learning" tabindex="-1"><a class="header-anchor" href="#exploring-simple-siamese-representation-learning"><span>Exploring Simple Siamese Representation Learning</span></a></h2><h4 id="论文地址-https-arxiv-org-abs-2011-10566" tabindex="-1"><a class="header-anchor" href="#论文地址-https-arxiv-org-abs-2011-10566"><span>论文地址:<a href="https://arxiv.org/abs/2011.10566" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2011.10566</a></span></a></h4><h4 id="非官方代码-https-github-com-patrickhua-simsiam" tabindex="-1"><a class="header-anchor" href="#非官方代码-https-github-com-patrickhua-simsiam"><span>非官方代码:<a href="https://github.com/PatrickHua/SimSiam" target="_blank" rel="noopener noreferrer">https://github.com/PatrickHua/SimSiam</a></span></a></h4><p>本文一大特色就是&quot;一切从简&quot;：没有使用负样本，没有用大的batch size，也没有用momentum encoder，但是依然能够防止模型坍塌从而达到一个不错的结果。</p><p><img src="'+p+'" alt="结构" loading="lazy"><br><img src="'+g+'" alt="伪代码" loading="lazy"><br> 本文这里两个encoder直接使用相同的权重，并且把BYOL中的encoder和projection合在一起(本文是CNN(encoder)+多层MLP(projection)的结构)，但是predictor依然保留</p><p><img src="'+c+'" alt="对比其他方法" loading="lazy"><br> SimCLR是使用经典的contrastive的方法，通过正负样本对比进行梯度回传<br> BYOL是通过momentum</p><p><img src="'+m+'" alt="消融实验" loading="lazy"><br> 使用了stop-grad(停止一侧网络传递loss)则很容易使得两个网络的参数一致造成网络坍塌</p><p><img src="'+h+'" alt="消融" loading="lazy"><br> predicator有用</p><p><img src="'+l+'" alt="消融" loading="lazy"><br> batch size的影响</p><p><img src="'+d+'" alt="假设" loading="lazy"><br> 为何work，SimSiam类似一种最大期望算法，首先固定η训练一个θ使得loss达到最小，再把θ锁死训练η，这也可以解释为什么通常在对比学习里面孪生网络通常target network需要使用缓慢的momentum更新参数</p>',19)])])}]]),f=JSON.parse('{"path":"/keyan/contrastiveLearning/contrastiveLearning.html","title":"对比学习综述性质的记录","lang":"zh-CN","frontmatter":{"date":"2023-10-01T00:00:00.000Z","category":["码头"],"tag":["对比学习"],"description":"对比学习综述性质的记录 Bootstrap your own latent: A new approach to self-supervised Learning 论文地址:https://arxiv.org/abs/2006.07733 项目代码:https://github.com/deepmind/deepmind-research/tree/m...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"对比学习综述性质的记录\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-10-01T00:00:00.000Z\\",\\"dateModified\\":\\"2025-09-30T15:22:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Mr.R\\",\\"url\\":\\"https://github.com/WillebrordSnell\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/keyan/contrastiveLearning/contrastiveLearning.html"}],["meta",{"property":"og:site_name","content":" "}],["meta",{"property":"og:title","content":"对比学习综述性质的记录"}],["meta",{"property":"og:description","content":"对比学习综述性质的记录 Bootstrap your own latent: A new approach to self-supervised Learning 论文地址:https://arxiv.org/abs/2006.07733 项目代码:https://github.com/deepmind/deepmind-research/tree/m..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-30T15:22:49.000Z"}],["meta",{"property":"article:tag","content":"对比学习"}],["meta",{"property":"article:published_time","content":"2023-10-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-30T15:22:49.000Z"}]]},"git":{"createdTime":1759245769000,"updatedTime":1759245769000,"contributors":[{"name":"R","username":"R","email":"799976781@qq.com","commits":1,"url":"https://github.com/R"}]},"readingTime":{"minutes":2.29,"words":688},"filePathRelative":"keyan/contrastiveLearning/_contrastiveLearning.md","excerpt":"\\n<h2>Bootstrap your own latent: A new approach to self-supervised Learning</h2>\\n<h4><a class=\\"header-anchor\\" href=\\"#论文地址-https-arxiv-org-abs-2006-07733\\"><span>论文地址:</span></a><a href=\\"https://arxiv.org/abs/2006.07733\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://arxiv.org/abs/2006.07733</a></h4>","autoDesc":true}')}}]);