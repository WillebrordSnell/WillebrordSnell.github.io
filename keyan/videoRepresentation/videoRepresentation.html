<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://github.com/WillebrordSnell/keyan/videoRepresentation/videoRepresentation.html"><meta property="og:site_name" content=" "><meta property="og:title" content="关于视频理解的论文收集(较新)"><meta property="og:description" content="关于视频理解的论文收集(较新) 本文可配合视频理解综述性质记录服用 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址：http://arxiv.org/abs/2103.14030 项目代码：https://github.com/microsoft/Swin-Transformer"><meta property="og:type" content="article"><meta property="og:image" content="https://github.com/WillebrordSnell/"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-10-28T08:20:32.000Z"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="关于视频理解的论文收集(较新)"><meta property="article:author" content="Mr.R"><meta property="article:tag" content="视频理解"><meta property="article:tag" content="视频对话"><meta property="article:published_time" content="2023-09-02T00:00:00.000Z"><meta property="article:modified_time" content="2023-10-28T08:20:32.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"关于视频理解的论文收集(较新)","image":["https://github.com/WillebrordSnell/"],"datePublished":"2023-09-02T00:00:00.000Z","dateModified":"2023-10-28T08:20:32.000Z","author":[{"@type":"Person","name":"Mr.R","url":"https://github.com/WillebrordSnell"}]}</script><title>关于视频理解的论文收集(较新) |  </title><meta name="description" content="关于视频理解的论文收集(较新) 本文可配合视频理解综述性质记录服用 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 论文地址：http://arxiv.org/abs/2103.14030 项目代码：https://github.com/microsoft/Swin-Transformer">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-512ce842.css" as="style"><link rel="stylesheet" href="/assets/style-512ce842.css">
    <link rel="modulepreload" href="/assets/app-578d0f47.js"><link rel="modulepreload" href="/assets/videoRepresentation.html-3d2ccc8b.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/videoRepresentation.html-86cfe134.js"><link rel="prefetch" href="/assets/index.html-d9458bfe.js" as="script"><link rel="prefetch" href="/assets/intro.html-2e5e4af5.js" as="script"><link rel="prefetch" href="/assets/slides.html-fd5a002b.js" as="script"><link rel="prefetch" href="/assets/index.html-bf1750b6.js" as="script"><link rel="prefetch" href="/assets/disable.html-28c49317.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-0f38acb2.js" as="script"><link rel="prefetch" href="/assets/markdown.html-d32d3131.js" as="script"><link rel="prefetch" href="/assets/page.html-d359f584.js" as="script"><link rel="prefetch" href="/assets/202309.html-74c2b100.js" as="script"><link rel="prefetch" href="/assets/202310.html-5598bf6d.js" as="script"><link rel="prefetch" href="/assets/202311.html-9a996c75.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-02808c56.js" as="script"><link rel="prefetch" href="/assets/index.html-aed730ad.js" as="script"><link rel="prefetch" href="/assets/dialog.html-67f127ce.js" as="script"><link rel="prefetch" href="/assets/video.html-b54bb940.js" as="script"><link rel="prefetch" href="/assets/index.html-a3a40393.js" as="script"><link rel="prefetch" href="/assets/index.html-3b3fc455.js" as="script"><link rel="prefetch" href="/assets/markdown01.html-8905e9eb.js" as="script"><link rel="prefetch" href="/assets/markdown02.html-054a564a.js" as="script"><link rel="prefetch" href="/assets/index.html-1c6f8d12.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-5bd139be.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-c152230c.js" as="script"><link rel="prefetch" href="/assets/videoDialog.html-6d0bd465.js" as="script"><link rel="prefetch" href="/assets/videoUnderstanding.html-d7d9f06e.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-ac9206cd.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-d5380b1b.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-2a3fce88.js" as="script"><link rel="prefetch" href="/assets/DDP.html-d779b619.js" as="script"><link rel="prefetch" href="/assets/trick.html-a4f8d089.js" as="script"><link rel="prefetch" href="/assets/index.html-a202766c.js" as="script"><link rel="prefetch" href="/assets/manual01.html-0a8da7f1.js" as="script"><link rel="prefetch" href="/assets/manual02.html-d7e0e274.js" as="script"><link rel="prefetch" href="/assets/manual03.html-fc2fcb88.js" as="script"><link rel="prefetch" href="/assets/index.html-94ec1b0f.js" as="script"><link rel="prefetch" href="/assets/documentnotes01.html-3e3bf028.js" as="script"><link rel="prefetch" href="/assets/documentnotes02.html-b0ab3628.js" as="script"><link rel="prefetch" href="/assets/documentnotes03.html-dffc6ca6.js" as="script"><link rel="prefetch" href="/assets/documentnotes04.html-5c8e168f.js" as="script"><link rel="prefetch" href="/assets/documentnotes05.html-e6c9c3ad.js" as="script"><link rel="prefetch" href="/assets/documentnotes06.html-8351650a.js" as="script"><link rel="prefetch" href="/assets/documentnotes07.html-a1156cd2.js" as="script"><link rel="prefetch" href="/assets/documentnotes08.html-4dd750d8.js" as="script"><link rel="prefetch" href="/assets/documentnotes09.html-c3df51ca.js" as="script"><link rel="prefetch" href="/assets/documentnotes10.html-22d25bd9.js" as="script"><link rel="prefetch" href="/assets/documentnotes11.html-739f34bd.js" as="script"><link rel="prefetch" href="/assets/404.html-33087db7.js" as="script"><link rel="prefetch" href="/assets/index.html-f543ab52.js" as="script"><link rel="prefetch" href="/assets/index.html-edca5983.js" as="script"><link rel="prefetch" href="/assets/index.html-9e922b59.js" as="script"><link rel="prefetch" href="/assets/index.html-5d327b0f.js" as="script"><link rel="prefetch" href="/assets/index.html-64a9f81e.js" as="script"><link rel="prefetch" href="/assets/index.html-1eb7a45d.js" as="script"><link rel="prefetch" href="/assets/index.html-3b8e3fc4.js" as="script"><link rel="prefetch" href="/assets/index.html-c24f66de.js" as="script"><link rel="prefetch" href="/assets/index.html-b28248cf.js" as="script"><link rel="prefetch" href="/assets/index.html-11a8a3d5.js" as="script"><link rel="prefetch" href="/assets/index.html-5f4f46c8.js" as="script"><link rel="prefetch" href="/assets/index.html-3fa1430b.js" as="script"><link rel="prefetch" href="/assets/index.html-50095564.js" as="script"><link rel="prefetch" href="/assets/index.html-170b7dac.js" as="script"><link rel="prefetch" href="/assets/index.html-4450e009.js" as="script"><link rel="prefetch" href="/assets/index.html-cd5496ff.js" as="script"><link rel="prefetch" href="/assets/index.html-bf5da18c.js" as="script"><link rel="prefetch" href="/assets/index.html-789dc91d.js" as="script"><link rel="prefetch" href="/assets/index.html-598316da.js" as="script"><link rel="prefetch" href="/assets/index.html-deffa58c.js" as="script"><link rel="prefetch" href="/assets/index.html-cc90af66.js" as="script"><link rel="prefetch" href="/assets/index.html-78e1edef.js" as="script"><link rel="prefetch" href="/assets/index.html-95c6778e.js" as="script"><link rel="prefetch" href="/assets/index.html-ef15ab18.js" as="script"><link rel="prefetch" href="/assets/index.html-de15c52f.js" as="script"><link rel="prefetch" href="/assets/index.html-3ffa64a8.js" as="script"><link rel="prefetch" href="/assets/index.html-0c4ebc78.js" as="script"><link rel="prefetch" href="/assets/index.html-7c39f58a.js" as="script"><link rel="prefetch" href="/assets/index.html-dc0cfaa1.js" as="script"><link rel="prefetch" href="/assets/index.html-e6f556de.js" as="script"><link rel="prefetch" href="/assets/index.html-996c738c.js" as="script"><link rel="prefetch" href="/assets/index.html-946da1ee.js" as="script"><link rel="prefetch" href="/assets/index.html-30e4e921.js" as="script"><link rel="prefetch" href="/assets/index.html-e2a85053.js" as="script"><link rel="prefetch" href="/assets/index.html-41d7be7d.js" as="script"><link rel="prefetch" href="/assets/index.html-36154924.js" as="script"><link rel="prefetch" href="/assets/index.html-334a54b6.js" as="script"><link rel="prefetch" href="/assets/index.html-2ae1afae.js" as="script"><link rel="prefetch" href="/assets/index.html-0c3db965.js" as="script"><link rel="prefetch" href="/assets/index.html-6b1ed3d2.js" as="script"><link rel="prefetch" href="/assets/index.html-4e97df2f.js" as="script"><link rel="prefetch" href="/assets/index.html-fb17bc04.js" as="script"><link rel="prefetch" href="/assets/intro.html-f8961ee1.js" as="script"><link rel="prefetch" href="/assets/slides.html-7469aa83.js" as="script"><link rel="prefetch" href="/assets/index.html-ac977551.js" as="script"><link rel="prefetch" href="/assets/disable.html-ee5debc6.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-d57eb777.js" as="script"><link rel="prefetch" href="/assets/markdown.html-430df109.js" as="script"><link rel="prefetch" href="/assets/page.html-ab1e8a76.js" as="script"><link rel="prefetch" href="/assets/202309.html-78a2a3f4.js" as="script"><link rel="prefetch" href="/assets/202310.html-cc599b3c.js" as="script"><link rel="prefetch" href="/assets/202311.html-64a6bad1.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-7c8fa641.js" as="script"><link rel="prefetch" href="/assets/index.html-6f9bec43.js" as="script"><link rel="prefetch" href="/assets/dialog.html-a8b5d05f.js" as="script"><link rel="prefetch" href="/assets/video.html-4c44c66b.js" as="script"><link rel="prefetch" href="/assets/index.html-2b169786.js" as="script"><link rel="prefetch" href="/assets/index.html-1f09a9e2.js" as="script"><link rel="prefetch" href="/assets/markdown01.html-8cfe1b58.js" as="script"><link rel="prefetch" href="/assets/markdown02.html-c2a802f5.js" as="script"><link rel="prefetch" href="/assets/index.html-4389560b.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-41b61713.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-ef8ff1f4.js" as="script"><link rel="prefetch" href="/assets/videoDialog.html-2f4b17a1.js" as="script"><link rel="prefetch" href="/assets/videoUnderstanding.html-83a81092.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-1e89f60a.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-3fc39ee6.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-ea4038ed.js" as="script"><link rel="prefetch" href="/assets/DDP.html-0a635fae.js" as="script"><link rel="prefetch" href="/assets/trick.html-0b82431f.js" as="script"><link rel="prefetch" href="/assets/index.html-fa96add0.js" as="script"><link rel="prefetch" href="/assets/manual01.html-df10d6b5.js" as="script"><link rel="prefetch" href="/assets/manual02.html-e61f972d.js" as="script"><link rel="prefetch" href="/assets/manual03.html-d8cec4af.js" as="script"><link rel="prefetch" href="/assets/index.html-491baa9d.js" as="script"><link rel="prefetch" href="/assets/documentnotes01.html-b5672be0.js" as="script"><link rel="prefetch" href="/assets/documentnotes02.html-96c6b508.js" as="script"><link rel="prefetch" href="/assets/documentnotes03.html-2a723f14.js" as="script"><link rel="prefetch" href="/assets/documentnotes04.html-93150a41.js" as="script"><link rel="prefetch" href="/assets/documentnotes05.html-571e7aca.js" as="script"><link rel="prefetch" href="/assets/documentnotes06.html-93e26d77.js" as="script"><link rel="prefetch" href="/assets/documentnotes07.html-801738e6.js" as="script"><link rel="prefetch" href="/assets/documentnotes08.html-639a65dc.js" as="script"><link rel="prefetch" href="/assets/documentnotes09.html-7b1dca8a.js" as="script"><link rel="prefetch" href="/assets/documentnotes10.html-41b307f1.js" as="script"><link rel="prefetch" href="/assets/documentnotes11.html-92410d17.js" as="script"><link rel="prefetch" href="/assets/404.html-09dadaa6.js" as="script"><link rel="prefetch" href="/assets/index.html-0fc912dd.js" as="script"><link rel="prefetch" href="/assets/index.html-848845fe.js" as="script"><link rel="prefetch" href="/assets/index.html-2009c92b.js" as="script"><link rel="prefetch" href="/assets/index.html-cae3a10f.js" as="script"><link rel="prefetch" href="/assets/index.html-489a16ba.js" as="script"><link rel="prefetch" href="/assets/index.html-26046414.js" as="script"><link rel="prefetch" href="/assets/index.html-da58a8b7.js" as="script"><link rel="prefetch" href="/assets/index.html-3e62d0da.js" as="script"><link rel="prefetch" href="/assets/index.html-31d13da9.js" as="script"><link rel="prefetch" href="/assets/index.html-cf8b6d9a.js" as="script"><link rel="prefetch" href="/assets/index.html-5d743a96.js" as="script"><link rel="prefetch" href="/assets/index.html-c91cadf4.js" as="script"><link rel="prefetch" href="/assets/index.html-4b3c4281.js" as="script"><link rel="prefetch" href="/assets/index.html-d6525c46.js" as="script"><link rel="prefetch" href="/assets/index.html-c99e3d73.js" as="script"><link rel="prefetch" href="/assets/index.html-3dd4672e.js" as="script"><link rel="prefetch" href="/assets/index.html-184fad45.js" as="script"><link rel="prefetch" href="/assets/index.html-c2b437c4.js" as="script"><link rel="prefetch" href="/assets/index.html-16dcfe72.js" as="script"><link rel="prefetch" href="/assets/index.html-db1d2274.js" as="script"><link rel="prefetch" href="/assets/index.html-c8f73a78.js" as="script"><link rel="prefetch" href="/assets/index.html-c37da273.js" as="script"><link rel="prefetch" href="/assets/index.html-d2e411ac.js" as="script"><link rel="prefetch" href="/assets/index.html-1ec62408.js" as="script"><link rel="prefetch" href="/assets/index.html-d3205b26.js" as="script"><link rel="prefetch" href="/assets/index.html-cc22f927.js" as="script"><link rel="prefetch" href="/assets/index.html-3c7a3837.js" as="script"><link rel="prefetch" href="/assets/index.html-2f03fe8d.js" as="script"><link rel="prefetch" href="/assets/index.html-311d67ea.js" as="script"><link rel="prefetch" href="/assets/index.html-290cd38b.js" as="script"><link rel="prefetch" href="/assets/index.html-a0b45f75.js" as="script"><link rel="prefetch" href="/assets/index.html-3c3849d7.js" as="script"><link rel="prefetch" href="/assets/index.html-920b2beb.js" as="script"><link rel="prefetch" href="/assets/index.html-6cebbb68.js" as="script"><link rel="prefetch" href="/assets/index.html-9e17e54c.js" as="script"><link rel="prefetch" href="/assets/index.html-d245d12f.js" as="script"><link rel="prefetch" href="/assets/index.html-78134756.js" as="script"><link rel="prefetch" href="/assets/index.html-cf4087a3.js" as="script"><link rel="prefetch" href="/assets/index.html-56ffc365.js" as="script"><link rel="prefetch" href="/assets/index.html-33cc9069.js" as="script"><link rel="prefetch" href="/assets/index.html-c6ba8575.js" as="script"><link rel="prefetch" href="/assets/auto-fe80bb03.js" as="script"><link rel="prefetch" href="/assets/index-2bf332f6.js" as="script"><link rel="prefetch" href="/assets/flowchart-c441f34d.js" as="script"><link rel="prefetch" href="/assets/mermaid.core-d218caec.js" as="script"><link rel="prefetch" href="/assets/highlight.esm-75b11b9d.js" as="script"><link rel="prefetch" href="/assets/markdown.esm-9d5bc2ce.js" as="script"><link rel="prefetch" href="/assets/math.esm-70a288c8.js" as="script"><link rel="prefetch" href="/assets/notes.esm-a106bb2c.js" as="script"><link rel="prefetch" href="/assets/reveal.esm-1a4c3ae7.js" as="script"><link rel="prefetch" href="/assets/search.esm-7e6792e2.js" as="script"><link rel="prefetch" href="/assets/zoom.esm-b83b91d0.js" as="script"><link rel="prefetch" href="/assets/VuePlayground-cab76975.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-5762295a.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><img class="vp-nav-logo" src="/logo.svg" alt=" "><!----><span class="vp-site-name hide-in-pad"> </span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="🔧 工具"><span class="title"><!---->🔧 工具</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>文档</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="Markdown" class="vp-link nav-link nav-link" href="/Tools/MarkDown.html"><!---->Markdown<!----></a></li><li class="dropdown-subitem"><a aria-label="资源整合" class="vp-link nav-link nav-link" href="/Tools/Resource.html"><!---->资源整合<!----></a></li></ul></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>工具</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="Git" class="vp-link nav-link nav-link" href="/Tools/Git.html"><!---->Git<!----></a></li></ul></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="  📑 码头"><span class="title"><!---->  📑 码头</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="视频理解" class="vp-link nav-link nav-link" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解<!----></a></li><li class="dropdown-item"><a aria-label="视频表征" class="vp-link nav-link active nav-link active" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->视频表征<!----></a></li><li class="dropdown-item"><a aria-label="视频对话" class="vp-link nav-link nav-link" href="/keyan/videoDialog/videoDialog.html"><!---->视频对话<!----></a></li><li class="dropdown-item"><a aria-label="对比学习" class="vp-link nav-link nav-link" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习<!----></a></li><li class="dropdown-item"><a aria-label="多模态" class="vp-link nav-link nav-link" href="/keyan/multiModal/multiModal.html"><!---->多模态<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="  🧫 炉"><span class="title"><!---->  🧫 炉</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="单机多卡DDP" class="vp-link nav-link nav-link" href="/train/DDP/DDP.html"><!---->单机多卡DDP<!----></a></li><li class="dropdown-item"><a aria-label="AVSD" class="vp-link nav-link nav-link" href="/train/AVSD/AVSD.html"><!---->AVSD<!----></a></li><li class="dropdown-item"><a aria-label="奇淫技巧" class="vp-link nav-link nav-link" href="/train/trick/trick.html"><!---->奇淫技巧<!----></a></li></ul></button></div></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><!----><div class="nav-item hide-in-mobile"><button type="button" class="outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="outlook-dropdown"><!----></div></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><!--[--><a aria-label="视频理解综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="关于视频理解的论文收集(较新)" class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active nav-link active vp-sidebar-link vp-sidebar-page active" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->关于视频理解的论文收集(较新)<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#swin-transformer-hierarchical-vision-transformer-using-shifted-windows"><!---->Swin Transformer: Hierarchical Vision Transformer using Shifted Windows<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Is Space-Time Attention All You Need for Video Understanding?" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#is-space-time-attention-all-you-need-for-video-understanding"><!---->Is Space-Time Attention All You Need for Video Understanding?<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="MViT: Multiscale Vision Transformers" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#mvit-multiscale-vision-transformers"><!---->MViT: Multiscale Vision Transformers<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#vatt-transformers-for-multimodal-self-supervised-learning-from-raw-video-audio-and-text"><!---->VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="SlowFast Networks for Video Recognition" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#slowfast-networks-for-video-recognition"><!---->SlowFast Networks for Video Recognition<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="X3D: Expanding Architectures for Efficient Video Recognition" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#x3d-expanding-architectures-for-efficient-video-recognition"><!---->X3D: Expanding Architectures for Efficient Video Recognition<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Self-supervised Video Transformer" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#self-supervised-video-transformer"><!---->Self-supervised Video Transformer<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#with-a-little-help-from-my-friends-nearest-neighbor-contrastive-learning-of-visual-representations"><!---->With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Solving Inefficiency of Self-supervised Representation Learning" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#solving-inefficiency-of-self-supervised-representation-learning"><!---->Solving Inefficiency of Self-supervised Representation Learning<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples"><!---->VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="ExtreMA: Extreme Masking for Learning Instance and Distributed Visual Representations" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#extrema-extreme-masking-for-learning-instance-and-distributed-visual-representations"><!---->ExtreMA: Extreme Masking for Learning Instance and Distributed Visual Representations<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Broaden Your Views for Self-Supervised Video Learning" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#broaden-your-views-for-self-supervised-video-learning"><!---->Broaden Your Views for Self-Supervised Video Learning<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Learning by Aligning Videos in Time" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#learning-by-aligning-videos-in-time"><!---->Learning by Aligning Videos in Time<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Vi2CLR: Video and Image for Visual Contrastive Learning of Representation" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#vi2clr-video-and-image-for-visual-contrastive-learning-of-representation"><!---->Vi2CLR: Video and Image for Visual Contrastive Learning of Representation<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#violet-end-to-end-video-language-transformers-with-masked-visual-token-modeling"><!---->VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="BEVT: BERT Pretraining of Video Transformers" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#bevt-bert-pretraining-of-video-transformers"><!---->BEVT: BERT Pretraining of Video Transformers<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#transrank-self-supervised-video-representation-learning-via-ranking-based-transformation-recognition"><!---->TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Probabilistic Representations for Video Contrastive Learning: ProViCo" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoRepresentation/videoRepresentation.html#probabilistic-representations-for-video-contrastive-learning-provico"><!---->Probabilistic Representations for Video Contrastive Learning: ProViCo<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul><!--]--></li><li><!--[--><a aria-label="视频对话方向大论文性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/videoDialog/videoDialog.html"><!---->视频对话方向大论文性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="对比学习综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="多模态方向综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/multiModal/multiModal.html"><!---->多模态方向综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->关于视频理解的论文收集(较新)</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/WillebrordSnell" target="_blank" rel="noopener noreferrer">Mr.R</a></span><span property="author" content="Mr.R"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-09-02T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 14 分钟</span><meta property="timeRequired" content="PT14M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category0 clickable" role="navigation">码头</span><!--]--><meta property="articleSection" content="码头"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag4 clickable" role="navigation">视频理解</span><span class="page-tag-item tag5 clickable" role="navigation">视频对话</span><!--]--><meta property="keywords" content="视频理解,视频对话"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#swin-transformer-hierarchical-vision-transformer-using-shifted-windows">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#is-space-time-attention-all-you-need-for-video-understanding">Is Space-Time Attention All You Need for Video Understanding?</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#mvit-multiscale-vision-transformers">MViT: Multiscale Vision Transformers</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#vatt-transformers-for-multimodal-self-supervised-learning-from-raw-video-audio-and-text">VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#slowfast-networks-for-video-recognition">SlowFast Networks for Video Recognition</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#x3d-expanding-architectures-for-efficient-video-recognition">X3D: Expanding Architectures for Efficient Video Recognition</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#self-supervised-video-transformer">Self-supervised Video Transformer</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#with-a-little-help-from-my-friends-nearest-neighbor-contrastive-learning-of-visual-representations">With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#solving-inefficiency-of-self-supervised-representation-learning">Solving Inefficiency of Self-supervised Representation Learning</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples">VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#extrema-extreme-masking-for-learning-instance-and-distributed-visual-representations">ExtreMA: Extreme Masking for Learning Instance and Distributed Visual Representations</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#broaden-your-views-for-self-supervised-video-learning">Broaden Your Views for Self-Supervised Video Learning</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#learning-by-aligning-videos-in-time">Learning by Aligning Videos in Time</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#vi2clr-video-and-image-for-visual-contrastive-learning-of-representation">Vi2CLR: Video and Image for Visual Contrastive Learning of Representation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#violet-end-to-end-video-language-transformers-with-masked-visual-token-modeling">VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#bevt-bert-pretraining-of-video-transformers">BEVT: BERT Pretraining of Video Transformers</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#transrank-self-supervised-video-representation-learning-via-ranking-based-transformation-recognition">TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#probabilistic-representations-for-video-contrastive-learning-provico">Probabilistic Representations for Video Contrastive Learning: ProViCo</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="关于视频理解的论文收集-较新" tabindex="-1"><a class="header-anchor" href="#关于视频理解的论文收集-较新" aria-hidden="true">#</a> 关于视频理解的论文收集(较新)</h1><p>本文可配合<a href="/keyan/videoUnderstanding/_videoUnderstanding.html" class="">视频理解综述性质记录</a>服用</p><h2 id="swin-transformer-hierarchical-vision-transformer-using-shifted-windows" tabindex="-1"><a class="header-anchor" href="#swin-transformer-hierarchical-vision-transformer-using-shifted-windows" aria-hidden="true">#</a> Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2103.14030" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2103.14030<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/microsoft/Swin-Transformer" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/Swin-Transformer<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>通过与CNN相似的分层结构来处理图片，使得模型能够灵活处理不同尺度的图片</p><figure><img src="/assets/image-674fdb37.png" alt="与Vit的异同" tabindex="0" loading="lazy"><figcaption>与Vit的异同</figcaption></figure><figure><img src="/assets/image-1-67298078.png" alt="网络结构" tabindex="0" loading="lazy"><figcaption>网络结构</figcaption></figure><figure><img src="/assets/image-2-1cd8ecd1.png" alt="W-MSA和SW-MSA" tabindex="0" loading="lazy"><figcaption>W-MSA和SW-MSA</figcaption></figure><p>W-MAS 只在红框内做self-Attention，但显然易见的就是与其他的local window没有任何交流，因此patch之间没有信息传递<br> SW-MSA 如图Layer+1，同样是在红框内做attention，但是由于区域划分有所变化，能够与前一层的其他patch有信息传递。但如左上角的patch，划分后的区域太小，不够token的数量</p><figure><img src="/assets/image-3-4c00eadd.png" alt="SW-MSA “改良”" tabindex="0" loading="lazy"><figcaption>SW-MSA “改良”</figcaption></figure><h2 id="is-space-time-attention-all-you-need-for-video-understanding" tabindex="-1"><a class="header-anchor" href="#is-space-time-attention-all-you-need-for-video-understanding" aria-hidden="true">#</a> Is Space-Time Attention All You Need for Video Understanding?</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2102.05095" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2102.05095<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/facebookresearch/TimeSformer" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/TimeSformer<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文的Space-Time思想类似之前的工作<a href="/keyan/videoUnderstanding/_videoUnderstanding.md/#r21da-closer-look-at-spatiotemporal-convolutions-for-action-recognition" class="">R(2+1)D</a>：通过拆分时间和空间减少时间复杂度<br><img src="/assets/image-21-fcda20b5.png" alt="框架" loading="lazy"></p><figure><img src="/assets/image-22-35bd1061.png" alt="一个粒子" tabindex="0" loading="lazy"><figcaption>一个粒子</figcaption></figure><p>上图的蓝色patch表示目前主要关注该帧该patch，其余颜色表示与蓝色patch的互动</p><p>从ViT很自然地可以想到，如果在时间维度上加上一个joint Space-Time Att就可以处理视频数据，但是这样的话随scequence的增加，其计算代价是呈指数形式增长的(视频稍微长一点，像素高一点的话，训练时的计算计算代价往往是难以承受的)。</p><figure><img src="/assets/image-24-d77d8234.png" alt="难以接受" tabindex="0" loading="lazy"><figcaption>难以接受</figcaption></figure><p>由此可以先做一个方向的(先Time再Space)，但是作者发现交换了Time和Space的位置，甚至是两个batch分别做，不过效果不如现在的好</p><figure><img src="/assets/image-23-9d5c35f0.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><p>Divided Space-Time效果最好</p><h2 id="mvit-multiscale-vision-transformers" tabindex="-1"><a class="header-anchor" href="#mvit-multiscale-vision-transformers" aria-hidden="true">#</a> MViT: Multiscale Vision Transformers</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2104.11227" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2104.11227<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/facebookresearch/SlowFast" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/SlowFast<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>Swin Transformer的后续工作，通过构建不同尺度的transformer实现各种下游任务的性能提升。相较于Swim Transformer，该工作关注点更多在于处理视频方面，并且更加generalize</p><figure><img src="/assets/image-4-e8b1d56f.png" alt="总览" tabindex="0" loading="lazy"><figcaption>总览</figcaption></figure><figure><img src="/assets/image-5-cc5ec93c.png" alt="Pooling Attention" tabindex="0" loading="lazy"><figcaption>Pooling Attention</figcaption></figure><figure><img src="/assets/image-6-d07c896f.png" alt="一个粒子" tabindex="0" loading="lazy"><figcaption>一个粒子</figcaption></figure><figure><img src="/assets/image-7-fe937dda.png" alt="如何在减少token长度的情况下，不损失图像信息：增加channel" tabindex="0" loading="lazy"><figcaption>如何在减少token长度的情况下，不损失图像信息：增加channel</figcaption></figure><p>作者在没减少4倍大小的情况下会增加2倍的channel</p><h2 id="vatt-transformers-for-multimodal-self-supervised-learning-from-raw-video-audio-and-text" tabindex="-1"><a class="header-anchor" href="#vatt-transformers-for-multimodal-self-supervised-learning-from-raw-video-audio-and-text" aria-hidden="true">#</a> VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2104.11178" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2104.11178<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/google-research/google-research/tree/master/vatt" target="_blank" rel="noopener noreferrer">https://github.com/google-research/google-research/tree/master/vatt<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><figure><img src="/assets/image-10-9b98e87f.png" alt="框架" tabindex="0" loading="lazy"><figcaption>框架</figcaption></figure><p>作者认为视频、音频、文本描述之间的有很大联系的，例如看到汽车就能想到引擎的轰鸣声和行驶的风噪声。本文一开始的思路就是将三个模态的信号做成小的patch(sequence，然后投成embedding扔到transformer里面，再做projection投入相同的latent space里面最后做后续的下游任务 ，本文大致就是大力出奇迹的效果<br> 具体是如何将视频、音频信号变成embedding输入到transformer呢，视频可以理解成一个一个cube + Temporal enc + Spatial enc 按照时间或空间切成小的patch通过linear projection投到一个vector里面；音频就直接切成一段一段再通过linear projection投影就行。由此就可以得到对应的embedding vector。但在本文中，文本的处理是通过one-hot进行投影的。<br> 特别的是，本文提出Modality-Specific:3 Trs和Modality-Agnostic:1 Tr分别通过三个tansformer模块和一个去学习表征(实验证明还是分开用不同的transformer效果比较好)，但由于video维度非常高，切块之后的得到token数量太多，这样会导致训练的时间和需要的显存也就是计算资源需求会很大。要解决这一问题作者提出DropToken，核心思想和Dropout差不多，也就是随机去掉一些input token(一般是50%)，以此解决高分辨率原始数据的问题。<br> 文章里面提到一个小的trick：因为video和audio是贯穿整个视频的，因此这两个模态之间的对齐比较简单，直接使用NCE Loss即可；但是video和text，由于文本会比较稀疏，作者在此采用MIL-NCE，大致思想就是即使video当前时刻没有文本，但是也可以通过该时刻前后的文本作为正样本解决文本稀疏/对应不上的问题</p><figure><img src="/assets/image-11-0b790258.png" alt="模态如何投影到一个空间中" tabindex="0" loading="lazy"><figcaption>模态如何投影到一个空间中</figcaption></figure><p>如果用相同结构的tansformer但是通过用一个distillation做信息交互会不会有搞头</p><h2 id="slowfast-networks-for-video-recognition" tabindex="-1"><a class="header-anchor" href="#slowfast-networks-for-video-recognition" aria-hidden="true">#</a> SlowFast Networks for Video Recognition</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/1812.03982" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1812.03982<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/facebookresearch/SlowFast" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/SlowFast<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文研究动机是人体的视觉系统有p(80%)、m两种细胞，分别处理静态的场景信息和高频的运动信息。因此作者提出一种通用做视频识别的框架，简单来说就是用两个path：一个以较低的频率处理视频语义信息，另一个以较高的频率处理运动信息，最后将二者合并。</p><figure><img src="/assets/image-12-56d0dbaa.png" alt="框架" tabindex="0" loading="lazy"><figcaption>框架</figcaption></figure><p>Slow pathway：以较低的频率提取一个庞大的、深的网络处理空间语义信息<br> Fast pathway：高频率地通过浅的网络处理运动信息</p><figure><img src="/assets/image-13-8805aaea.png" alt="一个粒子" tabindex="0" loading="lazy"><figcaption>一个粒子</figcaption></figure><p>以Slow pathway为例(每16帧取一张图片，fast是每2帧)，知道res4这一层，前面的所有层都是不处理时间轴的信息，作者认为时间轴的信息水太深了，前面几层浅层network把握不住，因此只在最后基层加上时间轴的信息做3D的conv。不过由于Fast需要捕捉运动信息，因此在一开始就需要加上时间轴的数据</p><figure><img src="/assets/image-14-11f3cb02.png" alt="融合两个path的数据" tabindex="0" loading="lazy"><figcaption>融合两个path的数据</figcaption></figure><figure><img src="/assets/image-16-83d123b5.png" alt="融合方式的消融实验" tabindex="0" loading="lazy"><figcaption>融合方式的消融实验</figcaption></figure><p>1：直接拼接 2：直接采一个样本 3：遇事不决直接3D Conv</p><figure><img src="/assets/image-15-d0b3eb77.png" alt="是否需要Fast path的消融实验" tabindex="0" loading="lazy"><figcaption>是否需要Fast path的消融实验</figcaption></figure><p>其中最右边，即使将Slow path处理的帧率提升一倍其性能也比不上Slow-Fast的组合，说明该结论确实有效。并且添加一个Fast path在没有加大多少计算难度的情况下均有不错的性能提升(2%起跳)</p><figure><img src="/assets/image-17-d5df1dae.png" alt="视频质量消融实验" tabindex="0" loading="lazy"><figcaption>视频质量消融实验</figcaption></figure><p>说明由于Fast path处理的是运动信息，因此把其转换成灰度、光流或者直接把图像缩小一倍，其性能也大差不差</p><h2 id="x3d-expanding-architectures-for-efficient-video-recognition" tabindex="-1"><a class="header-anchor" href="#x3d-expanding-architectures-for-efficient-video-recognition" aria-hidden="true">#</a> X3D: Expanding Architectures for Efficient Video Recognition</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2004.04730" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2004.04730<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/facebookresearch/SlowFast" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/SlowFast<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>核心思想就是从一个小的2D image classification开始将模型扩大，最后让它变成一个高效的视频理解、动作识别等的网络结构</p><figure><img src="/assets/image-18-9267cbc4.png" alt="结构" tabindex="0" loading="lazy"><figcaption>结构</figcaption></figure><p>帧率扩充，帧率增加，分辨率，...</p><figure><img src="/assets/image-20-8f87e020.png" alt="一个粒子" tabindex="0" loading="lazy"><figcaption>一个粒子</figcaption></figure><p>随着模型增大，regularzation也需要变强，并且之后甚至batch size也要调整(太花活了，整不来)</p><h2 id="self-supervised-video-transformer" tabindex="-1"><a class="header-anchor" href="#self-supervised-video-transformer" aria-hidden="true">#</a> Self-supervised Video Transformer</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/2112.01514" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2112.01514<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/kahnchana/svt" target="_blank" rel="noopener noreferrer">https://github.com/kahnchana/svt<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文属于填空题性质的文章，将之前成熟的技术结合起来放在空白领域上的一种应用。如MOCO提出大型的memory bank(后已取消)，或那些无负样本学习的策略(能够减少Batch size大小，提高普适性)，该类对比学习之间的区别一个很大的重点在于如何构建loss(是否使用负样本、如何衡量representaion之间的距离、如何确定positive sample)，例如 nnclr(with little help with my friends)。</p><p>之间对比学习应用在图像上的时候，无非就是通过一些数据增强构建正样本对，但是在视频上由于多了时间维度，那么可操作空间就大很多了</p><p>这篇论文本身所有的正样本对都来自于一个视频的各个片段</p><figure><img src="/assets/image-25-8c569bc9.png" alt="示例" tabindex="0" loading="lazy"><figcaption>示例</figcaption></figure><figure><img src="/assets/image-26-33cc8007.png" alt="how to do" tabindex="0" loading="lazy"><figcaption>how to do</figcaption></figure><figure><img src="/assets/image-27-731cbe3b.png" alt="架构" tabindex="0" loading="lazy"><figcaption>架构</figcaption></figure><p>teacher模型通过EMA(加权平均)的方式更新梯度</p><h2 id="with-a-little-help-from-my-friends-nearest-neighbor-contrastive-learning-of-visual-representations" tabindex="-1"><a class="header-anchor" href="#with-a-little-help-from-my-friends-nearest-neighbor-contrastive-learning-of-visual-representations" aria-hidden="true">#</a> With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2104.14548" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2104.14548<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><figure><img src="/assets/image-28-8a0b9679.png" alt="框架" tabindex="0" loading="lazy"><figcaption>框架</figcaption></figure><figure><img src="/assets/image-29-608deafe.png" alt="loss" tabindex="0" loading="lazy"><figcaption>loss</figcaption></figure><p>本文的关键思想在于通过NNs这个最近邻的过程能够选择一个更加generalize的data augment的过程。因为通过这种强行将另一张图像的representation视作为这张图像的representation，一方面相当于做了更加generalize的data augment，另一方面虽然另一张图片也是通过data augment但是不至于太离谱</p><h2 id="solving-inefficiency-of-self-supervised-representation-learning" tabindex="-1"><a class="header-anchor" href="#solving-inefficiency-of-self-supervised-representation-learning" aria-hidden="true">#</a> Solving Inefficiency of Self-supervised Representation Learning</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2104.08760" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2104.08760<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/wanggrun/triplet" target="_blank" rel="noopener noreferrer">https://github.com/wanggrun/triplet<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><figure><img src="/assets/image-30-d7ca8ddd.png" alt="粒子" tabindex="0" loading="lazy"><figcaption>粒子</figcaption></figure><p>作者认为一般来说提供positive和negative的sample数量是非常不平衡的，也就很容易导致over cluster的现象</p><figure><img src="/assets/image-31-5a9f3f3b.png" alt="loss" tabindex="0" loading="lazy"><figcaption>loss</figcaption></figure><p>核心思想就是只区分最近的negative(hardest)， 但这样就会导致一个问题：会误认为positive sample是negative的</p><figure><img src="/assets/image-32-a3fa084e.png" alt="loss" tabindex="0" loading="lazy"><figcaption>loss</figcaption></figure><p>直接往后看几位，解决了痛击友军的问题</p><h2 id="videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples" tabindex="-1"><a class="header-anchor" href="#videomoco-contrastive-video-representation-learning-with-temporally-adversarial-examples" aria-hidden="true">#</a> VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2103.05905" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2103.05905<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/tinapan-pt/VideoMoCo" target="_blank" rel="noopener noreferrer">https://github.com/tinapan-pt/VideoMoCo<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>核心贡献：1.Temporally Adversarial Learning 2.Temporal Decay</p><figure><img src="/assets/image-34-a37d7726.png" alt="Temporally Adversarial Learning" tabindex="0" loading="lazy"><figcaption>Temporally Adversarial Learning</figcaption></figure><p>Generator通过LSTM产生一个每一帧的重要性分数，再通过Drop out丢弃关键帧。混淆Discriminator判别出不是同一个视频的误判，从而形成对抗<br> 换种角度理解就是模型在搜索一种好的data augmentation去帮助representation learning学到好的表征。<br> (现在网络结构、loss有点卷不大动了，大家都把目光放在通过对抗学习去解决data augment的问题上了)</p><figure><img src="/assets/image-35-f03373b3.png" alt="Temporal Decay" tabindex="0" loading="lazy"><figcaption>Temporal Decay</figcaption></figure><p>摒弃之前FIFO的思想，通过一个系数减弱先前的样本影响因子</p><h2 id="extrema-extreme-masking-for-learning-instance-and-distributed-visual-representations" tabindex="-1"><a class="header-anchor" href="#extrema-extreme-masking-for-learning-instance-and-distributed-visual-representations" aria-hidden="true">#</a> ExtreMA: Extreme Masking for Learning Instance and Distributed Visual Representations</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2206.04667" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2206.04667<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>这篇是一个组合性质的paper，大致就是把masking，MAE，BERT和对比学习结合在一起的工作，整体结构是类似BYOL</p><figure><img src="/assets/image-36-0a3e0fde.png" alt="结构" tabindex="0" loading="lazy"><figcaption>结构</figcaption></figure><p>但是本文在mask的时候是mask掉大部分，只把剩下的结构放进transformer里面，这样的话相应的梯度计算量就减少了，同时这种random masking可以视为一种好的data augmentation能够提供一种相互独立并且互补的patch。<br> 并且由于online network传梯度但是target network不传递，相当于是把小的patch拉向这个整个patch的一个集合</p><figure><img src="/assets/image-38-bc026ae6.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><figure><img src="/assets/image-37-88e5f20a.png" alt="粒子" tabindex="0" loading="lazy"><figcaption>粒子</figcaption></figure><h2 id="broaden-your-views-for-self-supervised-video-learning" tabindex="-1"><a class="header-anchor" href="#broaden-your-views-for-self-supervised-video-learning" aria-hidden="true">#</a> Broaden Your Views for Self-Supervised Video Learning</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2103.16559" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2103.16559<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><figure><img src="/assets/image-39-7f49bf3f.png" alt="两种view" tabindex="0" loading="lazy"><figcaption>两种view</figcaption></figure><p>与之前的video moco的drop frame不同，本文是通过narrow view(时间较短的clips)和broad view(时间长的clips)</p><figure><img src="/assets/image-40-89e918b8.png" alt="框架" tabindex="0" loading="lazy"><figcaption>框架</figcaption></figure><figure><img src="/assets/image-42-48d5ea4e.png" alt="参数共享" tabindex="0" loading="lazy"><figcaption>参数共享</figcaption></figure><p>作者通过实验发现如图的配置是最好的(有些模块参数是相同的，有些是不同的)，也就是说projector和predicator共享是能取到最好性能的</p><figure><img src="/assets/image-41-99d2e4ce.png" alt="syncing" tabindex="0" loading="lazy"><figcaption>syncing</figcaption></figure><p>不可用syncing(同一时间点开始)，如果这样做的话，从narrow到broad就是一个简单的预测任务，而从broad到narrow就仅需要去掉一些帧就可以了</p><h2 id="learning-by-aligning-videos-in-time" tabindex="-1"><a class="header-anchor" href="#learning-by-aligning-videos-in-time" aria-hidden="true">#</a> Learning by Aligning Videos in Time</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2103.17260" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2103.17260<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><figure><img src="/assets/image-48-29f40197.png" alt="框架" tabindex="0" loading="lazy"><figcaption>框架</figcaption></figure><p>将两个视频有相同语义信息的一帧在embedding space应该尽量接近</p><figure><img src="/assets/image-49-8b34e6ea.png" alt="DTW &amp; Soft-DTW" tabindex="0" loading="lazy"><figcaption>DTW &amp; Soft-DTW</figcaption></figure><p>D就是两个视频各帧的embedding的第二范式距离,由于min操作不能得到梯度，那通过soft操作使得梯度得以计算</p><figure><img src="/assets/image-50-fd5caa07.png" alt="IDM" tabindex="0" loading="lazy"><figcaption>IDM</figcaption></figure><p>如果只有DTW，那么模型直接把所有视频都映射到一个embedding不就直接得到一个很低的loss然后坍塌了，因此需要IDM(对于每个视频序列单独加的一个loss)<br> W的意义在于，如果视频的两帧图像在时间序列上差距越远，那么在计算权重上就应该越小<br> 但这个loss仍然有缺陷，因为需要maxmize I(X)，也就是说如果我不管W，直接所有的S(X)取非常大不就又是一个shotcut了吗</p><figure><img src="/assets/image-51-dade7ce1.png" alt="改进IDM" tabindex="0" loading="lazy"><figcaption>改进IDM</figcaption></figure><p>本文提出一个contrastive的idea，也就是通过一个windows size阈值，取5(时间比较近)，6(时间比较远)其中一个loss<br> note：公式7这个loss没有考虑由于视频时间长使得loss会增加，实际上应该还需调整</p><h2 id="vi2clr-video-and-image-for-visual-contrastive-learning-of-representation" tabindex="-1"><a class="header-anchor" href="#vi2clr-video-and-image-for-visual-contrastive-learning-of-representation" aria-hidden="true">#</a> Vi2CLR: Video and Image for Visual Contrastive Learning of Representation</h2><blockquote><p>论文地址：<a href="https://ieeexplore.ieee.org/abstract/document/9710567" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/abstract/document/9710567<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><figure><img src="/assets/image-43-088ba100.png" alt="框架" tabindex="0" loading="lazy"><figcaption>框架</figcaption></figure><p>本文一个亮点就是将video representation和image representation串接起来然后送到一个MLP里面得到一个新的joint representation ，再通过聚类的思想得到正负样本</p><figure><img src="/assets/image-44-31a50793.png" alt="joint representation" tabindex="0" loading="lazy"><figcaption>joint representation</figcaption></figure><figure><img src="/assets/image-45-f4eaa844.png" alt="video&amp;image" tabindex="0" loading="lazy"><figcaption>video&amp;image</figcaption></figure><figure><img src="/assets/image-46-5a0fdaca.png" alt="算法" tabindex="0" loading="lazy"><figcaption>算法</figcaption></figure><figure><img src="/assets/image-47-0a1bc205.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><p>注意通过FINCH聚类的方法要比Kmeans效果好</p><h2 id="violet-end-to-end-video-language-transformers-with-masked-visual-token-modeling" tabindex="-1"><a class="header-anchor" href="#violet-end-to-end-video-language-transformers-with-masked-visual-token-modeling" aria-hidden="true">#</a> VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2111.12681" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2111.12681<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/tsujuifu/pytorch_violet" target="_blank" rel="noopener noreferrer">https://github.com/tsujuifu/pytorch_violet<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>这篇文章就是把mask、bert这些技术放到video和language共同训练的模型上，并且灵活地运用了transformers</p><figure><img src="/assets/image-52-9bcf41b9.png" alt="task" tabindex="0" loading="lazy"><figcaption>task</figcaption></figure><p>pra-train部分通过MLM和MVM训练，再做两个下游任务：video QA和Text-Video Retrieval</p><figure><img src="/assets/image-53-9bf6f9d2.png" alt="框架" tabindex="0" loading="lazy"><figcaption>框架</figcaption></figure><p>模型需要预测，mask掉的patch和文字</p><figure><img src="/assets/image-54-8a012e45.png" alt="LOSS" tabindex="0" loading="lazy"><figcaption>LOSS</figcaption></figure><p>dVAE：每一个token对应字典里面一个feature，仅需要预测出来字典里面对应的index即可</p><figure><img src="/assets/image-55-313635bb.png" alt="mask策略" tabindex="0" loading="lazy"><figcaption>mask策略</figcaption></figure><figure><img src="/assets/image-56-04a0616e.png" alt="video QA实验" tabindex="0" loading="lazy"><figcaption>video QA实验</figcaption></figure><figure><img src="/assets/image-57-27bbc465.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><figure><img src="/assets/image-58-ee72239e.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><h2 id="bevt-bert-pretraining-of-video-transformers" tabindex="-1"><a class="header-anchor" href="#bevt-bert-pretraining-of-video-transformers" aria-hidden="true">#</a> BEVT: BERT Pretraining of Video Transformers</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2112.01529" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2112.01529<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/xyzforever/BEVT" target="_blank" rel="noopener noreferrer">https://github.com/xyzforever/BEVT<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p></blockquote><figure><img src="/assets/image-59-bd7dd9f1.png" alt="BERT架构" tabindex="0" loading="lazy"><figcaption>BERT架构</figcaption></figure><figure><img src="/assets/image-60-ffccfaab.png" alt="BERT task" tabindex="0" loading="lazy"><figcaption>BERT task</figcaption></figure><figure><img src="/assets/image-61-b541e408.png" alt="BEVT" tabindex="0" loading="lazy"><figcaption>BEVT</figcaption></figure><p>对于视频无非就是得到Spatial representation和Temporal dynamics，该文章通过mask掉图像和视频两个分支进行image stream和video stream两个流去预测原始的图像和视频，具体来说是预测原始的embedding</p><figure><img src="/assets/image-62-5273bfa6.png" alt="MLM/MVM" tabindex="0" loading="lazy"><figcaption>MLM/MVM</figcaption></figure><p>本文是先用一个训练好的tokenizer(VQ-VAE)得到一个个token，最终是需要预测这些token而非像素。<br> 区别于BERT，本文是选取blockwise的mask(如果mask掉的比例不够，继续选择一个新的block进行mask，直到mask的面积达到期望要求)，扩展到视频层面就是整个一个tube都被mask掉了。<br> (本文中，image encoder和video encoder的参数是共享的)</p><figure><img src="/assets/image-63-277e2156.png" alt="decoder设计" tabindex="0" loading="lazy"><figcaption>decoder设计</figcaption></figure><figure><img src="/assets/image-64-531fa973.png" alt="loss设计" tabindex="0" loading="lazy"><figcaption>loss设计</figcaption></figure><figure><img src="/assets/image-65-dd53058d.png" alt="实验部分" tabindex="0" loading="lazy"><figcaption>实验部分</figcaption></figure><p>本文一个有意思的实验就是通过将视频所有帧都用同一帧代替/打乱时间线上的顺序，验证了本模型能够理解视频上下文的能力</p><figure><img src="/assets/image-66-84154cc5.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><h2 id="transrank-self-supervised-video-representation-learning-via-ranking-based-transformation-recognition" tabindex="-1"><a class="header-anchor" href="#transrank-self-supervised-video-representation-learning-via-ranking-based-transformation-recognition" aria-hidden="true">#</a> TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2205.02028" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2205.02028<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/kennymckormick/TransRank" target="_blank" rel="noopener noreferrer">https://github.com/kennymckormick/TransRank<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> （暂未上传）</p></blockquote><figure><img src="/assets/image-67-0cb26304.png" alt="background" tabindex="0" loading="lazy"><figcaption>background</figcaption></figure><p>一般来说这种self-supervise模型需要通过一个比较好的task来辅助模型学到representation。(例如这个SpeedNet通过模型来判断视频是否被加速来学习视频的表征、3D-RotNet将视频旋转再通过classifi而判断)，另一种方法就是通过contrastive learning通过instance discrimination来学习表征</p><figure><img src="/assets/image-68-3b16e5d0.png" alt="hinge loss" tabindex="0" loading="lazy"><figcaption>hinge loss</figcaption></figure><p>这种hinge loss能够帮助模型做排序任务</p><figure><img src="/assets/image-69-aa651cf4.png" alt="分析" tabindex="0" loading="lazy"><figcaption>分析</figcaption></figure><p>(需要精读)</p><h2 id="probabilistic-representations-for-video-contrastive-learning-provico" tabindex="-1"><a class="header-anchor" href="#probabilistic-representations-for-video-contrastive-learning-provico" aria-hidden="true">#</a> Probabilistic Representations for Video Contrastive Learning: ProViCo</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2204.03946" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2204.03946<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文通过contrastive learning处理video，特别的是其中的representation不是一个确定的representation而是一个概率的形式。<br> 作者提出一个问题，Contrastive Learning是通过将正、负样本学习representation，常见的在video的操作就是将同一个视频里截取出来几个clip作为正样本，其他视频截取的clip作为负样本，但是假设两个视频里面截取出来的clip分别包含抬胳膊、挥手两个不同的动作，由于是从不同的视频里面截取的理应是不同的类别(一对正负样本)，然而在两个clip中均包含抬手臂的动作，那么将这种clip强行区分成正负样本会导致encoder confused。</p><figure><img src="/assets/image-70-aa4bc79d.png" alt="与显示对比学习的区别" tabindex="0" loading="lazy"><figcaption>与显示对比学习的区别</figcaption></figure><figure><img src="/assets/image-71-720da722.png" alt="隐式对比方法" tabindex="0" loading="lazy"><figcaption>隐式对比方法</figcaption></figure><p>首先将两个video进行随机采样得到多个clip，再通过一个概率的embedding得到整个视频的representation，然后从这个representation中重新采样得到真实的clip表征，那么接下来就是考虑如何确定正负样本的过程了</p><figure><img src="/assets/image-72-c626fe94.png" alt="正负样本对比" tabindex="0" loading="lazy"><figcaption>正负样本对比</figcaption></figure><figure><img src="/assets/image-73-d9b677ba.png" alt="正负样本判断" tabindex="0" loading="lazy"><figcaption>正负样本判断</figcaption></figure><p>值得注意的是 本文作者用的是巴氏距离而不是MSE</p><figure><img src="/assets/image-74-7d445337.png" alt="计算loss" tabindex="0" loading="lazy"><figcaption>计算loss</figcaption></figure><figure><img src="/assets/image-75-e8141341.png" alt="高斯分布" tabindex="0" loading="lazy"><figcaption>高斯分布</figcaption></figure><p>由于每个clip提取的时候是符合高斯分布的，因此最后得到的representation也需要符合高斯分布</p></div><!----><footer class="page-meta"><!----><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 799976781@qq.com">WillebrordSnell</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a aria-label="视频理解综述性质的记录" class="vp-link nav-link prev nav-link prev" href="/keyan/videoUnderstanding/videoUnderstanding.html"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->视频理解综述性质的记录</div></a><a aria-label="视频对话方向大论文性质的记录" class="vp-link nav-link next nav-link next" href="/keyan/videoDialog/videoDialog.html"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">视频对话方向大论文性质的记录<!----></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">西湖美景, 三月天嘞~</div><div class="vp-copyright">Copyright © 2023 Mr.R</div></footer></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-578d0f47.js" defer></script>
  </body>
</html>
