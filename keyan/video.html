<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://mister-hope.github.io/keyan/video.html"><meta property="og:site_name" content=" "><meta property="og:title" content="一些关于video方向的论文收集"><meta property="og:description" content="一些关于video方向的论文收集 本文主要记录一下近4年(2019年起)各顶会顶刊有关video的paper名字，以便后续video dialog工作的调研和展开 (本文档未经过任何筛选，仅通过关键词搜索得到paper名字) 2022 ECCV DexMV: Imitation Learning for Dexterous Manipulation from Human Videos Video Dialog as Conversation About Objects Living in Space-Time Actor-Centered Representations for Action Localization in Streaming Videos AutoTransition: Learning to Recommend Video Transition Effects Sports Video Analysis on Large-Scale Data Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation Quantized GAN for Complex Music Generation from Dance Videos Telepresence Video Quality Assessment GAMa: Cross-View Video Geo-Localization FAR: Fourier Aerial Video Recognition Fabric Material Recovery from Video Using Multi-scale Geometric Auto-Encoder Video Graph Transformer for Video Question Answering Video Question Answering with Iterative Video-Text Co-tokenization Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding Selective Query-Guided Debiasing for Video Corpus Moment Retrieval Learning Linguistic Association Towards Efficient Text-Video Retrieval VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection CycDA: Unsupervised Cycle Domain Adaptation to Learn from Image to Video Expanding Language-Image Pretrained Models for General Video Recognition AdaFocusV3: On Unified Spatial-Temporal Dynamic Video Recognition Delving into Details: Synopsis-to-Detail Networks for Video Recognition Scale-Aware Spatio-Temporal Relation Learning for Video Anomaly Detection Continual 3D Convolutional Neural Networks for Real-time Processing of Videos Geometric Features Informed Multi-person Human-Object Interaction Recognition in Videos Neural Capture of Animatable 3D Human from Monocular Video FAST-VQA: Efficient End-to-End Video Quality Assessment with Fragment Sampling Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset Synthesizing Light Field Video from Monocular Video Video Interpolation by Event-Driven Anisotropic Adjustment of Optical Flow CelebV-HQ: A Large-Scale Video Facial Attributes Dataset SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos RayTran: 3D Pose Estimation and Shape Reconstruction of Multiple Objects from Videos with Ray-Traced Transformers SALISA: Saliency-Based Input Sampling for Efficient Video Object Detection Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions Contrast-Phys: Unsupervised Video-Based Remote Physiological Measurement via Spatiotemporal Contrast Hierarchical Contrastive Inconsistency Learning for Deepfake Video Detection Generative Adversarial Network for Future Hand Segmentation from Egocentric Video My View is the Best View: Procedure Learning from Egocentric Videos Self-supervised Sparse Representation for Video Anomaly Detection Few-Shot Video Object Detection Inductive and Transductive Few-Shot Video Classification via Appearance and Temporal Alignments Graph Neural Network for Cell Tracking in Microscopy Videos Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories Towards Generic 3D Tracking in RGBD Videos: Benchmark and Baseline Tackling Background Distraction in Video Object Segmentation Learned Variational Video Color Propagation Ensemble Learning Priors Driven Deep Unfolding for Scalable Video Snapshot Compressive Imaging Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection LocVTP: Video-Text Pre-training for Temporal Localization Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining Static and Dynamic Concepts for Self-supervised Video Representation Learning Neural Video Compression Using GANs for Detail Synthesis and Propagation Is It Necessary to Transfer Temporal Knowledge for Domain Adaptive Video Semantic Segmentation? Meta Spatio-Temporal Debiasing for Video Scene Graph Generation PolyphonicFormer: Unified Query Learning for Depth-Aware Video Panoptic Segmentation Video Restoration Framework and Its Meta-adaptations to Data-Poor Conditions SeqFormer: Sequential Transformer for Video Instance Segmentation In Defense of Online Models for Video Instance Segmentation XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model Video Mask Transfiner for High-Quality Video Instance Segmentation Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding Waymo Open Dataset: Panoramic Video Panoptic Segmentation One-Trimap Video Matting Learning Quality-aware Dynamic Memory for Video Object Segmentation Instance as Identity: A Generic Online Paradigm for Video Instance Segmentation BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation Global Spectral Filter Memory Network for Video Object Segmentation Video Instance Segmentation via Multi-Scale Spatio-Temporal Split Attention Transformer Domain Adaptive Video Segmentation via Temporal Pseudo Supervision GOCA: Guided Online Cluster Assignment for Self-supervised Video Representation Learning Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition Federated Self-supervised Learning for Video Understanding NeuMan: Neural Human Radiance Field from a Single Video Structure and Motion from Casual Videos The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration earning Omnidirectional Flow in 360$^\circ $ Video via Siamese Representation PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval Multi-query Video Retrieval TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval Learning Audio-Video Modalities from Image Captions Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval Audio-Visual Mismatch-Aware Video Retrieval via Association and Adjustment CAViT: Contextual Alignment Vision Transformer for Video Object Re-identification Relighting4D: Neural Relightable Human from Videos Real-Time Intermediate Flow Estimation for Video Frame Interpolation Deep Bayesian Video Frame Interpolation A Perceptual Quality Metric for Video Frame Interpolation Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis Temporally Consistent Semantic Video Editing Error Compensation Framework for Flow-Guided Video Inpainting Learning Cross-Video Neural Representations for High-Quality Frame Interpolation A Style-Based GAN Encoder for High Fidelity Reconstruction of Images and Videos Harmonizer: Learning to Perform White-Box Image and Video Harmonization Text2LIVE: Text-Driven Layered Image and Video Editing CANF-VC: Conditional Augmented Normalizing Flows for Video Compression Video Extrapolation in Space and Time Augmentation of rPPG Benchmark Datasets: Learning to Remove and Embed rPPG Signals via Double Cycle Consistent Learning from Unpaired Facial Videos Layered Controllable Video Generation Spatio-Temporal Deformable Attention Network for Video Deblurring Sound-Guided Semantic Video Generation Controllable Video Generation Through Global and Local Motion Dynamics Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer Combining Internal and External Constraints for Unrolling Shutter in Videos A Codec Information Assisted Framework for Efficient Compressed Video Super-Resolution Diverse Generation from a Single Video Made Possible Learning Shadow Correspondence for Video Shadow Detection Flow-Guided Transformer for Video Inpainting Learning Spatio-Temporal Downsampling for Effective Video Upscaling Learning Spatiotemporal Frequency-Transformer for Compressed Video Super-Resolution Efficient Meta-Tuning for Content-Aware Neural Video Delivery Towards Interpretable Video Super-Resolution via Alternating Optimization Event-guided Deblurring of Unknown Exposure Time Videos Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-Ahead Forward Ones ERDN: Equivalent Receptive Field Deformable Network for Video Deblurring RealFlow: EM-Based Realistic Optical Flow Dataset Generation from Videos Efficient Video Deblurring Guided by Motion Magnitude TempFormer: Temporally Consistent Transformer for Video Denoising Rethinking Video Rain Streak Removal: A New Synthesis Model and a Deraining Network with Video Rain Prior AlphaVC: High-Performance and Efficient Learned Video Compression Source-Free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition Towards Open Set Video Anomaly Detection EclipSE: Efficient Long-Range Video Retrieval Using Sight and Sound Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing Less Than Few: Self-shot Video Instance Segmentation Real-Time Online Video Detection with Temporal Smoothing Transformers Mining Relations Among Cross-Frame Affinities for Video Semantic Segmentation TL;DW? Summarizing Instructional Videos with Task Relevance and Cross-Modal Saliency DualFormer: Local-Global Stratified Transformer for Efficient Video Recognition Hierarchical Feature Alignment Network for Unsupervised Video Object Segmentation PAC-Net: Highlight Your Video via History Preference Modeling How Severe Is Benchmark-Sensitivity in Video Self-supervised Learning? NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition Video Activity Localisation with Uncertainties in Temporal Boundary Temporal Saliency Query Network for Efficient Video Recognition Efficient One-Stage Video Object Detection by Exploiting Temporal Consistency Spotting Temporally Precise, Fine-Grained Events in Video Efficient Video Transformers with Spatial-Temporal Token Selection Long Movie Clip Classification with State-Space Video Models Prompting Visual-Language Models for Efficient Video Understanding Asymmetric Relation Consistency Reasoning for Video Relation Grounding K-centered Patch Sampling for Efficient Video Recognition GraphVid: It only Takes a Few Nodes to Understand a Video Delta Distillation for Efficient Video Processing COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context TDViT: Temporal Dilated Video Transformer for Dense Video Tasks Flow Graph to Video Grounding for Weakly-Supervised Multi-step Localization MaCLR: Motion-Aware Contrastive Learning of Representations for Videos Frozen CLIP Models are Efficient Video Learners Panoramic Vision Transformer for Saliency Detection in 360$^\circ $ Videos Bayesian Tracking of Video Graphs Using Joint Kalman Smoothing and Registration Motion Sensitive Contrastive Learning for Self-supervised Video Representation Dynamic Temporal Filtering in Video Models VTC: Improving Video-Text Retrieval with User Comments Automatic Dense Annotation of Large-Vocabulary Sign Language Videos MILES: Visual BERT Pre-training with Injected Language Semantics for Video-Text Retrieval"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-10-18T15:20:18.000Z"><meta property="article:author" content="Mr.R"><meta property="article:tag" content="去码头整点论文"><meta property="article:published_time" content="2023-09-19T00:00:00.000Z"><meta property="article:modified_time" content="2023-10-18T15:20:18.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"一些关于video方向的论文收集","image":[""],"datePublished":"2023-09-19T00:00:00.000Z","dateModified":"2023-10-18T15:20:18.000Z","author":[{"@type":"Person","name":"Mr.R","url":"https://mister-hope.com"}]}</script><title>一些关于video方向的论文收集 |  </title><meta name="description" content="一些关于video方向的论文收集 本文主要记录一下近4年(2019年起)各顶会顶刊有关video的paper名字，以便后续video dialog工作的调研和展开 (本文档未经过任何筛选，仅通过关键词搜索得到paper名字) 2022 ECCV DexMV: Imitation Learning for Dexterous Manipulation from Human Videos Video Dialog as Conversation About Objects Living in Space-Time Actor-Centered Representations for Action Localization in Streaming Videos AutoTransition: Learning to Recommend Video Transition Effects Sports Video Analysis on Large-Scale Data Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation Quantized GAN for Complex Music Generation from Dance Videos Telepresence Video Quality Assessment GAMa: Cross-View Video Geo-Localization FAR: Fourier Aerial Video Recognition Fabric Material Recovery from Video Using Multi-scale Geometric Auto-Encoder Video Graph Transformer for Video Question Answering Video Question Answering with Iterative Video-Text Co-tokenization Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding Selective Query-Guided Debiasing for Video Corpus Moment Retrieval Learning Linguistic Association Towards Efficient Text-Video Retrieval VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection CycDA: Unsupervised Cycle Domain Adaptation to Learn from Image to Video Expanding Language-Image Pretrained Models for General Video Recognition AdaFocusV3: On Unified Spatial-Temporal Dynamic Video Recognition Delving into Details: Synopsis-to-Detail Networks for Video Recognition Scale-Aware Spatio-Temporal Relation Learning for Video Anomaly Detection Continual 3D Convolutional Neural Networks for Real-time Processing of Videos Geometric Features Informed Multi-person Human-Object Interaction Recognition in Videos Neural Capture of Animatable 3D Human from Monocular Video FAST-VQA: Efficient End-to-End Video Quality Assessment with Fragment Sampling Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset Synthesizing Light Field Video from Monocular Video Video Interpolation by Event-Driven Anisotropic Adjustment of Optical Flow CelebV-HQ: A Large-Scale Video Facial Attributes Dataset SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos RayTran: 3D Pose Estimation and Shape Reconstruction of Multiple Objects from Videos with Ray-Traced Transformers SALISA: Saliency-Based Input Sampling for Efficient Video Object Detection Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions Contrast-Phys: Unsupervised Video-Based Remote Physiological Measurement via Spatiotemporal Contrast Hierarchical Contrastive Inconsistency Learning for Deepfake Video Detection Generative Adversarial Network for Future Hand Segmentation from Egocentric Video My View is the Best View: Procedure Learning from Egocentric Videos Self-supervised Sparse Representation for Video Anomaly Detection Few-Shot Video Object Detection Inductive and Transductive Few-Shot Video Classification via Appearance and Temporal Alignments Graph Neural Network for Cell Tracking in Microscopy Videos Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories Towards Generic 3D Tracking in RGBD Videos: Benchmark and Baseline Tackling Background Distraction in Video Object Segmentation Learned Variational Video Color Propagation Ensemble Learning Priors Driven Deep Unfolding for Scalable Video Snapshot Compressive Imaging Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection LocVTP: Video-Text Pre-training for Temporal Localization Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining Static and Dynamic Concepts for Self-supervised Video Representation Learning Neural Video Compression Using GANs for Detail Synthesis and Propagation Is It Necessary to Transfer Temporal Knowledge for Domain Adaptive Video Semantic Segmentation? Meta Spatio-Temporal Debiasing for Video Scene Graph Generation PolyphonicFormer: Unified Query Learning for Depth-Aware Video Panoptic Segmentation Video Restoration Framework and Its Meta-adaptations to Data-Poor Conditions SeqFormer: Sequential Transformer for Video Instance Segmentation In Defense of Online Models for Video Instance Segmentation XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model Video Mask Transfiner for High-Quality Video Instance Segmentation Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding Waymo Open Dataset: Panoramic Video Panoptic Segmentation One-Trimap Video Matting Learning Quality-aware Dynamic Memory for Video Object Segmentation Instance as Identity: A Generic Online Paradigm for Video Instance Segmentation BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation Global Spectral Filter Memory Network for Video Object Segmentation Video Instance Segmentation via Multi-Scale Spatio-Temporal Split Attention Transformer Domain Adaptive Video Segmentation via Temporal Pseudo Supervision GOCA: Guided Online Cluster Assignment for Self-supervised Video Representation Learning Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition Federated Self-supervised Learning for Video Understanding NeuMan: Neural Human Radiance Field from a Single Video Structure and Motion from Casual Videos The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration earning Omnidirectional Flow in 360$^\circ $ Video via Siamese Representation PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval Multi-query Video Retrieval TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval Learning Audio-Video Modalities from Image Captions Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval Audio-Visual Mismatch-Aware Video Retrieval via Association and Adjustment CAViT: Contextual Alignment Vision Transformer for Video Object Re-identification Relighting4D: Neural Relightable Human from Videos Real-Time Intermediate Flow Estimation for Video Frame Interpolation Deep Bayesian Video Frame Interpolation A Perceptual Quality Metric for Video Frame Interpolation Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis Temporally Consistent Semantic Video Editing Error Compensation Framework for Flow-Guided Video Inpainting Learning Cross-Video Neural Representations for High-Quality Frame Interpolation A Style-Based GAN Encoder for High Fidelity Reconstruction of Images and Videos Harmonizer: Learning to Perform White-Box Image and Video Harmonization Text2LIVE: Text-Driven Layered Image and Video Editing CANF-VC: Conditional Augmented Normalizing Flows for Video Compression Video Extrapolation in Space and Time Augmentation of rPPG Benchmark Datasets: Learning to Remove and Embed rPPG Signals via Double Cycle Consistent Learning from Unpaired Facial Videos Layered Controllable Video Generation Spatio-Temporal Deformable Attention Network for Video Deblurring Sound-Guided Semantic Video Generation Controllable Video Generation Through Global and Local Motion Dynamics Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer Combining Internal and External Constraints for Unrolling Shutter in Videos A Codec Information Assisted Framework for Efficient Compressed Video Super-Resolution Diverse Generation from a Single Video Made Possible Learning Shadow Correspondence for Video Shadow Detection Flow-Guided Transformer for Video Inpainting Learning Spatio-Temporal Downsampling for Effective Video Upscaling Learning Spatiotemporal Frequency-Transformer for Compressed Video Super-Resolution Efficient Meta-Tuning for Content-Aware Neural Video Delivery Towards Interpretable Video Super-Resolution via Alternating Optimization Event-guided Deblurring of Unknown Exposure Time Videos Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-Ahead Forward Ones ERDN: Equivalent Receptive Field Deformable Network for Video Deblurring RealFlow: EM-Based Realistic Optical Flow Dataset Generation from Videos Efficient Video Deblurring Guided by Motion Magnitude TempFormer: Temporally Consistent Transformer for Video Denoising Rethinking Video Rain Streak Removal: A New Synthesis Model and a Deraining Network with Video Rain Prior AlphaVC: High-Performance and Efficient Learned Video Compression Source-Free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition Towards Open Set Video Anomaly Detection EclipSE: Efficient Long-Range Video Retrieval Using Sight and Sound Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing Less Than Few: Self-shot Video Instance Segmentation Real-Time Online Video Detection with Temporal Smoothing Transformers Mining Relations Among Cross-Frame Affinities for Video Semantic Segmentation TL;DW? Summarizing Instructional Videos with Task Relevance and Cross-Modal Saliency DualFormer: Local-Global Stratified Transformer for Efficient Video Recognition Hierarchical Feature Alignment Network for Unsupervised Video Object Segmentation PAC-Net: Highlight Your Video via History Preference Modeling How Severe Is Benchmark-Sensitivity in Video Self-supervised Learning? NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition Video Activity Localisation with Uncertainties in Temporal Boundary Temporal Saliency Query Network for Efficient Video Recognition Efficient One-Stage Video Object Detection by Exploiting Temporal Consistency Spotting Temporally Precise, Fine-Grained Events in Video Efficient Video Transformers with Spatial-Temporal Token Selection Long Movie Clip Classification with State-Space Video Models Prompting Visual-Language Models for Efficient Video Understanding Asymmetric Relation Consistency Reasoning for Video Relation Grounding K-centered Patch Sampling for Efficient Video Recognition GraphVid: It only Takes a Few Nodes to Understand a Video Delta Distillation for Efficient Video Processing COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context TDViT: Temporal Dilated Video Transformer for Dense Video Tasks Flow Graph to Video Grounding for Weakly-Supervised Multi-step Localization MaCLR: Motion-Aware Contrastive Learning of Representations for Videos Frozen CLIP Models are Efficient Video Learners Panoramic Vision Transformer for Saliency Detection in 360$^\circ $ Videos Bayesian Tracking of Video Graphs Using Joint Kalman Smoothing and Registration Motion Sensitive Contrastive Learning for Self-supervised Video Representation Dynamic Temporal Filtering in Video Models VTC: Improving Video-Text Retrieval with User Comments Automatic Dense Annotation of Large-Vocabulary Sign Language Videos MILES: Visual BERT Pre-training with Injected Language Semantics for Video-Text Retrieval">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-13796ab9.css" as="style"><link rel="stylesheet" href="/assets/style-13796ab9.css">
    <link rel="modulepreload" href="/assets/app-d9b91c43.js"><link rel="modulepreload" href="/assets/video.html-51997085.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/video.html-fe0150ef.js"><link rel="prefetch" href="/assets/index.html-06be6dbd.js" as="script"><link rel="prefetch" href="/assets/intro.html-0cbcbfad.js" as="script"><link rel="prefetch" href="/assets/slides.html-a4360897.js" as="script"><link rel="prefetch" href="/assets/202309.html-b8b8b02a.js" as="script"><link rel="prefetch" href="/assets/202310.html-a8f07497.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-313493aa.js" as="script"><link rel="prefetch" href="/assets/index.html-c19cc023.js" as="script"><link rel="prefetch" href="/assets/disable.html-de6f1fb7.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-a961ffbf.js" as="script"><link rel="prefetch" href="/assets/markdown.html-d64453e3.js" as="script"><link rel="prefetch" href="/assets/page.html-a14c059e.js" as="script"><link rel="prefetch" href="/assets/index.html-6098c265.js" as="script"><link rel="prefetch" href="/assets/dialog.html-d3391826.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-618c6010.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-593e8df9.js" as="script"><link rel="prefetch" href="/assets/videoRepresentation.html-47b5e20b.js" as="script"><link rel="prefetch" href="/assets/videoUnderstanding.html-7613b61c.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-f32487f7.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-b3725aa6.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-95730fa0.js" as="script"><link rel="prefetch" href="/assets/DDP.html-d779b619.js" as="script"><link rel="prefetch" href="/assets/dan.html-9609164e.js" as="script"><link rel="prefetch" href="/assets/trick.html-bb31f35e.js" as="script"><link rel="prefetch" href="/assets/404.html-130ff98b.js" as="script"><link rel="prefetch" href="/assets/index.html-10998ddb.js" as="script"><link rel="prefetch" href="/assets/index.html-e05d3217.js" as="script"><link rel="prefetch" href="/assets/index.html-cb0bac63.js" as="script"><link rel="prefetch" href="/assets/index.html-7fe952c2.js" as="script"><link rel="prefetch" href="/assets/index.html-82f2092e.js" as="script"><link rel="prefetch" href="/assets/index.html-e4678c97.js" as="script"><link rel="prefetch" href="/assets/index.html-c64b2618.js" as="script"><link rel="prefetch" href="/assets/index.html-11a8a3d5.js" as="script"><link rel="prefetch" href="/assets/index.html-5f4f46c8.js" as="script"><link rel="prefetch" href="/assets/index.html-3fa1430b.js" as="script"><link rel="prefetch" href="/assets/index.html-b222fddb.js" as="script"><link rel="prefetch" href="/assets/index.html-50095564.js" as="script"><link rel="prefetch" href="/assets/index.html-20b5478c.js" as="script"><link rel="prefetch" href="/assets/index.html-836ec29d.js" as="script"><link rel="prefetch" href="/assets/index.html-e7d44e3b.js" as="script"><link rel="prefetch" href="/assets/index.html-cccb4d6a.js" as="script"><link rel="prefetch" href="/assets/index.html-0917fcc6.js" as="script"><link rel="prefetch" href="/assets/index.html-52864055.js" as="script"><link rel="prefetch" href="/assets/index.html-c46cc60b.js" as="script"><link rel="prefetch" href="/assets/index.html-47792a83.js" as="script"><link rel="prefetch" href="/assets/index.html-aa30eeef.js" as="script"><link rel="prefetch" href="/assets/index.html-59dfac95.js" as="script"><link rel="prefetch" href="/assets/index.html-739d9637.js" as="script"><link rel="prefetch" href="/assets/index.html-5fabefb9.js" as="script"><link rel="prefetch" href="/assets/index.html-6476eeca.js" as="script"><link rel="prefetch" href="/assets/index.html-e909ac63.js" as="script"><link rel="prefetch" href="/assets/index.html-7cef4089.js" as="script"><link rel="prefetch" href="/assets/index.html-46567c63.js" as="script"><link rel="prefetch" href="/assets/index.html-8498dbec.js" as="script"><link rel="prefetch" href="/assets/index.html-4d1b2642.js" as="script"><link rel="prefetch" href="/assets/index.html-b32b66eb.js" as="script"><link rel="prefetch" href="/assets/index.html-1d386275.js" as="script"><link rel="prefetch" href="/assets/index.html-749de246.js" as="script"><link rel="prefetch" href="/assets/index.html-66325375.js" as="script"><link rel="prefetch" href="/assets/index.html-61263a6e.js" as="script"><link rel="prefetch" href="/assets/index.html-0e27a9a6.js" as="script"><link rel="prefetch" href="/assets/index.html-e6e96cde.js" as="script"><link rel="prefetch" href="/assets/index.html-15bff3b9.js" as="script"><link rel="prefetch" href="/assets/intro.html-fb4fb33f.js" as="script"><link rel="prefetch" href="/assets/slides.html-45e806b3.js" as="script"><link rel="prefetch" href="/assets/202309.html-f862ae33.js" as="script"><link rel="prefetch" href="/assets/202310.html-53fc1f42.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-88bc8189.js" as="script"><link rel="prefetch" href="/assets/index.html-66528150.js" as="script"><link rel="prefetch" href="/assets/disable.html-199b9e9c.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-44e525e5.js" as="script"><link rel="prefetch" href="/assets/markdown.html-2bf7e944.js" as="script"><link rel="prefetch" href="/assets/page.html-4bf7779d.js" as="script"><link rel="prefetch" href="/assets/index.html-c3c7f906.js" as="script"><link rel="prefetch" href="/assets/dialog.html-26e70bc8.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-b6aac286.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-5b0859a9.js" as="script"><link rel="prefetch" href="/assets/videoRepresentation.html-0f5f8245.js" as="script"><link rel="prefetch" href="/assets/videoUnderstanding.html-097e832c.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-9ec72f50.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-5e9a27b4.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-3aef78cc.js" as="script"><link rel="prefetch" href="/assets/DDP.html-92b35273.js" as="script"><link rel="prefetch" href="/assets/dan.html-6dfbbc3e.js" as="script"><link rel="prefetch" href="/assets/trick.html-a4bee3d8.js" as="script"><link rel="prefetch" href="/assets/404.html-a488bea4.js" as="script"><link rel="prefetch" href="/assets/index.html-11b3bce9.js" as="script"><link rel="prefetch" href="/assets/index.html-3c702ce1.js" as="script"><link rel="prefetch" href="/assets/index.html-e071077f.js" as="script"><link rel="prefetch" href="/assets/index.html-d85e9bb6.js" as="script"><link rel="prefetch" href="/assets/index.html-e61c477c.js" as="script"><link rel="prefetch" href="/assets/index.html-bf6f5293.js" as="script"><link rel="prefetch" href="/assets/index.html-7d8a447d.js" as="script"><link rel="prefetch" href="/assets/index.html-35f61568.js" as="script"><link rel="prefetch" href="/assets/index.html-a949961e.js" as="script"><link rel="prefetch" href="/assets/index.html-a5e0c65a.js" as="script"><link rel="prefetch" href="/assets/index.html-9e386b99.js" as="script"><link rel="prefetch" href="/assets/index.html-14dcd5d4.js" as="script"><link rel="prefetch" href="/assets/index.html-469a3f11.js" as="script"><link rel="prefetch" href="/assets/index.html-f9357c91.js" as="script"><link rel="prefetch" href="/assets/index.html-f852fca1.js" as="script"><link rel="prefetch" href="/assets/index.html-a69c478e.js" as="script"><link rel="prefetch" href="/assets/index.html-ab137ad4.js" as="script"><link rel="prefetch" href="/assets/index.html-ea96f52b.js" as="script"><link rel="prefetch" href="/assets/index.html-cbbb4ce5.js" as="script"><link rel="prefetch" href="/assets/index.html-bc85ec9a.js" as="script"><link rel="prefetch" href="/assets/index.html-c15492fe.js" as="script"><link rel="prefetch" href="/assets/index.html-70c0793c.js" as="script"><link rel="prefetch" href="/assets/index.html-113dff89.js" as="script"><link rel="prefetch" href="/assets/index.html-e8a54bbd.js" as="script"><link rel="prefetch" href="/assets/index.html-625e36c4.js" as="script"><link rel="prefetch" href="/assets/index.html-860061fd.js" as="script"><link rel="prefetch" href="/assets/index.html-26e0a819.js" as="script"><link rel="prefetch" href="/assets/index.html-87e579c5.js" as="script"><link rel="prefetch" href="/assets/index.html-910ae7bf.js" as="script"><link rel="prefetch" href="/assets/index.html-ee2cb3fc.js" as="script"><link rel="prefetch" href="/assets/index.html-837b9a27.js" as="script"><link rel="prefetch" href="/assets/index.html-0bd2a28c.js" as="script"><link rel="prefetch" href="/assets/index.html-382b8e92.js" as="script"><link rel="prefetch" href="/assets/index.html-f7cc6b09.js" as="script"><link rel="prefetch" href="/assets/index.html-886a463e.js" as="script"><link rel="prefetch" href="/assets/index.html-fa2a5be8.js" as="script"><link rel="prefetch" href="/assets/index.html-c0c2f97d.js" as="script"><link rel="prefetch" href="/assets/waline-meta-56fbc549.js" as="script"><link rel="prefetch" href="/assets/component-c2f8fbb8.js" as="script"><link rel="prefetch" href="/assets/auto-fe80bb03.js" as="script"><link rel="prefetch" href="/assets/index-2bf332f6.js" as="script"><link rel="prefetch" href="/assets/flowchart-c441f34d.js" as="script"><link rel="prefetch" href="/assets/mermaid.core-3e40c444.js" as="script"><link rel="prefetch" href="/assets/highlight.esm-75b11b9d.js" as="script"><link rel="prefetch" href="/assets/markdown.esm-9d5bc2ce.js" as="script"><link rel="prefetch" href="/assets/math.esm-70a288c8.js" as="script"><link rel="prefetch" href="/assets/notes.esm-a106bb2c.js" as="script"><link rel="prefetch" href="/assets/reveal.esm-1a4c3ae7.js" as="script"><link rel="prefetch" href="/assets/search.esm-7e6792e2.js" as="script"><link rel="prefetch" href="/assets/zoom.esm-b83b91d0.js" as="script"><link rel="prefetch" href="/assets/VuePlayground-abdd2839.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-5762295a.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><img class="vp-nav-logo" src="/logo.svg" alt=" "><!----><span class="vp-site-name hide-in-pad"> </span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="码头"><span class="title"><!---->码头</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="视频理解" class="vp-link nav-link nav-link" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解<!----></a></li><li class="dropdown-item"><a aria-label="视频表征" class="vp-link nav-link nav-link" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->视频表征<!----></a></li><li class="dropdown-item"><a aria-label="对比学习" class="vp-link nav-link nav-link" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习<!----></a></li><li class="dropdown-item"><a aria-label="多模态" class="vp-link nav-link nav-link" href="/keyan/multiModal/multiModal.html"><!---->多模态<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="炉"><span class="title"><!---->炉</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="单机多卡DDP" class="vp-link nav-link nav-link" href="/train/DDP/DDP.html"><!---->单机多卡DDP<!----></a></li><li class="dropdown-item"><a aria-label="AVSD" class="vp-link nav-link nav-link" href="/train/AVSD/AVSD.html"><!---->AVSD<!----></a></li><li class="dropdown-item"><a aria-label="奇淫技巧" class="vp-link nav-link nav-link" href="/train/trick/trick.html"><!---->奇淫技巧<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="道心"><span class="title"><!---->道心</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="2023年9月" class="vp-link nav-link nav-link" href="/book/202309.html"><!---->2023年9月<!----></a></li><li class="dropdown-item"><a aria-label="2023年10月" class="vp-link nav-link nav-link" href="/book/202310.html"><!---->2023年10月<!----></a></li><li class="dropdown-item"><a aria-label="毛泽东选集" class="vp-link nav-link nav-link" href="/book/maoxuan.html"><!---->毛泽东选集<!----></a></li></ul></button></div></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><!--[--><a aria-label="视频理解综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="关于视频理解的论文收集(较新)" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->关于视频理解的论文收集(较新)<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="对比学习综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="多模态方向综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/multiModal/multiModal.html"><!---->多模态方向综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->一些关于video方向的论文收集</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://mister-hope.com" target="_blank" rel="noopener noreferrer">Mr.R</a></span><span property="author" content="Mr.R"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-09-19T00:00:00.000Z"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="waline-pageview-count" data-path="/keyan/video.html">...</span></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 8 分钟</span><meta property="timeRequired" content="PT8M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category3 clickable" role="navigation">科研</span><!--]--><meta property="articleSection" content="科研"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag1 clickable" role="navigation">去码头整点论文</span><!--]--><meta property="keywords" content="去码头整点论文"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2022-eccv">2022 ECCV</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2020-eccv">2020 ECCV</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2021-iccv">2021 ICCV</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="一些关于video方向的论文收集" tabindex="-1"><a class="header-anchor" href="#一些关于video方向的论文收集" aria-hidden="true">#</a> 一些关于video方向的论文收集</h1><p>本文主要记录一下近4年(2019年起)各顶会顶刊有关video的paper名字，以便后续video dialog工作的调研和展开<br> (本文档未经过任何筛选，仅通过关键词搜索得到paper名字)</p><h3 id="_2022-eccv" tabindex="-1"><a class="header-anchor" href="#_2022-eccv" aria-hidden="true">#</a> 2022 ECCV</h3><p>DexMV: Imitation Learning for Dexterous Manipulation from Human Videos<br> Video Dialog as Conversation About Objects Living in Space-Time<br> Actor-Centered Representations for Action Localization in Streaming Videos<br> AutoTransition: Learning to Recommend Video Transition Effects<br> Sports Video Analysis on Large-Scale Data<br> Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation<br> Quantized GAN for Complex Music Generation from Dance Videos<br> Telepresence Video Quality Assessment<br> GAMa: Cross-View Video Geo-Localization<br> FAR: Fourier Aerial Video Recognition<br> Fabric Material Recovery from Video Using Multi-scale Geometric Auto-Encoder<br> Video Graph Transformer for Video Question Answering<br> Video Question Answering with Iterative Video-Text Co-tokenization<br> Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding<br> Selective Query-Guided Debiasing for Video Corpus Moment Retrieval<br> Learning Linguistic Association Towards Efficient Text-Video Retrieval<br> VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection<br> CycDA: Unsupervised Cycle Domain Adaptation to Learn from Image to Video<br> Expanding Language-Image Pretrained Models for General Video Recognition<br> AdaFocusV3: On Unified Spatial-Temporal Dynamic Video Recognition<br> Delving into Details: Synopsis-to-Detail Networks for Video Recognition<br> Scale-Aware Spatio-Temporal Relation Learning for Video Anomaly Detection<br> Continual 3D Convolutional Neural Networks for Real-time Processing of Videos<br> Geometric Features Informed Multi-person Human-Object Interaction Recognition in Videos<br> Neural Capture of Animatable 3D Human from Monocular Video<br> FAST-VQA: Efficient End-to-End Video Quality Assessment with Fragment Sampling<br> Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset<br> Synthesizing Light Field Video from Monocular Video<br> Video Interpolation by Event-Driven Anisotropic Adjustment of Optical Flow<br> CelebV-HQ: A Large-Scale Video Facial Attributes Dataset<br> SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos<br> RayTran: 3D Pose Estimation and Shape Reconstruction of Multiple Objects from Videos with Ray-Traced Transformers<br> SALISA: Saliency-Based Input Sampling for Efficient Video Object Detection<br> Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles<br> Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions<br> Contrast-Phys: Unsupervised Video-Based Remote Physiological Measurement via Spatiotemporal Contrast<br> Hierarchical Contrastive Inconsistency Learning for Deepfake Video Detection<br> Generative Adversarial Network for Future Hand Segmentation from Egocentric Video<br> My View is the Best View: Procedure Learning from Egocentric Videos<br> Self-supervised Sparse Representation for Video Anomaly Detection<br> Few-Shot Video Object Detection<br> Inductive and Transductive Few-Shot Video Classification via Appearance and Temporal Alignments<br> Graph Neural Network for Cell Tracking in Microscopy Videos<br> Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories<br> Towards Generic 3D Tracking in RGBD Videos: Benchmark and Baseline<br> Tackling Background Distraction in Video Object Segmentation<br> Learned Variational Video Color Propagation<br> Ensemble Learning Priors Driven Deep Unfolding for Scalable Video Snapshot Compressive Imaging<br> Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection<br> LocVTP: Video-Text Pre-training for Temporal Localization<br> Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining<br> Static and Dynamic Concepts for Self-supervised Video Representation Learning<br> Neural Video Compression Using GANs for Detail Synthesis and Propagation<br> Is It Necessary to Transfer Temporal Knowledge for Domain Adaptive Video Semantic Segmentation?<br> Meta Spatio-Temporal Debiasing for Video Scene Graph Generation<br> PolyphonicFormer: Unified Query Learning for Depth-Aware Video Panoptic Segmentation<br> Video Restoration Framework and Its Meta-adaptations to Data-Poor Conditions<br> SeqFormer: Sequential Transformer for Video Instance Segmentation<br> In Defense of Online Models for Video Instance Segmentation<br> XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model<br> Video Mask Transfiner for High-Quality Video Instance Segmentation<br> Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding<br> Waymo Open Dataset: Panoramic Video Panoptic Segmentation<br> One-Trimap Video Matting<br> Learning Quality-aware Dynamic Memory for Video Object Segmentation<br> Instance as Identity: A Generic Online Paradigm for Video Instance Segmentation<br> BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation<br> Global Spectral Filter Memory Network for Video Object Segmentation<br> Video Instance Segmentation via Multi-Scale Spatio-Temporal Split Attention Transformer<br> Domain Adaptive Video Segmentation via Temporal Pseudo Supervision<br> GOCA: Guided Online Cluster Assignment for Self-supervised Video Representation Learning<br> Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition<br> Federated Self-supervised Learning for Video Understanding<br> NeuMan: Neural Human Radiance Field from a Single Video<br> Structure and Motion from Casual Videos<br> The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing<br> MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration<br> earning Omnidirectional Flow in 360$^\circ $ Video via Siamese Representation<br> PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection<br> Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval<br> Multi-query Video Retrieval<br> TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval<br> Learning Audio-Video Modalities from Image Captions<br> Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval<br> Audio-Visual Mismatch-Aware Video Retrieval via Association and Adjustment<br> CAViT: Contextual Alignment Vision Transformer for Video Object Re-identification<br> Relighting4D: Neural Relightable Human from Videos<br> Real-Time Intermediate Flow Estimation for Video Frame Interpolation<br> Deep Bayesian Video Frame Interpolation<br> A Perceptual Quality Metric for Video Frame Interpolation<br> Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis<br> Temporally Consistent Semantic Video Editing<br> Error Compensation Framework for Flow-Guided Video Inpainting<br> Learning Cross-Video Neural Representations for High-Quality Frame Interpolation<br> A Style-Based GAN Encoder for High Fidelity Reconstruction of Images and Videos<br> Harmonizer: Learning to Perform White-Box Image and Video Harmonization<br> Text2LIVE: Text-Driven Layered Image and Video Editing<br> CANF-VC: Conditional Augmented Normalizing Flows for Video Compression<br> Video Extrapolation in Space and Time<br> Augmentation of rPPG Benchmark Datasets: Learning to Remove and Embed rPPG Signals via Double Cycle Consistent Learning from Unpaired Facial Videos<br> Layered Controllable Video Generation<br> Spatio-Temporal Deformable Attention Network for Video Deblurring<br> Sound-Guided Semantic Video Generation<br> Controllable Video Generation Through Global and Local Motion Dynamics<br> Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer<br> Combining Internal and External Constraints for Unrolling Shutter in Videos<br> A Codec Information Assisted Framework for Efficient Compressed Video Super-Resolution<br> Diverse Generation from a Single Video Made Possible<br> Learning Shadow Correspondence for Video Shadow Detection<br> Flow-Guided Transformer for Video Inpainting<br> Learning Spatio-Temporal Downsampling for Effective Video Upscaling<br> Learning Spatiotemporal Frequency-Transformer for Compressed Video Super-Resolution<br> Efficient Meta-Tuning for Content-Aware Neural Video Delivery<br> Towards Interpretable Video Super-Resolution via Alternating Optimization<br> Event-guided Deblurring of Unknown Exposure Time Videos<br> Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-Ahead Forward Ones<br> ERDN: Equivalent Receptive Field Deformable Network for Video Deblurring<br> RealFlow: EM-Based Realistic Optical Flow Dataset Generation from Videos<br> Efficient Video Deblurring Guided by Motion Magnitude<br> TempFormer: Temporally Consistent Transformer for Video Denoising<br> Rethinking Video Rain Streak Removal: A New Synthesis Model and a Deraining Network with Video Rain Prior<br> AlphaVC: High-Performance and Efficient Learned Video Compression<br> Source-Free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition<br> Towards Open Set Video Anomaly Detection<br> EclipSE: Efficient Long-Range Video Retrieval Using Sight and Sound<br> Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing<br> Less Than Few: Self-shot Video Instance Segmentation<br> Real-Time Online Video Detection with Temporal Smoothing Transformers<br> Mining Relations Among Cross-Frame Affinities for Video Semantic Segmentation<br> TL;DW? Summarizing Instructional Videos with Task Relevance and Cross-Modal Saliency<br> DualFormer: Local-Global Stratified Transformer for Efficient Video Recognition<br> Hierarchical Feature Alignment Network for Unsupervised Video Object Segmentation<br> PAC-Net: Highlight Your Video via History Preference Modeling<br> How Severe Is Benchmark-Sensitivity in Video Self-supervised Learning?<br> NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition<br> Video Activity Localisation with Uncertainties in Temporal Boundary<br> Temporal Saliency Query Network for Efficient Video Recognition<br> Efficient One-Stage Video Object Detection by Exploiting Temporal Consistency<br> Spotting Temporally Precise, Fine-Grained Events in Video<br> Efficient Video Transformers with Spatial-Temporal Token Selection<br> Long Movie Clip Classification with State-Space Video Models<br> Prompting Visual-Language Models for Efficient Video Understanding<br> Asymmetric Relation Consistency Reasoning for Video Relation Grounding<br> K-centered Patch Sampling for Efficient Video Recognition<br> GraphVid: It only Takes a Few Nodes to Understand a Video<br> Delta Distillation for Efficient Video Processing<br> COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality<br> E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context<br> TDViT: Temporal Dilated Video Transformer for Dense Video Tasks<br> Flow Graph to Video Grounding for Weakly-Supervised Multi-step Localization<br> MaCLR: Motion-Aware Contrastive Learning of Representations for Videos<br> Frozen CLIP Models are Efficient Video Learners<br> Panoramic Vision Transformer for Saliency Detection in 360$^\circ $ Videos<br> Bayesian Tracking of Video Graphs Using Joint Kalman Smoothing and Registration<br> Motion Sensitive Contrastive Learning for Self-supervised Video Representation<br> Dynamic Temporal Filtering in Video Models<br> VTC: Improving Video-Text Retrieval with User Comments<br> Automatic Dense Annotation of Large-Vocabulary Sign Language Videos<br> MILES: Visual BERT Pre-training with Injected Language Semantics for Video-Text Retrieval</p><h3 id="_2020-eccv" tabindex="-1"><a class="header-anchor" href="#_2020-eccv" aria-hidden="true">#</a> 2020 ECCV</h3><p>Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring<br> CoTeRe-Net: Discovering Collaborative Ternary Relations in Videos<br> Visual Relation Grounding in Videos<br> SODA: Story Oriented Dense Video Captioning Evaluation Framework<br> Optical Flow Distillation: Towards Efficient and Stable Video Style Transfer<br> Learning Object Depth from Camera Motion and Video Object Segmentation<br> Localizing the Common Action Among a Few Videos<br> Two-Branch Recurrent Network for Isolating Deepfakes in Videos<br> World-Consistent Video-to-Video Synthesis<br> AttentionNAS: Spatiotemporal Attention Cell Search for Video Classification<br> Temporal Coherence or Temporal Motion: Which Is More Critical for Video-Based Person Re-identification?<br> Learning Event-Driven Video Deblurring and Interpolation<br> VPN: Learning Video-Pose Embedding for Activities of Daily Living<br> Joint Learning of Social Groups, Individuals Action and Sub-group Activities in Videos<br> Naive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation<br> RhyRNN: Rhythmic RNN for Recognizing Events in Long and Complex Videos<br> MuCAN: Multi-correspondence Aggregation Network for Video Super-Resolution<br> Efficient Semantic Video Segmentation with Per-Frame Inference<br> TexMesh: Reconstructing Detailed Human Texture and Geometry from RGB-D Video<br> Deep Space-Time Video Upsampling Networks<br> Fast Video Object Segmentation Using the Global Context Module<br> Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos<br> STEm-Seg: Spatio-Temporal Embeddings for Instance Segmentation in Videos<br> Procedure Planning in Instructional Videos<br> Foley Music: Learning to Generate Music from Videos<br> Online Multi-modal Person Search in Videos<br> G-LBM: Generative Low-Dimensional Background Model Estimation from Video Sequences<br> Generating Videos of Zero-Shot Compositions of Actions and Objects<br> Video Super-Resolution with Recurrent Structure-Detail Network<br> Shuffle and Attend: Video Domain Adaptation<br> Flow-edge Guided Video Completion<br> Towards End-to-End Video-Based Eye-Tracking<br> Low Light Video Enhancement Using Synthetic Data Produced with an Intermediate Domain Mapping<br> ScribbleBox: Interactive Annotation Framework for Video Object Segmentation<br> MINI-Net: Multiple Instance Ranking Network for Video Highlight Detection<br> AutoTrajectory: Label-Free Trajectory Extraction and Prediction from Videos Using Dynamic Points<br> Motion Guided 3D Pose Estimation from Videos<br> SipMask: Spatial Information Preservation for Fast Image and Video Instance Segmentation<br> BMBC: Bilateral Motion Estimation with Bilateral Cost Volume for Video Interpolation<br> Video Object Detection via Object-Level Temporal Aggregation<br> READ: Reciprocal Attention Discriminator for Image-to-Video Re-identification<br> Multi-level Wavelet-Based Generative Adversarial Network for Perceptual Quality Enhancement of Compressed Video<br> Unsupervised Video Object Segmentation with Joint Hotspot Tracking<br> Memory Selection Network for Video Propagation<br> URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark<br> Clustering Driven Deep Autoencoder for Video Anomaly Detection<br> Omni-Sourced Webly-Supervised Learning for Video Recognition<br> Learning Where to Focus for Efficient Video Object Detection<br> Learning Object Permanence from Video<br> Temporal Aggregate Representations for Long-Range Video Understanding<br> Multimodal Memorability: Modeling Effects of Semantics and Decay on Video Memorability<br> MotionSqueeze: Neural Motion Feature Learning for Video Understanding<br> Learning Joint Spatial-Temporal Transformations for Video Inpainting<br> Probabilistic Future Prediction for Video Scene Understanding<br> Interactive Video Object Segmentation Using Global and Local Transfer Modules<br> Is Sharing of Egocentric Video Giving Away Your Biometric Signature?<br> Conditional Entropy Coding for Efficient Video Compression<br> Self-supervised Video Representation Learning by Pace Prediction<br> Self-supervised Multi-task Procedure Learning from Instructional Videos<br> Key Frame Proposal Network for Efficient Pose Estimation in Videos<br> We Have So Much in Common: Modeling Semantic Relational Set Abstractions in Videos<br> Self-supervised Learning of Audio-Visual Objects from Video<br> Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions<br> RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition<br> Self-supervised Keypoint Correspondences for Multi-person Pose Estimation and Tracking in Videos<br> Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior<br> DVI: Depth Guided Video Inpainting for Autonomous Driving<br> Adaptive Video Highlight Detection by Learning from User History<br> dentity-Aware Multi-sentence Video Description<br> Mining Inter-Video Proposal Relations for Video Object Detection<br> TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval<br> Kernelized Memory Network for Video Object Segmentation<br> Disentangling Multiple Features in Video Sequences Using Gaussian Processes in Variational Autoencoders<br> Kinematic 3D Object Detection in Monocular Video<br> Describing Unseen Videos via Multi-modal Cooperative Dialog Agents<br> DeepLandscape: Adversarial Modeling of Landscape Videos<br> BIRNAT: Bidirectional Recurrent Neural Networks with Adversarial Training for Video Snapshot Compressive Imaging<br> Cross-Identity Motion Transfer for Arbitrary Objects Through Pose-Attentive Video Reassembling<br> Aligning Videos in Space and Time<br> Proposal-Based Video Completion<br> Exploiting Temporal Coherence for Self-Supervised One-Shot Video Re-identification<br> Multi-view Action Recognition Using Cross-View Video Prediction<br> Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation<br> VLANet: Video-Language Alignment Network for Weakly-Supervised Video Moment Retrieval<br> Video Representation Learning by Recognizing Temporal Transformations<br> Measuring the Importance of Temporal Features in Video Saliency<br> Representation Learning on Visual-Symbolic Graphs for Video Understanding<br> S3Net: Semantic-Aware Self-supervised Depth Estimation with Monocular Videos and Synthetic Data<br> High-Quality Single-Model Deep Video Compression with Frame-Conv3D and Multi-frame Differential Modulation</p><h3 id="_2021-iccv" tabindex="-1"><a class="header-anchor" href="#_2021-iccv" aria-hidden="true">#</a> 2021 ICCV</h3></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/keyan/video.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 799976781@qq.com">WillebrordSnell</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="waline-wrapper" darkmode="false" style="display:block;"><div data-waline provider="Waline"><!--v-if--><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">昵称</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">邮箱</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">网址</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="请留言。(填写邮箱可在被回复时收到邮件提醒)"></textarea><div class="wl-preview" style="display:none;"><hr><h4>预览:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="表情" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="表情包"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="上传图片"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="预览"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-captcha-container"></div><div class="wl-text-number">0 <!--v-if-->  字</div><button type="button" class="wl-btn">登录</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->提交<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="搜索表情包"><!--v-if--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> 评论</div><ul class="wl-sort"><!--[--><li class="active">按正序</li><li class="">按倒序</li><li class="">按热度</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><!--[--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><!--]--><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v2.15.8</div></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">西湖美景, 三月天嘞~</div><div class="vp-copyright">Copyright © 2023 Mr.R</div></footer></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-d9b91c43.js" defer></script>
  </body>
</html>
