<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://mister-hope.github.io/keyan/videoUnderstanding/videoUnderstanding.html"><meta property="og:site_name" content=" "><meta property="og:title" content="视频理解综述性质的记录"><meta property="og:description" content="视频理解综述性质的记录 简介 视频数据本身就是一种多模态信息，并且是一种很丰富的数据来源，其包含的信息远远多于2D的图片，例如有物体移动的信息，以及长期的时序信息，音频信号等，并且视频数据是一种天生的数据增强，比如在一段视频中一个物体会有各种各样的变化，形变，遮挡，光照变化等，这种改变通常是十分丰富且自由的，远比通过一些图片处理得到的数据增强要自然的多。视频理解领域三巨头就是：Action Recognition、Temporal Action Localization、Spatio-Temporal Action Localization"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-10-18T15:20:18.000Z"><meta property="article:author" content="Mr.R"><meta property="article:tag" content="视频理解"><meta property="article:tag" content="视频对话"><meta property="article:published_time" content="2023-10-09T00:00:00.000Z"><meta property="article:modified_time" content="2023-10-18T15:20:18.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"视频理解综述性质的记录","image":[""],"datePublished":"2023-10-09T00:00:00.000Z","dateModified":"2023-10-18T15:20:18.000Z","author":[{"@type":"Person","name":"Mr.R","url":"https://mister-hope.com"}]}</script><title>视频理解综述性质的记录 |  </title><meta name="description" content="视频理解综述性质的记录 简介 视频数据本身就是一种多模态信息，并且是一种很丰富的数据来源，其包含的信息远远多于2D的图片，例如有物体移动的信息，以及长期的时序信息，音频信号等，并且视频数据是一种天生的数据增强，比如在一段视频中一个物体会有各种各样的变化，形变，遮挡，光照变化等，这种改变通常是十分丰富且自由的，远比通过一些图片处理得到的数据增强要自然的多。视频理解领域三巨头就是：Action Recognition、Temporal Action Localization、Spatio-Temporal Action Localization">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-13796ab9.css" as="style"><link rel="stylesheet" href="/assets/style-13796ab9.css">
    <link rel="modulepreload" href="/assets/app-74fdf822.js"><link rel="modulepreload" href="/assets/videoUnderstanding.html-7613b61c.js"><link rel="modulepreload" href="/assets/videoUnderstanding.html-04610dc6.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="prefetch" href="/assets/index.html-ac47bc91.js" as="script"><link rel="prefetch" href="/assets/intro.html-0cbcbfad.js" as="script"><link rel="prefetch" href="/assets/slides.html-a4360897.js" as="script"><link rel="prefetch" href="/assets/202309.html-b8b8b02a.js" as="script"><link rel="prefetch" href="/assets/202310.html-a8f07497.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-313493aa.js" as="script"><link rel="prefetch" href="/assets/index.html-c19cc023.js" as="script"><link rel="prefetch" href="/assets/disable.html-de6f1fb7.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-a961ffbf.js" as="script"><link rel="prefetch" href="/assets/markdown.html-d64453e3.js" as="script"><link rel="prefetch" href="/assets/page.html-a14c059e.js" as="script"><link rel="prefetch" href="/assets/index.html-6098c265.js" as="script"><link rel="prefetch" href="/assets/dialog.html-d3391826.js" as="script"><link rel="prefetch" href="/assets/video.html-fe0150ef.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-618c6010.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-593e8df9.js" as="script"><link rel="prefetch" href="/assets/videoRepresentation.html-47b5e20b.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-f32487f7.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-b3725aa6.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-55392bfb.js" as="script"><link rel="prefetch" href="/assets/DDP.html-f2b6dbc8.js" as="script"><link rel="prefetch" href="/assets/dan.html-9609164e.js" as="script"><link rel="prefetch" href="/assets/trick.html-b74e4cb5.js" as="script"><link rel="prefetch" href="/assets/404.html-130ff98b.js" as="script"><link rel="prefetch" href="/assets/index.html-10998ddb.js" as="script"><link rel="prefetch" href="/assets/index.html-e05d3217.js" as="script"><link rel="prefetch" href="/assets/index.html-cb0bac63.js" as="script"><link rel="prefetch" href="/assets/index.html-7fe952c2.js" as="script"><link rel="prefetch" href="/assets/index.html-82f2092e.js" as="script"><link rel="prefetch" href="/assets/index.html-e4678c97.js" as="script"><link rel="prefetch" href="/assets/index.html-c64b2618.js" as="script"><link rel="prefetch" href="/assets/index.html-7386b951.js" as="script"><link rel="prefetch" href="/assets/index.html-8a440362.js" as="script"><link rel="prefetch" href="/assets/index.html-305bc367.js" as="script"><link rel="prefetch" href="/assets/index.html-e683e385.js" as="script"><link rel="prefetch" href="/assets/index.html-b116232f.js" as="script"><link rel="prefetch" href="/assets/index.html-20b5478c.js" as="script"><link rel="prefetch" href="/assets/index.html-836ec29d.js" as="script"><link rel="prefetch" href="/assets/index.html-e7d44e3b.js" as="script"><link rel="prefetch" href="/assets/index.html-cccb4d6a.js" as="script"><link rel="prefetch" href="/assets/index.html-0917fcc6.js" as="script"><link rel="prefetch" href="/assets/index.html-52864055.js" as="script"><link rel="prefetch" href="/assets/index.html-c46cc60b.js" as="script"><link rel="prefetch" href="/assets/index.html-47792a83.js" as="script"><link rel="prefetch" href="/assets/index.html-aa30eeef.js" as="script"><link rel="prefetch" href="/assets/index.html-59dfac95.js" as="script"><link rel="prefetch" href="/assets/index.html-739d9637.js" as="script"><link rel="prefetch" href="/assets/index.html-5fabefb9.js" as="script"><link rel="prefetch" href="/assets/index.html-6476eeca.js" as="script"><link rel="prefetch" href="/assets/index.html-e909ac63.js" as="script"><link rel="prefetch" href="/assets/index.html-7cef4089.js" as="script"><link rel="prefetch" href="/assets/index.html-46567c63.js" as="script"><link rel="prefetch" href="/assets/index.html-8498dbec.js" as="script"><link rel="prefetch" href="/assets/index.html-4d1b2642.js" as="script"><link rel="prefetch" href="/assets/index.html-b32b66eb.js" as="script"><link rel="prefetch" href="/assets/index.html-1d386275.js" as="script"><link rel="prefetch" href="/assets/index.html-749de246.js" as="script"><link rel="prefetch" href="/assets/index.html-66325375.js" as="script"><link rel="prefetch" href="/assets/index.html-61263a6e.js" as="script"><link rel="prefetch" href="/assets/index.html-0e27a9a6.js" as="script"><link rel="prefetch" href="/assets/index.html-e6e96cde.js" as="script"><link rel="prefetch" href="/assets/index.html-03cae17f.js" as="script"><link rel="prefetch" href="/assets/intro.html-f5244ae9.js" as="script"><link rel="prefetch" href="/assets/slides.html-b03099cb.js" as="script"><link rel="prefetch" href="/assets/202309.html-38004f2a.js" as="script"><link rel="prefetch" href="/assets/202310.html-30aff286.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-d02bbcc3.js" as="script"><link rel="prefetch" href="/assets/index.html-086d0b0a.js" as="script"><link rel="prefetch" href="/assets/disable.html-6fec57b1.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-8393844d.js" as="script"><link rel="prefetch" href="/assets/markdown.html-f2d7c06e.js" as="script"><link rel="prefetch" href="/assets/page.html-ebc1090b.js" as="script"><link rel="prefetch" href="/assets/index.html-ece04cf2.js" as="script"><link rel="prefetch" href="/assets/dialog.html-baece10e.js" as="script"><link rel="prefetch" href="/assets/video.html-51f15773.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-c6ac5b05.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-96967b5d.js" as="script"><link rel="prefetch" href="/assets/videoRepresentation.html-c9895fcc.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-41ef006a.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-d2fc7f09.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-9d81eaae.js" as="script"><link rel="prefetch" href="/assets/DDP.html-37f2a7c8.js" as="script"><link rel="prefetch" href="/assets/dan.html-3cbee6d0.js" as="script"><link rel="prefetch" href="/assets/trick.html-2f539e22.js" as="script"><link rel="prefetch" href="/assets/404.html-8a5e4649.js" as="script"><link rel="prefetch" href="/assets/index.html-005de4ce.js" as="script"><link rel="prefetch" href="/assets/index.html-b2157bce.js" as="script"><link rel="prefetch" href="/assets/index.html-8524e8e7.js" as="script"><link rel="prefetch" href="/assets/index.html-c6e57a1d.js" as="script"><link rel="prefetch" href="/assets/index.html-e34696d8.js" as="script"><link rel="prefetch" href="/assets/index.html-c34972f4.js" as="script"><link rel="prefetch" href="/assets/index.html-e10882a3.js" as="script"><link rel="prefetch" href="/assets/index.html-e59c95ae.js" as="script"><link rel="prefetch" href="/assets/index.html-e9a51674.js" as="script"><link rel="prefetch" href="/assets/index.html-f7109581.js" as="script"><link rel="prefetch" href="/assets/index.html-bfd9926d.js" as="script"><link rel="prefetch" href="/assets/index.html-5f0bf8a5.js" as="script"><link rel="prefetch" href="/assets/index.html-6dc43d1c.js" as="script"><link rel="prefetch" href="/assets/index.html-da901163.js" as="script"><link rel="prefetch" href="/assets/index.html-9442be3b.js" as="script"><link rel="prefetch" href="/assets/index.html-8e6157b2.js" as="script"><link rel="prefetch" href="/assets/index.html-ad34391b.js" as="script"><link rel="prefetch" href="/assets/index.html-889f46c7.js" as="script"><link rel="prefetch" href="/assets/index.html-a6790584.js" as="script"><link rel="prefetch" href="/assets/index.html-ccc2267f.js" as="script"><link rel="prefetch" href="/assets/index.html-72276f1c.js" as="script"><link rel="prefetch" href="/assets/index.html-cf1eeaa6.js" as="script"><link rel="prefetch" href="/assets/index.html-0c4f9e5e.js" as="script"><link rel="prefetch" href="/assets/index.html-fc713a35.js" as="script"><link rel="prefetch" href="/assets/index.html-1d313839.js" as="script"><link rel="prefetch" href="/assets/index.html-bef2066d.js" as="script"><link rel="prefetch" href="/assets/index.html-4291d6a6.js" as="script"><link rel="prefetch" href="/assets/index.html-c9c1a728.js" as="script"><link rel="prefetch" href="/assets/index.html-4ed79f24.js" as="script"><link rel="prefetch" href="/assets/index.html-d8d3a09c.js" as="script"><link rel="prefetch" href="/assets/index.html-18397d3a.js" as="script"><link rel="prefetch" href="/assets/index.html-f3bff45e.js" as="script"><link rel="prefetch" href="/assets/index.html-0d8aede0.js" as="script"><link rel="prefetch" href="/assets/index.html-93cb600d.js" as="script"><link rel="prefetch" href="/assets/index.html-e409da94.js" as="script"><link rel="prefetch" href="/assets/index.html-dbc6d5f9.js" as="script"><link rel="prefetch" href="/assets/index.html-18611048.js" as="script"><link rel="prefetch" href="/assets/waline-meta-56fbc549.js" as="script"><link rel="prefetch" href="/assets/component-d4e4d75a.js" as="script"><link rel="prefetch" href="/assets/auto-fe80bb03.js" as="script"><link rel="prefetch" href="/assets/index-2bf332f6.js" as="script"><link rel="prefetch" href="/assets/flowchart-c441f34d.js" as="script"><link rel="prefetch" href="/assets/mermaid.core-b9780612.js" as="script"><link rel="prefetch" href="/assets/highlight.esm-75b11b9d.js" as="script"><link rel="prefetch" href="/assets/markdown.esm-9d5bc2ce.js" as="script"><link rel="prefetch" href="/assets/math.esm-70a288c8.js" as="script"><link rel="prefetch" href="/assets/notes.esm-a106bb2c.js" as="script"><link rel="prefetch" href="/assets/reveal.esm-1a4c3ae7.js" as="script"><link rel="prefetch" href="/assets/search.esm-7e6792e2.js" as="script"><link rel="prefetch" href="/assets/zoom.esm-b83b91d0.js" as="script"><link rel="prefetch" href="/assets/VuePlayground-98989328.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-5762295a.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><img class="vp-nav-logo" src="/logo.svg" alt=" "><!----><span class="vp-site-name hide-in-pad"> </span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="码头"><span class="title"><!---->码头</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="视频理解" class="vp-link nav-link active nav-link active" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解<!----></a></li><li class="dropdown-item"><a aria-label="视频表征" class="vp-link nav-link nav-link" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->视频表征<!----></a></li><li class="dropdown-item"><a aria-label="对比学习" class="vp-link nav-link nav-link" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习<!----></a></li><li class="dropdown-item"><a aria-label="多模态" class="vp-link nav-link nav-link" href="/keyan/multiModal/multiModal.html"><!---->多模态<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="炉"><span class="title"><!---->炉</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="单机多卡DDP" class="vp-link nav-link nav-link" href="/train/DDP/DDP.html"><!---->单机多卡DDP<!----></a></li><li class="dropdown-item"><a aria-label="AVSD" class="vp-link nav-link nav-link" href="/train/AVSD/AVSD.html"><!---->AVSD<!----></a></li><li class="dropdown-item"><a aria-label="奇淫技巧" class="vp-link nav-link nav-link" href="/train/trick/trick.html"><!---->奇淫技巧<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="道心"><span class="title"><!---->道心</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="2023年9月" class="vp-link nav-link nav-link" href="/book/202309.html"><!---->2023年9月<!----></a></li><li class="dropdown-item"><a aria-label="2023年10月" class="vp-link nav-link nav-link" href="/book/202310.html"><!---->2023年10月<!----></a></li><li class="dropdown-item"><a aria-label="毛泽东选集" class="vp-link nav-link nav-link" href="/book/maoxuan.html"><!---->毛泽东选集<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="金樽"><span class="title"><!---->金樽</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>游戏</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="死亡搁浅" class="vp-link nav-link nav-link" href="/life/game/DeathStranding.html"><!---->死亡搁浅<!----></a></li><li class="dropdown-subitem"><a aria-label="血缘诅咒" class="vp-link nav-link nav-link" href="/life/game/Bloodborne.html"><!---->血缘诅咒<!----></a></li></ul></li></ul></button></div></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><!--[--><a aria-label="视频理解综述性质的记录" class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active nav-link active vp-sidebar-link vp-sidebar-page active" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="简介" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#简介"><!---->简介<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="DeepVideo：Large-scale Video Classification with Convolutional Neural Networks" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#deepvideo-large-scale-video-classification-with-convolutional-neural-networks"><!---->DeepVideo：Large-scale Video Classification with Convolutional Neural Networks<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="双流网络：Two-Stream Convolutional Networks for Action Recognition in Videos (2014)" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#双流网络-two-stream-convolutional-networks-for-action-recognition-in-videos-2014"><!---->双流网络：Two-Stream Convolutional Networks for Action Recognition in Videos (2014)<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Beyond-short-snippets：Beyond Short Snippets: Deep Networks for Video Classification" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#beyond-short-snippets-beyond-short-snippets-deep-networks-for-video-classification"><!---->Beyond-short-snippets：Beyond Short Snippets: Deep Networks for Video Classification<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Convolutional fusion：Beyond Short Snippets: Deep Networks for Video Classification" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#convolutional-fusion-beyond-short-snippets-deep-networks-for-video-classification"><!---->Convolutional fusion：Beyond Short Snippets: Deep Networks for Video Classification<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="TSN：Convolutional Two-Stream Network Fusion for Video Action Recognition" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#tsn-convolutional-two-stream-network-fusion-for-video-action-recognition"><!---->TSN：Convolutional Two-Stream Network Fusion for Video Action Recognition<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="C3D：Learning Spatiotemporal Features with 3D Convolutional Networks" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#c3d-learning-spatiotemporal-features-with-3d-convolutional-networks"><!---->C3D：Learning Spatiotemporal Features with 3D Convolutional Networks<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="I3D：Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#i3d-quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset"><!---->I3D：Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Non-local：Non-local Neural Networks" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#non-local-non-local-neural-networks"><!---->Non-local：Non-local Neural Networks<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="R(2+1)D：A Closer Look at Spatiotemporal Convolutions for Action Recognition" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#r-2-1-d-a-closer-look-at-spatiotemporal-convolutions-for-action-recognition"><!---->R(2+1)D：A Closer Look at Spatiotemporal Convolutions for Action Recognition<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="SlowFast Networks for Video Recognition" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#slowfast-networks-for-video-recognition"><!---->SlowFast Networks for Video Recognition<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Is Space-Time Attention All You Need for Video Understanding?" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/keyan/videoUnderstanding/videoUnderstanding.html#is-space-time-attention-all-you-need-for-video-understanding"><!---->Is Space-Time Attention All You Need for Video Understanding?<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul><!--]--></li><li><!--[--><a aria-label="关于视频理解的论文收集(较新)" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->关于视频理解的论文收集(较新)<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="对比学习综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="多模态方向综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/multiModal/multiModal.html"><!---->多模态方向综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->视频理解综述性质的记录</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://mister-hope.com" target="_blank" rel="noopener noreferrer">Mr.R</a></span><span property="author" content="Mr.R"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-10-09T00:00:00.000Z"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="waline-pageview-count" data-path="/keyan/videoUnderstanding/videoUnderstanding.html">...</span></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 20 分钟</span><meta property="timeRequired" content="PT20M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category0 clickable" role="navigation">码头</span><!--]--><meta property="articleSection" content="码头"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag4 clickable" role="navigation">视频理解</span><span class="page-tag-item tag5 clickable" role="navigation">视频对话</span><!--]--><meta property="keywords" content="视频理解,视频对话"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#简介">简介</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#deepvideo-large-scale-video-classification-with-convolutional-neural-networks">DeepVideo：Large-scale Video Classification with Convolutional Neural Networks</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#双流网络-two-stream-convolutional-networks-for-action-recognition-in-videos-2014">双流网络：Two-Stream Convolutional Networks for Action Recognition in Videos (2014)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#beyond-short-snippets-beyond-short-snippets-deep-networks-for-video-classification">Beyond-short-snippets：Beyond Short Snippets: Deep Networks for Video Classification</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#convolutional-fusion-beyond-short-snippets-deep-networks-for-video-classification">Convolutional fusion：Beyond Short Snippets: Deep Networks for Video Classification</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#tsn-convolutional-two-stream-network-fusion-for-video-action-recognition">TSN：Convolutional Two-Stream Network Fusion for Video Action Recognition</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#c3d-learning-spatiotemporal-features-with-3d-convolutional-networks">C3D：Learning Spatiotemporal Features with 3D Convolutional Networks</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#i3d-quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset">I3D：Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#non-local-non-local-neural-networks">Non-local：Non-local Neural Networks</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#r-2-1-d-a-closer-look-at-spatiotemporal-convolutions-for-action-recognition">R(2+1)D：A Closer Look at Spatiotemporal Convolutions for Action Recognition</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#slowfast-networks-for-video-recognition">SlowFast Networks for Video Recognition</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#is-space-time-attention-all-you-need-for-video-understanding">Is Space-Time Attention All You Need for Video Understanding?</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="视频理解综述性质的记录" tabindex="-1"><a class="header-anchor" href="#视频理解综述性质的记录" aria-hidden="true">#</a> 视频理解综述性质的记录</h1><h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介" aria-hidden="true">#</a> 简介</h2><p>视频数据本身就是一种多模态信息，并且是一种很丰富的数据来源，其包含的信息远远多于2D的图片，例如有物体移动的信息，以及长期的时序信息，音频信号等，并且视频数据是一种天生的数据增强，比如在一段视频中一个物体会有各种各样的变化，形变，遮挡，光照变化等，这种改变通常是十分丰富且自由的，远比通过一些图片处理得到的数据增强要自然的多。视频理解领域三巨头就是：Action Recognition、Temporal Action Localization、Spatio-Temporal Action Localization</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>考虑一下从视频当中设定一些自监督信号</p></div><p>视频理解领域发展大致如下：<strong>双流网络——TSN——I3D——SlowFast——TimeSformer</strong></p><details class="hint-container details"><summary>详情</summary><p>DeepVideo (<a href="https://cs.stanford.edu/people/karpathy/deepvideo/" target="_blank" rel="noopener noreferrer">Large-scale Video Classification with Convolutional Neural Networks<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>提出sports1M数据集，用深度学习做视频理解</p></blockquote><p>双流网络 (<a href="https://arxiv.org/abs/1406.2199" target="_blank" rel="noopener noreferrer">Two-Stream Convolutional Networks for Action Recognition in Videos<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>引入光流做时序建模，神经网络首次超越手工特征</p></blockquote><p>C3D (<a href="https://arxiv.org/abs/1412.0767" target="_blank" rel="noopener noreferrer">Learning Spatiotemporal Features with 3D Convolutional Networks<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>比较深的3D-CNN做视频理解</p></blockquote><p>Beyond-short-snippets (<a href="https://arxiv.org/abs/1503.08909" target="_blank" rel="noopener noreferrer">Beyond Short Snippets: Deep Networks for Video Classification<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>尝试使用LSTM</p></blockquote><p>Convolutional fusion (<a href="https://arxiv.org/abs/1604.06573" target="_blank" rel="noopener noreferrer">Beyond Short Snippets: Deep Networks for Video Classification<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>做early fusion来加强时空间建模</p></blockquote><p>TSN (<a href="https://arxiv.org/abs/1608.00859" target="_blank" rel="noopener noreferrer">Convolutional Two-Stream Network Fusion for Video Action Recognition<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>超级有效的视频分段建模，bag of tricks in video</p></blockquote><p>I3D (<a href="https://arxiv.org/abs/1705.07750" target="_blank" rel="noopener noreferrer">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>提出Kinetics数据集，膨胀2D网络到3D，开启3D-CNN时代</p></blockquote><p>R2+1D (<a href="https://arxiv.org/abs/1711.11248" target="_blank" rel="noopener noreferrer">A Closer Look at Spatiotemporal Convolutions for Action Recognition<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>拆分3D卷积核，使3D网络容易优化</p></blockquote><p>Non-local (<a href="https://arxiv.org/abs/1711.07971" target="_blank" rel="noopener noreferrer">Non-local Neural Networks<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>引入自注意力做视觉问题</p></blockquote><p>SlowFast (<a href="https://arxiv.org/abs/1812.03982" target="_blank" rel="noopener noreferrer">SlowFast Networks for Video Recognition<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>快慢两支路提升效率</p></blockquote><p>TimeSformer (<a href="https://arxiv.org/abs/2102.05095" target="_blank" rel="noopener noreferrer">Is Space-Time Attention All You Need for Video Understanding?<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</p><blockquote><p>视频中第一个引入transformer，开启video transformer时代</p></blockquote></details><h2 id="deepvideo-large-scale-video-classification-with-convolutional-neural-networks" tabindex="-1"><a class="header-anchor" href="#deepvideo-large-scale-video-classification-with-convolutional-neural-networks" aria-hidden="true">#</a> DeepVideo：Large-scale Video Classification with Convolutional Neural Networks</h2><blockquote><p>论文地址：<a href="https://cs.stanford.edu/people/karpathy/deepvideo/" target="_blank" rel="noopener noreferrer">https://cs.stanford.edu/people/karpathy/deepvideo/<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文属于最早期的通过深度学习方法处理视频模态的信息，其想法是比较直接的：就是如何把图片识别应用到视频识别里面。视频比图片多一个时间维度，因此需要考虑几个变体来适应视频信息的输入<br><img src="/assets/image-6-e173c751.png" alt="几种范式" loading="lazy"></p><ol><li><p><strong>Single Frame</strong>：其实这种方式就是图片分类的任务，就是取其中一帧通过CNN网络得到特征(但事实上后续也有实验证明只要取得一帧关键帧里面的信息就可以在很多任务上得出不错的结果，所以目前也有很多网络其实并未很好的学习到视频里面的时序信息)，这种方法相当于一种baseline，也就是完全没有用到时序信息</p></li><li><p><strong>Late Fusion</strong>：选取视频几帧作为输入，最后分别通过CNN(通常是权值共享的)得到特征最后把这些得到的特征进行融合，虽然做法上还是类似图片分类，但是最后有fusion操作所以稍微有利用到一点时序信息</p></li><li><p><strong>Early Fusion</strong>：此处采取的是十分暴力的融和方法，直接把n帧视频帧在rgb通道上进行融合，也就是说本来是 h × w × 3 的图片变成 h × w × 3n</p></li><li><p><strong>Slow Fusion</strong>：相当于2+3,效果较强于前面几种方法，但是也大差不差。</p></li></ol><p>值得注意的是，上面几种方法其实不如原始的手工选取特征(多么痛的领悟)。</p><figure><img src="/assets/image-7-17772220.png" alt="本文方法" tabindex="0" loading="lazy"><figcaption>本文方法</figcaption></figure><p>该方法是采取一种多分辨率的结构，也就是说一边输入的是完整的视频，一边输入的是从视频正中间扣出来的小框得到的视频。</p><figure><img src="/assets/image-8-c8f9787d.png" alt="实验" tabindex="0" loading="lazy"><figcaption>实验</figcaption></figure><p>Late Fusion、Early Fusion的实验结果都不如baseline，只有Slow Fusion经过一顿操作之后勉强比Single Frame性能高一点点。而且其实在UCF-101上的结果显示，最好的Slow Fusion性能仅65%而通过手工选取特征的结果也有87%。但<strong>本文的主要贡献在于提出了一个新的数据集，并且把从CNN处理图片的方式直观地运用到视频领域，起了抛砖引玉的作用</strong>。</p><h2 id="双流网络-two-stream-convolutional-networks-for-action-recognition-in-videos-2014" tabindex="-1"><a class="header-anchor" href="#双流网络-two-stream-convolutional-networks-for-action-recognition-in-videos-2014" aria-hidden="true">#</a> 双流网络：Two-Stream Convolutional Networks for Action Recognition in Videos (2014)</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/1406.2199" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1406.2199<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>双流网络，故名思意就是用两个卷积神经网络做有关video的动作识别分类</p><figure><img src="/assets/image-3876d241.png" alt="时空卷积网络" tabindex="0" loading="lazy"><figcaption>时空卷积网络</figcaption></figure><p>早期处理视频的方式就是：抽取一些关键帧，再通过conv，最后把结果合并起来；或者就是把一些帧叠起来当作一个整体的输入丢进神经网络，再在网络里面做一些early fusion或者late fusion达到一种时空学习的效果。但上面方法效果都差强人意，甚至达不到手工设置特征的效果，本文作者发现<strong>卷积神经网络只能处理好一些局部的特征而不是视频中物体的运动规律</strong>。因此本文通过先抽取motion-information(通过光流)到最后的动作映射实现对视频信息的处理。</p><p>上层叫做空间流卷积神经网络，下层关注motion-information的叫时间流卷积神经网络。两个网络分别输入单帧图片和光流图片，最后得到对应的分类概率，再通过加权平均得到最终的预测。</p><figure><img src="/assets/image-1-885f4aa0.png" alt="光流图" tabindex="0" loading="lazy"><figcaption>光流图</figcaption></figure><p>光流简要来说就是用来观测场景中运动物体的轨迹信息，物体在于上一帧的位置差距越大则颜色越明显，反正越黑。光流能够忽略背景噪声，从而完全关注动作本身。但是光流目前也比较受人诟病：例如计算光流需要时间很长，并且占用的硬盘空间比较大。</p><p>本文作者认为一些视觉信息(颜色，位置等)是可以通过CNN这种提取局部信息的网络得到，但是动作信息(光流部分)需要通过其他网络进行学习。并且作者提到即使在只有少量训练数据的情况下一个直接在光流训练数据上训练的网络也有一个不错的结果，<strong>因此其实某种意义来本文提升性能的关键在于光流数据的使用</strong></p><figure><img src="/assets/image-2-40f18a99.png" alt="光流介绍" tabindex="0" loading="lazy"><figcaption>光流介绍</figcaption></figure><p>图片里面每个像素点都对应一个光流点，并且光流一般维度为 H × W × 2,最后一个维度通常是 水平上(运动) 和 垂直上，并且两帧图片之间得到一个光流图，也就是说L帧的视频能够得到L-1张光流图。</p><figure><img src="/assets/image-3-dacb4255.png" alt="光流处理" tabindex="0" loading="lazy"><figcaption>光流处理</figcaption></figure><p>左：直接把所有的光流图叠加在一起，这种方式的好处就是比较简单，没有多余的处理工作，但是没有充分的利用到光流信息</p><p>右：在光流的轨迹上进行光流数值的叠加，也就是说在第一张图的P1已经在下一张图运动到P2的时候，就在第二图里从P2找下一张图P2运动终点的位置。(虽然右边方法比左边要合理，可是本文最后实验证明左边的方式效果比右边的好)</p><p><strong>不过作者同样做了Bi-directional操作，因为倒放也是合理的(笑)，并且一般来说这种双向操作都是可以涨点的(起码不会掉点)，例如BERT、pyramid、cascade里面类似</strong></p><p>未来工作：</p><ol><li>本文在空间流上使用了预训练模型，那么处理光流数据的时间流是否同样可以通过预训练网络得到更好的结果</li><li>基于轨迹叠加光流数据的方法按理来说效果应该更好，但是在本文的实验中并未显示出来它的优势</li><li>视频里面通常带有camera-motion，也就是说相机自带的移动会影响光流里面物体运动信息的计算，也就是说如何避免这种全局的影响(本文使用的是直接减去一个平均)</li><li>能否直接做early fusion，或者其他复杂的late fusion，本文仅是加权平均。</li><li>按理来说LSTM能很好的处理时序信息的，那么把一张张视频帧通过CNN得到特征之后再进入LSTM学习，最后得到特征应该会更强</li><li>长时间的视频理解，而非本文中取10帧光流(0.5s左右)，因为一般来说一个动作时间大概在2-3秒左右</li></ol><h2 id="beyond-short-snippets-beyond-short-snippets-deep-networks-for-video-classification" tabindex="-1"><a class="header-anchor" href="#beyond-short-snippets-beyond-short-snippets-deep-networks-for-video-classification" aria-hidden="true">#</a> Beyond-short-snippets：Beyond Short Snippets: Deep Networks for Video Classification</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/1503.08909" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1503.08909<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><figure><img src="/assets/image-9-c4c068d2.png" alt="主要方法" tabindex="0" loading="lazy"><figcaption>主要方法</figcaption></figure><figure><img src="/assets/image-10-82e4da64.png" alt="LSTM部分" tabindex="0" loading="lazy"><figcaption>LSTM部分</figcaption></figure><p>本文通过LSTM进一步处理从CNN里面提取出的特征，其效果理应得到进一步提升，但是从实验结果看来提升的幅度十分有限。</p><figure><img src="/assets/image-11-22d26d0c.png" alt="实验结果" tabindex="0" loading="lazy"><figcaption>实验结果</figcaption></figure><p>虽然看似实验结果比之前好不少，但是其实主要贡献来自于使用了光流而非LSTM</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>LSTM更加适合具有变化的输入，这样LSTM才能起到学习语义信息的作用，像是这种六七秒的视频可能本身就没有语义信息的改变，这样就相当于把很多一摸一样的输入传递给LSTM，也就根本学不出个东西出来，所以在本文里面效果一般</p></div><h2 id="convolutional-fusion-beyond-short-snippets-deep-networks-for-video-classification" tabindex="-1"><a class="header-anchor" href="#convolutional-fusion-beyond-short-snippets-deep-networks-for-video-classification" aria-hidden="true">#</a> Convolutional fusion：Beyond Short Snippets: Deep Networks for Video Classification</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/1503.08909" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1503.08909<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文主要讲述当有了这种视频帧(空间流)+光流(时间流)的输入之后应该如何做fusion</p><figure><img src="/assets/image-12-47c37024.png" alt="一个粒子" tabindex="0" loading="lazy"><figcaption>一个粒子</figcaption></figure><ol><li>Spatial fusion：作者的用意是想确定，当有了时间流和空间流两个网络之后如何保证这两个流的特征图能够在同样的位置上产生的通道response是差不多能够联系起来的，也就是说在特征图层面就做一个fusion(early fusion)</li></ol><blockquote><p>例如：max fusion：在a(空间流)，b(时间流)的特征图中各个像素位置点上取max；<br> concatenation fusion：直接合并就完事；<br> conv fusion：先把a，b堆叠起来，再做一层卷积操作；<br> sum fusion：直接对应位置做加法；<br> Bilinear fusion：在a，b上做一个out product，乘积之后再在所有维度上取一个加权平均；</p></blockquote><ol start="2"><li>Temporal fusion：当有很多视频帧，每一帧抽得特征之后，如何在时间轴维度上把它们合并起来。本文采取两种方式：3D Pooling和3D Conv + 3D pooling</li></ol><figure><img src="/assets/image-14-b81a454f.png" alt="Temporal fusion" tabindex="0" loading="lazy"><figcaption>Temporal fusion</figcaption></figure><ol start="3"><li>何时fusion：作者经过大量消融实验得到两种比较好的方式，也就是下图所示。</li></ol><figure><img src="/assets/image-13-f8f433c0.png" alt="fusion位置" tabindex="0" loading="lazy"><figcaption>fusion位置</figcaption></figure><figure><img src="/assets/image-15-4507ac89.png" alt="总体框架" tabindex="0" loading="lazy"><figcaption>总体框架</figcaption></figure><p>蓝色代表空间流，绿色代表时间流。</p><p>从框架图里面可以看出，本文先是做了一个spatialtemporal branch然后再专门地做了temporal branch，其原因在于之前的双流网络也发现了视频领域里面这种时间学习是很有必要的。不过在推理的时候其实还是和双流网络一样，两个branch通过一个加权平均做late fusion最后得到分类结果</p><figure><img src="/assets/image-16-64b9ef2f.png" alt="才艺展示" tabindex="0" loading="lazy"><figcaption>才艺展示</figcaption></figure><p>本文将双流网络中原始网络替换成VGG-16(更深)后在UCF上有所提升(3个多点)，但在HMDB51上略微有所下降(2个点)。<strong>其实很大程度的原因就是当一个数据集特别小的时候，但是要用比较深的网络进行训练的时候就容易造成过拟合的问题</strong>，所以其实不见得用容量更大的、更深的网络就能得到更好的结果。</p><p>不过呢，本文提出的方法虽然在UCF上有所提升(0.8个多点)，但在HMDB51上提升确实十分明显(近7个点)，说明这种early fusion可能算是一种变相的对网络的约束，让模型早期训练中能够先一步将空间流和时间流的特征相互弥补相互学习。</p><h2 id="tsn-convolutional-two-stream-network-fusion-for-video-action-recognition" tabindex="-1"><a class="header-anchor" href="#tsn-convolutional-two-stream-network-fusion-for-video-action-recognition" aria-hidden="true">#</a> TSN：Convolutional Two-Stream Network Fusion for Video Action Recognition</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/1608.00859" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1608.00859<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目仓库：<a href="https://github.com/ZJCV/TSN" target="_blank" rel="noopener noreferrer">https://github.com/ZJCV/TSN<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> (非官方)</p></blockquote><p>本文主要贡献在于通过一种特别简单的方式处理长时间的视频，而且效果很好。<strong>还有一大贡献就是确定了很多好用的技巧，后续工作可以利用好这些trick</strong>，例如怎么做数据增强、怎么做模型的初始化、怎么使用光流、使用哪个网络、如何防止过拟合。</p><figure><img src="/assets/image-17-88c95b9a.png" alt="网络结构" tabindex="0" loading="lazy"><figcaption>网络结构</figcaption></figure><p>本文的想法也确实简单，长视频处理不了把它切成n段不就成了吗。每一段里面选取一帧并且以这一帧作为起点得到光流图，再通过双流网络往下做得到logits，其中spatial convnet是共享参数的，temporal convnet也是共享参数的。</p><p>TSN的想法就是，如果视频不太长，里面只有一个事件或者动作的话，即使抽出来的帧表面不大一样，但其实最高层的语义信息应当是差不多的，所以空间流、时间流中的每段logits最后做一个segmental consensus(其实也是某种意义上的fusion，具体操作可以做加法、乘法、max、average，复杂的话也可用MLP,LSTM等)<br> 最后做一个late fusion(加权平均)得到最终分类结果。</p><p>文章提及的trick部分：</p><ol><li>cross-modality pre-training：作者把video和光流看做是两个不同的模态(倒也是)，但是在光流部分是没有一个足够大的数据集去做预训练的。在此，作者提出直接用Image-Net上预训练的模型来处理光流是能够很好的接受其信息的，不过需要在网络上把第一层conv修改一下结构和参数(Image-Net输入是rgb图像，也就是3个channel。光流图却是一次性输入10张光流图，x、y方向的位移共2个channel，也就是总共20个channel)。具体操作也是十分简单，直接把3个channel的参数做了平均，然后复制20遍。</li><li>通过Batch Normalization做一个模型的正则化：对于数据集小的task来说，使用BN虽然能够加速训练，但是也容易造成过拟合的问题。文本提出一种partial BN，具体来说就是第一层BN全打开，后面的BN层全冻住，这样做的意义就是因为输入变了(预训练模型输入是image，而现在是video和光流)所以第一层必须要学一学(某种意义上是一种跨模态对齐？)。<strong>对于一些小数据集的下游任务应该是十分有用的(没错 说的就是AVSD)</strong></li><li>data augmentation：本文重点提到corner cropping和scale-jittering，做corner cropping的理由就是如果是random cropping其实很难裁剪到video里面边边角角的地方，通常是截取到中间位置；而scale-jittering也就是改变视频长宽比增加输入图片的多样性</li></ol><h2 id="c3d-learning-spatiotemporal-features-with-3d-convolutional-networks" tabindex="-1"><a class="header-anchor" href="#c3d-learning-spatiotemporal-features-with-3d-convolutional-networks" aria-hidden="true">#</a> C3D：Learning Spatiotemporal Features with 3D Convolutional Networks</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/1412.0767" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1412.0767<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p><img src="/assets/1697263294366-333aa7d8.jpg" alt="框架" loading="lazy"><br> 其实C3D相当于把vgg里面每个block去掉一层conv层再把卷积核变成3D的，但是在本文里作者发现微调这个网络费时费力，不如直接用fc6抽出来的特征再训练一个SVM分类器，本文C3D一半特指fc6中提取出来的特征</p><h2 id="i3d-quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset" tabindex="-1"><a class="header-anchor" href="#i3d-quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset" aria-hidden="true">#</a> I3D：Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/1705.07750" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1705.07750<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目仓库：<a href="https://github.com/google-deepmind/kinetics-i3d" target="_blank" rel="noopener noreferrer">https://github.com/google-deepmind/kinetics-i3d<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文一大亮点就是将2D的一些网络扩张(inflated)到3D，其次就是新的数据集。但其实kinetics还是很special heavy，也就是说取视频最中间那一帧，仅用这一帧做图像分类，也可以取得不错的结果，不大需要完整的上下文信息和模型的时序能力。但是其实事实上，至今为止也没有一个数据集能说让模型真的能关注到时序上的信息，或者说是能够处理时间长的、复杂的视频，也难以扩展到生活中各个方面上去。</p><figure><img src="/assets/image-4-eaaba3f8.png" alt="视频理解的几种处理范式" tabindex="0" loading="lazy"><figcaption>视频理解的几种处理范式</figcaption></figure><p>过去常见视频理解领域的处理方法就是：</p><ol><li>CNN + LSTM：CNN抽取特征，LSTM用来处理时序信息</li><li>直接训练3D网络</li><li>双流网络</li></ol><p>因此作者提出：<br> 4. <strong>双流I3D (目前大多数视频领域下游任务抽取出来的特征都是用本文提出的网络得到的)</strong><br> note：后续很多实验证明，先做2D CNN最后再做3D CNN效果比较好。</p><p>UCF-101和HMDB-51差不多是13000、7000多个视频，可见数量是十分之少的，其实是很难发挥出深度学习的作用的。<strong>因此如果需要在AVSD上取得好的结果肯定是需要其他预训练网络的结果再做微调</strong><br> 本文里面提出的扩张(inflated)也很容易理解，例如ResNET-50里面所有的3×3卷积或者pooling直接变成3×3×3(所谓的3D CNN)，这样的话就可以不用专门的去设计一个针对视频理解的网络架构。因为之前在做2D图片的时候，其实CNN网络架构已经做了很多消融实验了，包括怎么选取kernal size、网络应该分成几个阶段、或者一个阶段里面应该包含多少residual block等，基于前人的工作，最直观的想法就是直接套用2D里面的工作应用到视频领域，比较video也是一帧帧图片叠加起来的。<br> 但是本文除了使用这种3D CNN以外其实还是用到了双流的结构，因为作者发现，仅仅扩张卷积核的维度最后效果也不会太好，因为毕竟卷积神经网络不擅长处理动作信息，因此还是需要时间流的网络进一步提高性能。<br><strong>目前其实视频领域没也有一个定论说到底2D、3D、transformer结构那种效果是最好的</strong></p><figure><img src="/assets/image-5-cfd36ee3.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><p>从图中可以看出，无论是时间流、空间流孰好孰坏，只要是把视频和光流结合到一起就会得到性能上的提升。<strong>说明不管怎样，光流的数据都能给模型的性能带来提升</strong></p><h2 id="non-local-non-local-neural-networks" tabindex="-1"><a class="header-anchor" href="#non-local-non-local-neural-networks" aria-hidden="true">#</a> Non-local：Non-local Neural Networks</h2><p><a href="https://arxiv.org/abs/1711.07971" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1711.07971<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/facebookresearch/video-nonlocal-net" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/video-nonlocal-net<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Non-local这篇文章把自注意力机制引入到视频领域之后直接卷疯了，后续很多工作都是通过Non-local的算子做各种不同的网络、各种不同的变体。总体而言就是自注意力确实有效，本文主要是采取Non-local的思想，也就是希望模型关注更多的上下文信息而不是一个局部的信息，并且因为该模块是个即插即用的模块，所以能够用到各个下游任务里面，泛化性很好。</p><figure><img src="/assets/image-18-4a300a53.png" alt="空间算子" tabindex="0" loading="lazy"><figcaption>空间算子</figcaption></figure><figure><img src="/assets/image-19-9850c89c.png" alt="消融实验" tabindex="0" loading="lazy"><figcaption>消融实验</figcaption></figure><blockquote><p>(a) Non-local block中自注意力如何计算：dot-product效果最好，也是transformer里面默认的计算方式。<br> (b) Non-local插在哪个block：res5层的时候可能由于特征图太小了，因此效果没那么好。<br> (c) Non-local应该插多少个：R50的结构是3463，因此插10个就是在每个conv后面接一个Non-local。<br> (d) Non-local在空间、时间、时空的对比：self-attention提出本来就是在特征图上处理的，因此可能就在space上面做attention就可以了，没有必要做space-time上的attention，不过结果显示时间、空间上一样重要，并且时空都做的话效果是最好的。<br> (g) Non-local能否在更长的视频里面表现更好，本文提出的动机就是希望能够处理这种上下文信息，如果只使用16/32帧(1秒)的话其实不大长也没什么长距离信息，因此作者通过输入128帧(4秒)的视频验证了Non-local确实对长距离的时序建模有好处的也能够抓住这种视频的上下文信息。</p></blockquote><figure><img src="/assets/image-20-d34330d4.png" alt="对比SOTA" tabindex="0" loading="lazy"><figcaption>对比SOTA</figcaption></figure><p>本文的方法要好过之前用光流的方法，并且能够超过以Inception-ResNet这个更强的网络为骨干、用三个模态信息训练的网络 <strong>(但是不证明光流效果不好，也许加上光流更好了呢)</strong>，某种意义上也给那些处理不了光流数据的实验室一个做视频理解领域相关实验的机会。</p><h2 id="r-2-1-d-a-closer-look-at-spatiotemporal-convolutions-for-action-recognition" tabindex="-1"><a class="header-anchor" href="#r-2-1-d-a-closer-look-at-spatiotemporal-convolutions-for-action-recognition" aria-hidden="true">#</a> R(2+1)D：A Closer Look at Spatiotemporal Convolutions for Action Recognition</h2><blockquote><p>论文地址：<a href="https://arxiv.org/abs/1711.11248" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1711.11248<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>本文是一篇实验性的论文，主要是为了验证在动作识别这个task里面，时空卷积应该怎么去做，是2D还是3D，还是2D + 3D，是怎么组合，串联或是并联。简要来说作者发现把3D拆成空间上的3D和时间上的1D能够取得更好的效果，并且训练也会简单很多</p><figure><img src="/assets/image-21-65a9c090.png" alt="网络结构性实验" tabindex="0" loading="lazy"><figcaption>网络结构性实验</figcaption></figure><figure><img src="/assets/image-22-55bade5b.png" alt="结果" tabindex="0" loading="lazy"><figcaption>结果</figcaption></figure><figure><img src="/assets/image-23-c85cbab3.png" alt="R2+1D示意图" tabindex="0" loading="lazy"><figcaption>R2+1D示意图</figcaption></figure><p>如图所示：简单来说就是空间上做1×d×d的卷积(时间上什么都不做)，然后做一个空间投射(目的是和C3D网络参数保持一致，最后做公平的性能对比)，最后做一个t×1×1的这样一个时间上的卷积。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>作者提出R(2+1)D的效果比C3D要好的原因就是：增强了网络的非线性，因为3D conv后面只有一个relu层，也就是只有一个非线性操作；现在做了两次卷积，后面有两个relu。</p></div><figure><img src="/assets/image-24-0720d828.png" alt="训练过程" tabindex="0" loading="lazy"><figcaption>训练过程</figcaption></figure><p>并且从训练角度，2D + 1D确实比3D的conv好学很多</p><figure><img src="/assets/image-25-313420ea.png" alt="SOTA性能对比" tabindex="0" loading="lazy"><figcaption>SOTA性能对比</figcaption></figure><p>R(2+1)D效果在单独的RGB和flow上效果比i3d效果好，但是做了late fusion之后效果却没有I3D的效果好。(作者此处也没给出解释)<br> 但是其实，R(2+1)D是用的112×112的输入，而I3D用的224×224。并且前者的训练过程相较于后者也确实有优化，能够降低过拟合。</p><h2 id="slowfast-networks-for-video-recognition" tabindex="-1"><a class="header-anchor" href="#slowfast-networks-for-video-recognition" aria-hidden="true">#</a> SlowFast Networks for Video Recognition</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/1812.03982" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1812.03982<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/facebookresearch/SlowFast" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/SlowFast<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>此篇介绍在<a href="/keyan/videoRepresentation/_videoRepresentation.md/#slowfast-networks-for-video-recognition" class="">视频理解综述性质记录</a>提及</p><h2 id="is-space-time-attention-all-you-need-for-video-understanding" tabindex="-1"><a class="header-anchor" href="#is-space-time-attention-all-you-need-for-video-understanding" aria-hidden="true">#</a> Is Space-Time Attention All You Need for Video Understanding?</h2><blockquote><p>论文地址：<a href="http://arxiv.org/abs/2102.05095" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2102.05095<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> 项目代码：<a href="https://github.com/facebookresearch/TimeSformer" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/TimeSformer<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p></blockquote><p>此篇介绍在<a href="/keyan/videoRepresentation/_videoRepresentation.md/#is-space-time-attention-all-you-need-for-video-understanding" class="">视频理解综述性质记录</a>提及</p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/keyan/videoUnderstanding/_videoUnderstanding.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 799976781@qq.com">WillebrordSnell</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><!----><a aria-label="关于视频理解的论文收集(较新)" class="vp-link nav-link next nav-link next" href="/keyan/videoRepresentation/videoRepresentation.html"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">关于视频理解的论文收集(较新)<!----></div></a></nav><div id="comment" class="waline-wrapper" darkmode="false" style="display:block;"><div data-waline provider="Waline"><!--v-if--><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">昵称</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">邮箱</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">网址</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="请留言。(填写邮箱可在被回复时收到邮件提醒)"></textarea><div class="wl-preview" style="display:none;"><hr><h4>预览:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="表情" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="表情包"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="上传图片"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="预览"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-captcha-container"></div><div class="wl-text-number">0 <!--v-if-->  字</div><button type="button" class="wl-btn">登录</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->提交<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="搜索表情包"><!--v-if--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> 评论</div><ul class="wl-sort"><!--[--><li class="active">按正序</li><li class="">按倒序</li><li class="">按热度</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><!--[--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><!--]--><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v2.15.8</div></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">西湖美景, 三月天嘞~</div><div class="vp-copyright">Copyright © 2023 Mr.R</div></footer></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-74fdf822.js" defer></script>
  </body>
</html>
