<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.67" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://mister-hope.github.io/keyan/dialog.html"><meta property="og:site_name" content=" "><meta property="og:title" content="一些关于dialog方向的论文收集"><meta property="og:description" content="一些关于dialog方向的论文收集 本文主要记录一下近4年(2019年起)各顶会顶刊有关dialog的paper名字，以便后续video dialog工作的调研和展开 (本文档未经过任何筛选，仅通过关键词搜索得到paper名字) 推荐二级检索关键词：history 、 genera、 visual、 Supervis、 video等 ECCV 2022 ECCV New Datasets and Models for Contextual Reasoning in Visual Dialog Video Dialog as Conversation About Objects Living in Space-Time"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-10-18T15:20:18.000Z"><meta property="article:author" content="Mr.R"><meta property="article:tag" content="去码头整点论文"><meta property="article:published_time" content="2023-09-19T00:00:00.000Z"><meta property="article:modified_time" content="2023-10-18T15:20:18.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"一些关于dialog方向的论文收集","image":[""],"datePublished":"2023-09-19T00:00:00.000Z","dateModified":"2023-10-18T15:20:18.000Z","author":[{"@type":"Person","name":"Mr.R","url":"https://mister-hope.com"}]}</script><title>一些关于dialog方向的论文收集 |  </title><meta name="description" content="一些关于dialog方向的论文收集 本文主要记录一下近4年(2019年起)各顶会顶刊有关dialog的paper名字，以便后续video dialog工作的调研和展开 (本文档未经过任何筛选，仅通过关键词搜索得到paper名字) 推荐二级检索关键词：history 、 genera、 visual、 Supervis、 video等 ECCV 2022 ECCV New Datasets and Models for Contextual Reasoning in Visual Dialog Video Dialog as Conversation About Objects Living in Space-Time">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-13796ab9.css" as="style"><link rel="stylesheet" href="/assets/style-13796ab9.css">
    <link rel="modulepreload" href="/assets/app-17b327d8.js"><link rel="modulepreload" href="/assets/dialog.html-80b173b0.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/dialog.html-d3391826.js"><link rel="prefetch" href="/assets/index.html-ac47bc91.js" as="script"><link rel="prefetch" href="/assets/intro.html-0cbcbfad.js" as="script"><link rel="prefetch" href="/assets/slides.html-a4360897.js" as="script"><link rel="prefetch" href="/assets/202309.html-b8b8b02a.js" as="script"><link rel="prefetch" href="/assets/202310.html-a8f07497.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-313493aa.js" as="script"><link rel="prefetch" href="/assets/index.html-c19cc023.js" as="script"><link rel="prefetch" href="/assets/disable.html-de6f1fb7.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-a961ffbf.js" as="script"><link rel="prefetch" href="/assets/markdown.html-d64453e3.js" as="script"><link rel="prefetch" href="/assets/page.html-a14c059e.js" as="script"><link rel="prefetch" href="/assets/index.html-6098c265.js" as="script"><link rel="prefetch" href="/assets/video.html-fe0150ef.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-618c6010.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-593e8df9.js" as="script"><link rel="prefetch" href="/assets/videoRepresentation.html-47b5e20b.js" as="script"><link rel="prefetch" href="/assets/videoUnderstanding.html-7613b61c.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-f32487f7.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-b3725aa6.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-55392bfb.js" as="script"><link rel="prefetch" href="/assets/DDP.html-f2b6dbc8.js" as="script"><link rel="prefetch" href="/assets/dan.html-9609164e.js" as="script"><link rel="prefetch" href="/assets/trick.html-b74e4cb5.js" as="script"><link rel="prefetch" href="/assets/404.html-130ff98b.js" as="script"><link rel="prefetch" href="/assets/index.html-10998ddb.js" as="script"><link rel="prefetch" href="/assets/index.html-e05d3217.js" as="script"><link rel="prefetch" href="/assets/index.html-cb0bac63.js" as="script"><link rel="prefetch" href="/assets/index.html-7fe952c2.js" as="script"><link rel="prefetch" href="/assets/index.html-82f2092e.js" as="script"><link rel="prefetch" href="/assets/index.html-e4678c97.js" as="script"><link rel="prefetch" href="/assets/index.html-c64b2618.js" as="script"><link rel="prefetch" href="/assets/index.html-7386b951.js" as="script"><link rel="prefetch" href="/assets/index.html-8a440362.js" as="script"><link rel="prefetch" href="/assets/index.html-305bc367.js" as="script"><link rel="prefetch" href="/assets/index.html-e683e385.js" as="script"><link rel="prefetch" href="/assets/index.html-b116232f.js" as="script"><link rel="prefetch" href="/assets/index.html-20b5478c.js" as="script"><link rel="prefetch" href="/assets/index.html-836ec29d.js" as="script"><link rel="prefetch" href="/assets/index.html-e7d44e3b.js" as="script"><link rel="prefetch" href="/assets/index.html-cccb4d6a.js" as="script"><link rel="prefetch" href="/assets/index.html-0917fcc6.js" as="script"><link rel="prefetch" href="/assets/index.html-52864055.js" as="script"><link rel="prefetch" href="/assets/index.html-c46cc60b.js" as="script"><link rel="prefetch" href="/assets/index.html-47792a83.js" as="script"><link rel="prefetch" href="/assets/index.html-aa30eeef.js" as="script"><link rel="prefetch" href="/assets/index.html-59dfac95.js" as="script"><link rel="prefetch" href="/assets/index.html-739d9637.js" as="script"><link rel="prefetch" href="/assets/index.html-5fabefb9.js" as="script"><link rel="prefetch" href="/assets/index.html-6476eeca.js" as="script"><link rel="prefetch" href="/assets/index.html-e909ac63.js" as="script"><link rel="prefetch" href="/assets/index.html-7cef4089.js" as="script"><link rel="prefetch" href="/assets/index.html-46567c63.js" as="script"><link rel="prefetch" href="/assets/index.html-8498dbec.js" as="script"><link rel="prefetch" href="/assets/index.html-4d1b2642.js" as="script"><link rel="prefetch" href="/assets/index.html-b32b66eb.js" as="script"><link rel="prefetch" href="/assets/index.html-1d386275.js" as="script"><link rel="prefetch" href="/assets/index.html-749de246.js" as="script"><link rel="prefetch" href="/assets/index.html-66325375.js" as="script"><link rel="prefetch" href="/assets/index.html-61263a6e.js" as="script"><link rel="prefetch" href="/assets/index.html-0e27a9a6.js" as="script"><link rel="prefetch" href="/assets/index.html-e6e96cde.js" as="script"><link rel="prefetch" href="/assets/index.html-9e0818b4.js" as="script"><link rel="prefetch" href="/assets/intro.html-27b6cb1e.js" as="script"><link rel="prefetch" href="/assets/slides.html-9e7d362f.js" as="script"><link rel="prefetch" href="/assets/202309.html-f3567a49.js" as="script"><link rel="prefetch" href="/assets/202310.html-12cc1ee4.js" as="script"><link rel="prefetch" href="/assets/maoxuan.html-7dac67c6.js" as="script"><link rel="prefetch" href="/assets/index.html-ba848ff9.js" as="script"><link rel="prefetch" href="/assets/disable.html-c0001b75.js" as="script"><link rel="prefetch" href="/assets/encrypt.html-c784d4e2.js" as="script"><link rel="prefetch" href="/assets/markdown.html-d903980d.js" as="script"><link rel="prefetch" href="/assets/page.html-9c089929.js" as="script"><link rel="prefetch" href="/assets/index.html-6be1e917.js" as="script"><link rel="prefetch" href="/assets/video.html-6236e50e.js" as="script"><link rel="prefetch" href="/assets/contrastiveLearning.html-bc9e4325.js" as="script"><link rel="prefetch" href="/assets/multiModal.html-add604f0.js" as="script"><link rel="prefetch" href="/assets/videoRepresentation.html-fa9902ff.js" as="script"><link rel="prefetch" href="/assets/videoUnderstanding.html-90c6ffb2.js" as="script"><link rel="prefetch" href="/assets/Bloodborne.html-bd571bd8.js" as="script"><link rel="prefetch" href="/assets/DeathStranding.html-23b30f3e.js" as="script"><link rel="prefetch" href="/assets/AVSD.html-b2e8d1df.js" as="script"><link rel="prefetch" href="/assets/DDP.html-3a76d9a8.js" as="script"><link rel="prefetch" href="/assets/dan.html-2eff742b.js" as="script"><link rel="prefetch" href="/assets/trick.html-b30b93ac.js" as="script"><link rel="prefetch" href="/assets/404.html-ae0d2837.js" as="script"><link rel="prefetch" href="/assets/index.html-adf914b5.js" as="script"><link rel="prefetch" href="/assets/index.html-def928f3.js" as="script"><link rel="prefetch" href="/assets/index.html-cf2a15c5.js" as="script"><link rel="prefetch" href="/assets/index.html-cfdfc634.js" as="script"><link rel="prefetch" href="/assets/index.html-92e0865f.js" as="script"><link rel="prefetch" href="/assets/index.html-c47a33d1.js" as="script"><link rel="prefetch" href="/assets/index.html-68ae4099.js" as="script"><link rel="prefetch" href="/assets/index.html-6a334378.js" as="script"><link rel="prefetch" href="/assets/index.html-b9124836.js" as="script"><link rel="prefetch" href="/assets/index.html-dd979141.js" as="script"><link rel="prefetch" href="/assets/index.html-3e6633f0.js" as="script"><link rel="prefetch" href="/assets/index.html-f6fb643d.js" as="script"><link rel="prefetch" href="/assets/index.html-0c603433.js" as="script"><link rel="prefetch" href="/assets/index.html-d89bc738.js" as="script"><link rel="prefetch" href="/assets/index.html-3639012a.js" as="script"><link rel="prefetch" href="/assets/index.html-a5033146.js" as="script"><link rel="prefetch" href="/assets/index.html-9316a5f0.js" as="script"><link rel="prefetch" href="/assets/index.html-4ec7dd2c.js" as="script"><link rel="prefetch" href="/assets/index.html-b7e6581e.js" as="script"><link rel="prefetch" href="/assets/index.html-a819d862.js" as="script"><link rel="prefetch" href="/assets/index.html-e9181f4f.js" as="script"><link rel="prefetch" href="/assets/index.html-919d7bd5.js" as="script"><link rel="prefetch" href="/assets/index.html-3772bfd7.js" as="script"><link rel="prefetch" href="/assets/index.html-b6d41058.js" as="script"><link rel="prefetch" href="/assets/index.html-d7d8cf19.js" as="script"><link rel="prefetch" href="/assets/index.html-51514955.js" as="script"><link rel="prefetch" href="/assets/index.html-dc044c14.js" as="script"><link rel="prefetch" href="/assets/index.html-bc2fd5f8.js" as="script"><link rel="prefetch" href="/assets/index.html-f4c6341f.js" as="script"><link rel="prefetch" href="/assets/index.html-b7784f9f.js" as="script"><link rel="prefetch" href="/assets/index.html-d7825969.js" as="script"><link rel="prefetch" href="/assets/index.html-1d94fad6.js" as="script"><link rel="prefetch" href="/assets/index.html-ec44be3f.js" as="script"><link rel="prefetch" href="/assets/index.html-8e66ed1f.js" as="script"><link rel="prefetch" href="/assets/index.html-6cb3876b.js" as="script"><link rel="prefetch" href="/assets/index.html-8a62ded0.js" as="script"><link rel="prefetch" href="/assets/index.html-71dfc575.js" as="script"><link rel="prefetch" href="/assets/waline-meta-56fbc549.js" as="script"><link rel="prefetch" href="/assets/component-0ceb28ec.js" as="script"><link rel="prefetch" href="/assets/auto-fe80bb03.js" as="script"><link rel="prefetch" href="/assets/index-2bf332f6.js" as="script"><link rel="prefetch" href="/assets/flowchart-c441f34d.js" as="script"><link rel="prefetch" href="/assets/mermaid.core-6195ead1.js" as="script"><link rel="prefetch" href="/assets/highlight.esm-75b11b9d.js" as="script"><link rel="prefetch" href="/assets/markdown.esm-9d5bc2ce.js" as="script"><link rel="prefetch" href="/assets/math.esm-70a288c8.js" as="script"><link rel="prefetch" href="/assets/notes.esm-a106bb2c.js" as="script"><link rel="prefetch" href="/assets/reveal.esm-1a4c3ae7.js" as="script"><link rel="prefetch" href="/assets/search.esm-7e6792e2.js" as="script"><link rel="prefetch" href="/assets/zoom.esm-b83b91d0.js" as="script"><link rel="prefetch" href="/assets/VuePlayground-6c760c1d.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-5762295a.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><img class="vp-nav-logo" src="/logo.svg" alt=" "><!----><span class="vp-site-name hide-in-pad"> </span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="码头"><span class="title"><!---->码头</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="视频理解" class="vp-link nav-link nav-link" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解<!----></a></li><li class="dropdown-item"><a aria-label="视频表征" class="vp-link nav-link nav-link" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->视频表征<!----></a></li><li class="dropdown-item"><a aria-label="对比学习" class="vp-link nav-link nav-link" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习<!----></a></li><li class="dropdown-item"><a aria-label="多模态" class="vp-link nav-link nav-link" href="/keyan/multiModal/multiModal.html"><!---->多模态<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="炉"><span class="title"><!---->炉</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="单机多卡DDP" class="vp-link nav-link nav-link" href="/train/DDP/DDP.html"><!---->单机多卡DDP<!----></a></li><li class="dropdown-item"><a aria-label="AVSD" class="vp-link nav-link nav-link" href="/train/AVSD/AVSD.html"><!---->AVSD<!----></a></li><li class="dropdown-item"><a aria-label="奇淫技巧" class="vp-link nav-link nav-link" href="/train/trick/trick.html"><!---->奇淫技巧<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="道心"><span class="title"><!---->道心</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="2023年9月" class="vp-link nav-link nav-link" href="/book/202309.html"><!---->2023年9月<!----></a></li><li class="dropdown-item"><a aria-label="2023年10月" class="vp-link nav-link nav-link" href="/book/202310.html"><!---->2023年10月<!----></a></li><li class="dropdown-item"><a aria-label="毛泽东选集" class="vp-link nav-link nav-link" href="/book/maoxuan.html"><!---->毛泽东选集<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="金樽"><span class="title"><!---->金樽</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>游戏</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a aria-label="死亡搁浅" class="vp-link nav-link nav-link" href="/life/game/DeathStranding.html"><!---->死亡搁浅<!----></a></li><li class="dropdown-subitem"><a aria-label="血缘诅咒" class="vp-link nav-link nav-link" href="/life/game/Bloodborne.html"><!---->血缘诅咒<!----></a></li></ul></li></ul></button></div></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><!--[--><a aria-label="视频理解综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/videoUnderstanding/videoUnderstanding.html"><!---->视频理解综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="关于视频理解的论文收集(较新)" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/videoRepresentation/videoRepresentation.html"><!---->关于视频理解的论文收集(较新)<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="对比学习综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/contrastiveLearning/contrastiveLearning.html"><!---->对比学习综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="多模态方向综述性质的记录" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/keyan/multiModal/multiModal.html"><!---->多模态方向综述性质的记录<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->一些关于dialog方向的论文收集</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://mister-hope.com" target="_blank" rel="noopener noreferrer">Mr.R</a></span><span property="author" content="Mr.R"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-09-19T00:00:00.000Z"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="waline-pageview-count" data-path="/keyan/dialog.html">...</span></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 20 分钟</span><meta property="timeRequired" content="PT20M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category3 clickable" role="navigation">科研</span><!--]--><meta property="articleSection" content="科研"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag1 clickable" role="navigation">去码头整点论文</span><!--]--><meta property="keywords" content="去码头整点论文"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#eccv">ECCV</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2022-eccv">2022 ECCV</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2020-eccv">2020 ECCV</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#iccv">ICCV</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2021-iccv">2021 ICCV</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2019-iccv">2019 ICCV</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#cvpr">CVPR</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2023-cvpr">2023 CVPR</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2022-cvpr">2022 CVPR</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2021-cvpr">2021 CVPR</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2020-cvpr">2020 CVPR</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2019-cvpr">2019 CVPR</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#aaai">AAAI</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2023-aaai">2023 AAAI</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2022-aaai">2022 AAAI</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2021-aaai">2021 AAAI</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2020-aaai">2020 AAAI</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2019-aaai">2019 AAAI</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#emnlp">EMNLP</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2023-emnlp">2023 EMNLP</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2022-emnlp">2022 EMNLP</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2021-emnlp">2021 EMNLP</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2020-emnlp">2020 EMNLP</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2019-emnlp">2019 EMNLP</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="/#acl">ACL</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2023-acl">2023 ACL</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2022-acl">2022 ACL</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2021-acl">2021 ACL</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2020-acl">2020 ACL</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="/#_2019-acl">2019 ACL</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="一些关于dialog方向的论文收集" tabindex="-1"><a class="header-anchor" href="#一些关于dialog方向的论文收集" aria-hidden="true">#</a> 一些关于dialog方向的论文收集</h1><p>本文主要记录一下近4年(2019年起)各顶会顶刊有关dialog的paper名字，以便后续video dialog工作的调研和展开<br> (本文档未经过任何筛选，仅通过关键词搜索得到paper名字)<br> 推荐二级检索关键词：history 、 genera、 visual、 Supervis、 video等</p><h2 id="eccv" tabindex="-1"><a class="header-anchor" href="#eccv" aria-hidden="true">#</a> ECCV</h2><h3 id="_2022-eccv" tabindex="-1"><a class="header-anchor" href="#_2022-eccv" aria-hidden="true">#</a> 2022 ECCV</h3><p>New Datasets and Models for Contextual Reasoning in Visual Dialog<br> Video Dialog as Conversation About Objects Living in Space-Time</p><h3 id="_2020-eccv" tabindex="-1"><a class="header-anchor" href="#_2020-eccv" aria-hidden="true">#</a> 2020 ECCV</h3><p>Efficient Attention Mechanism for Visual Dialog that Can Handle All the Interactions Between Multiple Inputs<br> Describing Unseen Videos via Multi-modal Cooperative Dialog Agents<br> Large-Scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline<br> Guessing State Tracking for Visual Dialogue</p><h2 id="iccv" tabindex="-1"><a class="header-anchor" href="#iccv" aria-hidden="true">#</a> ICCV</h2><h3 id="_2021-iccv" tabindex="-1"><a class="header-anchor" href="#_2021-iccv" aria-hidden="true">#</a> 2021 ICCV</h3><p>Self-Motivated Communication Agent for Real-World Vision-Dialog Navigation<br> Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue<br> On the hidden treasure of dialog in video question answering<br> Talk-to-Edit: Fine-Grained Facial Editing via Dialog</p><h3 id="_2019-iccv" tabindex="-1"><a class="header-anchor" href="#_2019-iccv" aria-hidden="true">#</a> 2019 ICCV</h3><p>Making History Matter: History-Advantage Sequence Training for Visual Dialog</p><h2 id="cvpr" tabindex="-1"><a class="header-anchor" href="#cvpr" aria-hidden="true">#</a> CVPR</h2><h3 id="_2023-cvpr" tabindex="-1"><a class="header-anchor" href="#_2023-cvpr" aria-hidden="true">#</a> 2023 CVPR</h3><p>The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</p><h3 id="_2022-cvpr" tabindex="-1"><a class="header-anchor" href="#_2022-cvpr" aria-hidden="true">#</a> 2022 CVPR</h3><p>UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog</p><h3 id="_2021-cvpr" tabindex="-1"><a class="header-anchor" href="#_2021-cvpr" aria-hidden="true">#</a> 2021 CVPR</h3><p>Learning Better Visual Dialog Agents With Pretrained Visual-Linguistic Representation</p><h3 id="_2020-cvpr" tabindex="-1"><a class="header-anchor" href="#_2020-cvpr" aria-hidden="true">#</a> 2020 CVPR</h3><p>Iterative Context-Aware Graph Inference for Visual Dialog<br> Vision-Dialog Navigation by Exploring Cross-Modal Memory<br> Two Causal Principles for Improving Visual Dialog</p><h3 id="_2019-cvpr" tabindex="-1"><a class="header-anchor" href="#_2019-cvpr" aria-hidden="true">#</a> 2019 CVPR</h3><p>Reasoning Visual Dialogs With Structural and Partial Observations<br> Recursive Visual Attention in Visual Dialog<br> Audio Visual Scene-Aware Dialog<br> Image-Question-Answer Synergistic Network for Visual Dialog<br> A Simple Baseline for Audio-Visual Scene-Aware Dialog</p><h2 id="aaai" tabindex="-1"><a class="header-anchor" href="#aaai" aria-hidden="true">#</a> AAAI</h2><h3 id="_2023-aaai" tabindex="-1"><a class="header-anchor" href="#_2023-aaai" aria-hidden="true">#</a> 2023 AAAI</h3><p>Learning to Memorize Entailment and Discourse Relations for Persona-Consistent Dialogues<br> Learning towards Selective Data Augmentation for Dialogue Generation<br> Learning to Imagine: Distillation-Based Interactive Context Exploitation for Dialogue State Tracking<br> Personalized Dialogue Generation with Persona-Adaptive Attention<br> Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues<br> Mitigating Negative Style Transfer in Hybrid Dialogue System<br> Heterogeneous-Branch Collaborative Learning for Dialogue Generation<br> Learning to Know Myself: A Coarse-to-Fine Persona-Aware Training Framework for Personalized Dialogue Generation<br> A Disentangled-Attention Based Framework with Persona-Aware Prompt Learning for Dialogue Generation<br> Towards Credible Human Evaluation of Open-Domain Dialog Systems Using Interactive Setup<br> Towards Complex Scenarios: Building End-to-End Task-Oriented Dialogue System across Multiple Knowledge Bases<br> Towards Diverse, Relevant and Coherent Open-Domain Dialogue Generation via Hybrid Latent Variables<br> Taming Continuous Posteriors for Latent Variational Dialogue Policies<br> Dialogue Rewriting via Skeleton-Guided Generation<br> Dialogue State Distillation Network with Inter-slot Contrastive Learning for Dialogue State Tracking<br> Balanced Meta Learning and Diverse Sampling for Lifelong Task-Oriented Dialogue Systems<br> On the Calibration and Uncertainty with Pólya-Gamma Augmentation for Dialog Retrieval Models<br> Multi-Action Dialog Policy Learning from Logged User Feedback<br> KPT: Keyword-Guided Pre-training for Grounded Dialog Generation<br> Help Me Heal: A Reinforced Polite and Empathetic Mental Health and Legal Counseling Dialogue System for Crime Victims<br> Improving Dialogue Intent Classification with a Knowledge-Enhanced Multifactor Graph Model (Student Abstract)<br> Class Incremental Learning for Task-Oriented Dialogue System with Contrastive Distillation on Internal Representations (Student Abstract)</p><h3 id="_2022-aaai" tabindex="-1"><a class="header-anchor" href="#_2022-aaai" aria-hidden="true">#</a> 2022 AAAI</h3><p>ALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-supervised Learning and Explicit Policy Injection<br> Dual Task Framework for Improving Persona-Grounded Dialogue Dataset<br> SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems<br> Knowledge Bridging for Empathetic Dialogue Generation<br> Contrast and Generation Make BART a Good Dialogue Emotion Recognizer<br> A Semi-supervised Learning Approach with Two Teachers to Improve Breakdown Identification in Dialogues<br> Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-Based Encoder<br> CINS: Comprehensive Instruction for Few-Shot Learning in Task-Oriented Dialog Systems<br> GraphMemDialog: Optimizing End-to-End Task-Oriented Dialog Systems Using Graph Memory Networks<br> ValueNet: A New Dataset for Human Value Driven Dialogue System<br> Fusing Task-Oriented and Open-Domain Dialogues in Conversational Agents<br> MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation<br> Efficient Dialog Policy Learning by Reasoning with Contextual Knowledge<br> DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization<br> Multi-Dimension Attention for Multi-Turn Dialog Generation (Student Abstract)<br> Building Goal-Oriented Dialogue Systems with Situated Visual Context</p><h3 id="_2021-aaai" tabindex="-1"><a class="header-anchor" href="#_2021-aaai" aria-hidden="true">#</a> 2021 AAAI</h3><p>Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers<br> Structured Co-reference Graph Attention for Video-grounded Dialogue<br> Reasoning in Dialog: Improving Response Generation by Context Reading Comprehension<br> MultiTalk: A Highly-Branching Dialog Testbed for Diverse Conversations<br> Multi-View Feature Representation for Dialogue Generation with Bidirectional Distillation<br> DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances<br> DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues<br> Interpretable NLG for Task-oriented Dialogue Systems with Heterogeneous Rendering Machines<br> Graph-Evolving Meta-Learning for Low-Resource Medical Dialogue Generation<br> Converse, Focus and Guess - Towards Multi-Document Driven Dialogue<br> Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue<br> Dialog Policy Learning for Joint Clarification and Active Learning Queries<br> A Student-Teacher Architecture for Dialog Domain Adaptation Under the Meta-Learning Setting<br> Exploring Auxiliary Reasoning Tasks for Task-oriented Dialog Systems with Meta Cooperative Learning<br> Co-GAT: A Co-Interactive Graph Attention Network for Joint Dialog Act Recognition and Sentiment Classification<br> DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition<br> Unsupervised Learning of Deterministic Dialogue Structure with Edge-Enhanced Graph Auto-Encoder<br> NaturalConv: A Chinese Dialogue Dataset Towards Multi-turn Topic-driven Conversation<br> Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues<br> Topic-Aware Multi-turn Dialogue Modeling<br> UBAR: Towards Fully End-to-End Task-Oriented Dialog System with GPT-2<br> Open Domain Dialogue Generation with Latent Images<br> Unsupervised Abstractive Dialogue Summarization for Tete-a-Tetes<br> Automatic Curriculum Learning With Over-repetition Penalty for Dialogue Policy Learning<br> Stylized Dialogue Response Generation Using Stylized Unpaired Texts<br> Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling<br> Lifelong and Continual Learning Dialogue Systems: Learning during Conversation<br> Empowering Conversational AI is a Trip to Mars: Progress and Future of Open Domain Human-Computer Dialogues<br> Knowledge-aware Dialogue Generation with Hybrid Attention (Student Abstract)<br> Bootstrapping Dialog Models from Human to Human Conversation Logs<br> Dialog Router: Automated Dialog Transition via Multi-Task Learning<br> Integrating Pre-trained Model into Rule-based Dialogue Management</p><h3 id="_2020-aaai" tabindex="-1"><a class="header-anchor" href="#_2020-aaai" aria-hidden="true">#</a> 2020 AAAI</h3><p>Towards Hands-Free Visual Dialog Interactive Recommendation<br> Modeling Dialogues with Hashcode Representations: A Nonparametric Approach<br> Learning from Easy to Complex: Adaptive Multi-Curricula Learning for Neural Dialogue Generation<br> DMRM: A Dual-Channel Multi-Hop Reasoning Model for Visual Dialog<br> Schema-Guided Multi-Domain Dialogue State Tracking with Graph Attention Neural Networks<br> Guiding Attention in Sequence-to-Sequence Models for Dialogue Act Prediction<br> Likelihood Ratios and Generative Classifiers for Unsupervised Out-of-Domain Detection in Task Oriented Dialog<br> Predictive Engagement: An Efficient Metric for Automatic Evaluation of Open-Domain Dialogue Systems<br> MALA: Cross-Domain Dialogue Generation with Action Learning<br> Bayes-Adaptive Monte-Carlo Planning and Learning for Goal-Oriented Dialogues<br> Modality-Balanced Models for Visual Dialogue<br> MA-DST: Multi-Attention-Based Scalable Dialog State Tracking<br> ALOHA: Artificial Learning of Human Attributes for Dialogue Agents<br> End-to-End Trainable Non-Collaborative Dialog System<br> MOSS: End-to-End Dialog System Framework with Modular Supervision<br> Attention-Informed Mixed-Language Training for Zero-Shot Cross-Lingual Task-Oriented Dialogue Systems<br> MTSS: Learn from Multiple Domain Teachers and Become a Multi-Domain Dialogue Expert<br> DCR-Net: A Deep Co-Interactive Relation Network for Joint Dialog Act Recognition and Sentiment Classification<br> Entrainment2Vec: Embedding Entrainment for Multi-Party Dialogues<br> Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset<br> Hierarchical Reinforcement Learning for Open-Domain Dialog<br> Generating Persona Consistent Dialogues by Exploiting Natural Language Inference<br> History-Adaption Knowledge Incorporation Mechanism for Multi-Turn Dialogue System<br> Improving Knowledge-Aware Dialogue Generation via Knowledge Base Question Answering<br> Sentiment Classification in Customer Service Dialogue with Topic-Aware Multi-Task Learning<br> Masking Orchestration: Multi-Task Pretraining for Multi-Role Dialogue Representation Learning<br> Dialog State Tracking with Reinforced Data Augmentation<br> Filling Conversation Ellipsis for Better Social Dialog Understanding<br> Task-Oriented Dialog Systems That Consider Multiple Appropriate Responses under the Same Context<br> A Pre-Training Based Personalized Dialogue Generation Model with Persona-Sparse Data<br> DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue<br> Visual Dialogue State Tracking for Question Generation<br> Multi-Speaker Video Dialog with Frame-Level Temporal Localization<br> Back to the Future for Dialogue Research<br> Doc2Dial: A Framework for Dialogue Composition Grounded in Documents<br> Automatic Text-Based Personality Recognition on Monologues and Multiparty Dialogues Using Attentive Networks and Contextual Embeddings (Student Abstract)<br> Topic Enhanced Controllable CVAE for Dialogue Generation (Student Abstract)<br> Breakdown Detection in Negotiation Dialogues (Student Abstract)</p><h3 id="_2019-aaai" tabindex="-1"><a class="header-anchor" href="#_2019-aaai" aria-hidden="true">#</a> 2019 AAAI</h3><p>Goal-Oriented Dialogue Policy Learning from Failures<br> Re-Evaluating ADEM: A Deeper Look at Scoring Dialogue Responses<br> Dialogue Generation: From Imitation Learning to Inverse Reinforcement Learning<br> Learning Personalized End-to-End Goal-Oriented Dialog<br> DialogueRNN: An Attentive RNN for Emotion Detection in Conversations<br> A Deep Sequential Model for Discourse Parsing on Multi-Party Dialogues<br> Switch-Based Active Deep Dyna-Q: Efficient Adaptive Planning for Task-Completion Dialogue Policy Learning<br> End-to-End Knowledge-Routed Relational Dialogue System for Automatic Diagnosis<br> MAi: An Intelligent Model Acquisition Interface for Interactive Specification of Dialogue Agents<br> Reinforcement Learning for Improved Low Resource Dialogue Generation</p><h2 id="emnlp" tabindex="-1"><a class="header-anchor" href="#emnlp" aria-hidden="true">#</a> EMNLP</h2><h3 id="_2023-emnlp" tabindex="-1"><a class="header-anchor" href="#_2023-emnlp" aria-hidden="true">#</a> 2023 EMNLP</h3><h3 id="_2022-emnlp" tabindex="-1"><a class="header-anchor" href="#_2022-emnlp" aria-hidden="true">#</a> 2022 EMNLP</h3><p>BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems<br> InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning<br> Correctable-DST: Mitigating Historical Context Mismatch between Training and Inference for Improved Dialogue State Tracking<br> Curriculum Prompt Learning with Self-Training for Abstractive Dialogue Summarization<br> MetaASSIST: Robust Dialogue State Tracking with Meta Learning<br> Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues<br> There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning<br> D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat<br> Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking<br> Navigating Connected Memories with a Task-oriented Dialog System<br> Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling<br> Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning<br> Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems<br> FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation<br> ProsocialDialog: A Prosocial Backbone for Conversational Agents<br> CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation<br> Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue<br> A Speaker-Aware Co-Attention Framework for Medical Dialogue Information Extraction<br> Analyzing and Evaluating Faithfulness in Dialogue Summarization<br> STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension<br> BotsTalk: Machine-sourced Framework for Automatic Curation of Large-scale Multi-skill Dialogue Datasets<br> Q-TOD: A Query-driven Task-oriented Dialogue System<br> Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings<br> Extending Phrase Grounding with Pronouns in Visual Dialogues<br> ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization<br> Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering<br> Don&#39;t Copy the Teacher: Data and Model Challenges in Embodied Dialogue<br> Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence<br> Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure<br> Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality<br> FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows<br> Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task<br> Structural Constraints and Natural Language Inference for End-to-End Flowchart Grounded Dialog Response Generation<br> FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue<br> IM⌃2: an Interpretable and Multi-category Integrated Metric Framework for Automatic Dialogue Evaluation<br> Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue<br> End-to-End Neural Discourse Deixis Resolution in Dialogue<br> CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation<br> Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems<br> JDDC 2.1: A Multimodal Chinese Dialogue Dataset with Joint Tasks of Query Rewriting, Response Generation, Discourse Parsing, and Summarization<br> DialogConv: A Lightweight Fully Convolutional Network for Multi-view Response Selection</p><h3 id="_2021-emnlp" tabindex="-1"><a class="header-anchor" href="#_2021-emnlp" aria-hidden="true">#</a> 2021 EMNLP</h3><p>Athena 2.0: Contextualized Dialogue Management for an Alexa Prize SocialBot<br> Towards Making the Most of Dialogue Characteristics for Neural Chat Translation<br> Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining<br> Controllable Neural Dialogue Summarization with Personal Named Entity Planning<br> GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation<br> Graph Based Network with Contextualized Representations of Turns in Dialogue<br> Automatically Exposing Problems with Neural Dialog Models<br> MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks<br> Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking<br> We&#39;ve had this conversation before: A Novel Approach to Measuring Dialog Similarity<br> CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation<br> DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization<br> Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems<br> Contextual Rephrase Detection for Reducing Friction in Dialogue Systems<br> Reference-Centric Models for Grounded Collaborative Dialogue<br> Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding<br> Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems<br> Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration<br> CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation<br> A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation<br> Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue<br> More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge<br> Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks<br> Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks<br> Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation<br> Don&#39;t be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System<br> Transferable Persona-Grounded Dialogues via Grounded Minimal Edits<br> DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings<br> Adaptive Bridge between Training and Inference for Dialogue Generation<br> Smoothing Dialogue States for Open Conversational Machine Reading<br> Exophoric Pronoun Resolution in Dialogues with Topic Regularization<br> Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems<br> Efficient Dialogue Complementary Policy Learning via Deep Q-network Policy and Episodic Memory Policy<br> End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs<br> CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization<br> Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions<br> Region under Discussion for visual dialog<br> Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts<br> Multi-Modal Open-Domain Dialogue<br> Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection<br> SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations<br> RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging<br> MRF-Chat: Improving Dialogue with Markov Random Fields<br> Dialogue State Tracking with a Language Model using Schema-Driven Prompting<br> MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents<br> NDH-Full: Learning and Evaluating Navigational Agents on Full-Length Dialogue<br> Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue Summarization<br> Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach<br> Continual Learning in Task-Oriented Dialogue Systems<br> Investigating Robustness of Dialog Models to Popular Figurative Language Constructs<br> Effective Sequence-to-Sequence Dialogue State Tracking<br> Learning Neural Templates for Recommender Dialogue System<br> Proxy Indicators for the Quality of Open-domain Dialogues<br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">Q^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering<br> Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking<br> A Collaborative Multi-agent Reinforcement Learning Framework for Dialog Action Decomposition<br> Zero-Shot Dialogue State Tracking via Cross-Task Transfer<br> Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance<br> A Bag of Tricks for Dialogue Summarization<br> Is Information Density Uniform in Task-Oriented Dialogues<br> Code-switched inspired losses for spoken dialog representations<br> Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy</p><h3 id="_2020-emnlp" tabindex="-1"><a class="header-anchor" href="#_2020-emnlp" aria-hidden="true">#</a> 2020 EMNLP</h3><p>Dialogue Response Ranking Training with Large-Scale Human Feedback Data<br> Where Are You? Localization from Embodied Dialog<br> Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning<br> Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness<br> TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue<br> RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling<br> Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness<br> BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues<br> UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues<br> GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems<br> Structured Attention for Unsupervised Dialogue Structure Induction<br> Cross Copy Network for Dialogue Generation<br> Multi-turn Response Selection using Dialogue Dependency Relations<br> Parallel Interactive Networks for Multi-Domain Dialogue State Generation<br> How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue<br> Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking<br> A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses<br> VD-BERT: A Unified Vision and Dialog Transformer with BERT<br> Knowledge-Grounded Dialogue Generation with Pre-trained Language Models<br> MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems<br> Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation<br> Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation<br> Counterfactual Off-Policy Training for Neural Dialogue Generation<br> Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data<br> Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network<br> AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue<br> Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training<br> Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems<br> Human-centric dialog training via offline reinforcement learning<br> Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization<br> Generating Dialogue Responses from a Semantic Latent Space<br> Probing Task-Oriented Dialogue Representation from Language Models<br> Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging<br> Sound Natural: Content Rephrasing in Dialog Systems<br> Template Guided Text Generation for Task-Oriented Dialogue<br> Regularizing Dialogue Generation by Imitating Implicit Scenarios<br> Semantic Role Labeling Guided Multi-turn Dialogue ReWriter<br> Profile Consistency Identification for Open-domain Dialogue Agents<br> Conversational Semantic Parsing for Dialog State Tracking<br> doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset<br> Interview: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding<br> INSPIRED: Toward Sociable Recommendation Dialog Systems<br> Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation<br> Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions<br> A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning<br> The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection<br> GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems<br> MedDialog: Large-scale Medical Dialogue Datasets</p><h3 id="_2019-emnlp" tabindex="-1"><a class="header-anchor" href="#_2019-emnlp" aria-hidden="true">#</a> 2019 EMNLP</h3><p>LIDA: Lightweight Interactive Dialogue Annotator<br> PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking<br> PyOpenDial: A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules<br> Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog<br> Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever<br> Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation<br> DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation<br> Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning<br> Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue<br> Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation<br> Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling<br> Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems<br> Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables<br> Modeling Multi-Action Policy for Task-Oriented Dialogues<br> Automatically Learning Data Augmentation Policies for Dialogue Tasks<br> Improving Generative Visual Dialog by Answering Diverse Questions<br> Dependency Parsing for Spoken Dialog Systems<br> Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog<br> Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks<br> Multi-Granularity Representations of Dialog<br> Are You for Real? Detecting Identity Fraud via Dialogue Interactions<br> Adaptive Parameterization for Neural Dialogue Generation<br> Towards Knowledge-Based Recommender Dialog System<br> Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration<br> DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs<br> Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework<br> Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation<br> A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation<br> Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue<br> A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection<br> How to Build User Simulators to Train RL-based Dialog Systems<br> Dual Attention Networks for Visual Reference Resolution in Visual Dialog<br> Video Dialog via Progressive Inference and Cross-Transformer<br> Dialog Intent Induction with Deep Multi-View Clustering<br> Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset<br> Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data<br> Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack<br> GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue<br> Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph<br> What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues</p><h2 id="acl" tabindex="-1"><a class="header-anchor" href="#acl" aria-hidden="true">#</a> ACL</h2><h3 id="_2023-acl" tabindex="-1"><a class="header-anchor" href="#_2023-acl" aria-hidden="true">#</a> 2023 ACL</h3><p>ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?<br> Controllable Mixed-Initiative Dialogue Generation through Prompting<br> Towards Fewer Hallucinations in Knowledge-Grounded Dialogue Generation via Augmentative and Contrastive Knowledge-Dialogue<br> One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems<br> Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking<br> EM Pre-training for Multi-party Dialogue Response Generation<br> Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information<br> DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations<br> DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization<br> White-Box Multi-Objective Adversarial Attack on Dialogue Generation<br> Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking<br> Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues<br> MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions<br> Injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue<br> BREAK: Breaking the Dialogue State Tracking Barrier with Beam Search and Re-ranking<br> Learning to Generate Equitable Text in Dialogue from Biased Training Data<br> CORE: Cooperative Training of Retriever-Reranker for Effective Dialogue Response Selection<br> PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism<br> DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue<br> Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations<br> ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems<br> Towards Faithful Dialogues via Focus Learning<br> Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation<br> Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation<br> VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions<br> Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona<br> SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams<br> FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue<br> Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models<br> Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization<br> MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation<br> Envisioning Future from the Past: Hierarchical Duality Learning for Multi-Turn Dialogue Generation<br> Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic<br> A Dataset of Argumentative Dialogues on Scientific Papers<br> Contextual Knowledge Learning for Dialogue Generation<br> Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment<br> MidMed: Towards Mixed-Type Dialogues for Medical Consultation<br> RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation<br> Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process<br> Pre-training Multi-party Dialogue Models with Latent Discourse Inference<br> PAED: Zero-Shot Persona Attribute Extraction in Dialogues<br> SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation<br> Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training<br> A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment<br> A Synthetic Data Generation Framework for Grounded Dialogues<br> Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog<br> XDailyDialog: A Multilingual Parallel Dialogue Corpus<br> RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue<br> PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts<br> Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback<br> On the Compositional Generalization in Versatile Open-domain Dialogue<br> Dialogue Summarization with Static-Dynamic Structure Fusion Graph<br> Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework<br> Multimodal Persona Based Generation of Comic Dialogs<br> Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation<br> Towards Understanding Omission in Dialogue Summarization<br> Don&#39;t Forget Your ABC&#39;s: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems<br> LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming</p><h3 id="_2022-acl" tabindex="-1"><a class="header-anchor" href="#_2022-acl" aria-hidden="true">#</a> 2022 ACL</h3><p>Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension<br> Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking<br> Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances<br> UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue<br> Investigating person-specific errors in chat-oriented dialogue systems<br> Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge<br> [CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue<br> UniTranSeR: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System<br> Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking<br> Structural Characterization for Dialogue Disentanglement<br> Multi-Party Empathetic Dialogue Generation: A New Task for Dialog Systems<br> DEAM: Dialogue Coherence Evaluation using AMR-based Semantic Manipulations<br> ProphetChat: Enhancing Dialogue Generation with Simulation of Future Conversation<br> Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals<br> Continual Prompt Tuning for Dialog State Tracking<br> Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue<br> Summ<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">^N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span></span></span>: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents<br> GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems<br> Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking<br> Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions<br> Multimodal Dialogue Response Generation<br> A Taxonomy of Empathetic Questions in Social Dialogs<br> Knowledge Enhanced Reflection Generation for Counseling Dialogues<br> Improving Multi-label Malevolence Detection in Dialogues through Multi-faceted Label Correlation Enhancement<br> The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems<br> DialFact: A Benchmark for Fact-Checking in Dialogue<br> There Are a Thousand Hamlets in a Thousand People&#39;s Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory<br> Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System<br> DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation<br> An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation<br> CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues<br> M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database<br> What does the sea say to the shore? A BERT based DST style approach for speaker to dialogue attribution in novels<br> When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues<br> SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues<br> Achieving Reliable Human Assessment of Open-Domain Dialogue Systems<br> SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures<br> The AI Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications<br> Lexical Knowledge Internalization for Neural Dialog Generation<br> A Model-agnostic Data Manipulation Method for Persona-based Dialogue Generation<br> Situated Dialogue Learning through Procedural Environment Generation<br> Internet-Augmented Dialogue Generation</p><h3 id="_2021-acl" tabindex="-1"><a class="header-anchor" href="#_2021-acl" aria-hidden="true">#</a> 2021 ACL</h3><p>Saying No is An Art: Contextualized Fallback Responses for Unanswerable Dialogue Queries<br> PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation<br> Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking<br> Unsupervised Enrichment of Persona-grounded Dialog with Background Stories<br> Domain-Adaptive Pretraining Methods for Dialogue Understanding<br> Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking<br> On the Generation of Medical Dialogs for COVID-19<br> Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images<br> Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances<br> Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking<br> Transferable Dialogue Systems and User Simulators<br> BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data<br> SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues<br> TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems<br> Improving Dialog Systems for Negotiation with Personality Modeling<br> Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training<br> Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features<br> Prosodic segmentation for parsing spoken dialogue<br> Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization<br> Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection<br> I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling<br> Discovering Dialog Structure Graph for Coherent Dialog Generation<br> A Sequence-to-Sequence Approach to Dialogue State Tracking<br> Dialogue Response Selection with Hierarchical Curriculum Learning<br> Discovering Dialogue Slots with Weak Supervision<br> Robustness Testing of Language Understanding in Task-Oriented Dialog<br> Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?<br> OTTers: One-turn Topic Transitions for Open-Domain Dialogue<br> Towards Quantifiable Dialogue Coherence Evaluation<br> Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System<br> Towards Emotional Support Dialog Systems<br> Diversifying Dialog Generation via Adaptive Label Smoothing<br> NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation<br> RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems<br> Semantic Representation for Dialogue Modeling<br> Structural Pre-training for Dialogue Comprehension<br> A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues<br> Generating Relevant and Coherent Dialogue Responses using Self-Separated Conditional Variational AutoEncoders<br> DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue<br> DynaEval: Unifying Turn and Dialogue Level Evaluation<br> RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy<br> PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling<br> Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer<br> DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations<br> TIMEDIAL: Temporal Commonsense Reasoning in Dialog</p><h3 id="_2020-acl" tabindex="-1"><a class="header-anchor" href="#_2020-acl" aria-hidden="true">#</a> 2020 ACL</h3><p>ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems<br> DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation<br> Conversation Learner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems<br> Designing Precise and Robust Dialogue Response Evaluators<br> Dialogue State Tracking with Explicit Slot Connection Modeling<br> Large Scale Multi-Actor Generative Dialog Modeling<br> PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable<br> Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network<br> Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations<br> Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking<br> Efficient Dialogue State Tracking by Selectively Overwriting Memory<br> End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2<br> Evaluating Dialogue Generation Systems via Response Selection<br> Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment<br> Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition<br> Paraphrase Augmented Task-Oriented Dialog Generation<br> Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation<br> USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation<br> Towards Conversational Recommendation over Multi-Type Dialogs<br> Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis<br> Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation<br> Conversational Word Embedding for Retrieval-Based Dialog System<br> Learning Dialog Policies from Weak Demonstrations<br> MuTual: A Dataset for Multi-Turn Dialogue Reasoning<br> You Impress Me: Dialogue Generation via Mutual Persona Perception<br> Dialogue Coherence Assessment Without Explicit Dialogue Act Labels<br> &quot;None of the Above&quot;: Measure Uncertainty in Dialog Response Retrieval<br> Negative Training for Neural Dialogue Response Generation<br> Recursive Template-based Frame Generation for Task Oriented Dialog<br> Grounding Conversations with Improvised Dialogues<br> Learning an Unreferenced Metric for Online Dialogue Evaluation<br> Neural Generation of Dialogue Response Timings<br> The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents<br> Learning to execute instructions in a Minecraft dialogue<br> ChartDialogs: Plotting from Natural Language Instructions<br> Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation<br> Towards Emotion-aided Multi-modal Dialogue Act Classification<br> Don&#39;t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training<br> Dialogue-Based Relation Extraction<br> More Diverse Dialogue Datasets via Diversity-Informed Data Collection<br> Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset<br> Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering<br> Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness<br> Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation<br> Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks<br> Video-Grounded Dialogues with Pretrained Generation Language Models<br> A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking<br> Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight<br> Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog<br> Learning Efficient Dialogue Policy from Demonstrations through Shaping<br> SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing<br> MIE: A Medical Information Extractor towards Medical Dialogues<br> Diversifying Dialogue Generation with Non-Conversational Text<br> KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation<br> Meta-Reinforced Multi-Domain State Generator for Dialogue Systems<br> Modeling Long Context for Task-Oriented Dialogue State Generation<br> Multi-Domain Dialogue Acts and Response Co-Generation<br> History for Visual Dialog: Do we really need it?</p><h3 id="_2019-acl" tabindex="-1"><a class="header-anchor" href="#_2019-acl" aria-hidden="true">#</a> 2019 ACL</h3><p>Dialogue-Act Prediction of Future Responses Based on Conversation History<br> One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues<br> Improving Multi-turn Dialogue Modelling with Utterance ReWriter<br> Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study<br> Boosting Dialog Response Generation<br> Implicit Discourse Relation Identification for Open-domain Dialogues<br> Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems<br> Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue<br> Domain Adaptive Dialog Generation via Meta Learning<br> The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue<br> Learning from Dialogue after Deployment: Feed Yourself, Chatbot!<br> A Working Memory Model for Task-oriented Dialog Response Generation<br> Generating Responses with a Specific Emotion in Dialog<br> Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention<br> ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation<br> Incremental Learning from Scratch for Task-Oriented Dialogue Systems<br> Dialogue Natural Language Inference<br> Budgeted Policy Learning for Task-Oriented Dialogue Systems<br> Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems<br> Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References<br> Pretraining Methods for Dialog Context Representation Learning<br> Self-Supervised Dialogue Learning<br> Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment<br> Training Neural Response Selection for Task-Oriented Dialogue Systems<br> Collaborative Dialogue in Minecraft<br> Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System<br> Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference<br> Personalizing Dialogue Agents via Meta-Learning<br> Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension<br> Rationally Reappraising ATIS-based Dialogue Systems<br> Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes<br> Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems<br> Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good<br> What Should I Ask? Using Conversationally Informative Rewards for Goal-oriented Visual Dialog<br> Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog</p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/keyan/dialog.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 799976781@qq.com">WillebrordSnell</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="waline-wrapper" darkmode="false" style="display:block;"><div data-waline provider="Waline"><!--v-if--><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">昵称</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">邮箱</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">网址</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="请留言。(填写邮箱可在被回复时收到邮件提醒)"></textarea><div class="wl-preview" style="display:none;"><hr><h4>预览:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="表情" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="表情包"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="上传图片"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="预览"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-captcha-container"></div><div class="wl-text-number">0 <!--v-if-->  字</div><button type="button" class="wl-btn">登录</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->提交<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="搜索表情包"><!--v-if--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> 评论</div><ul class="wl-sort"><!--[--><li class="active">按正序</li><li class="">按倒序</li><li class="">按热度</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><!--[--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><!--]--><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v2.15.8</div></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">西湖美景, 三月天嘞~</div><div class="vp-copyright">Copyright © 2023 Mr.R</div></footer></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-17b327d8.js" defer></script>
  </body>
</html>
